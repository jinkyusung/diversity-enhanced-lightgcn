loss: -3.472607 [ 2048/1237259]
loss: -3.554026 [206848/1237259]
loss: -3.567947 [411648/1237259]
loss: -3.575756 [616448/1237259]
loss: -3.578899 [821248/1237259]
loss: -3.581018 [1026048/1237259]
loss: -3.586966 [1230848/1237259]
Train Avg loss: -3.565364
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0437  
ndcg@20: 0.0355  
diversity: 0.2858  


Epoch 2
-------------------------------
loss: -3.622333 [ 2048/1237259]
loss: -3.613048 [206848/1237259]
loss: -3.612670 [411648/1237259]
loss: -3.604342 [616448/1237259]
loss: -3.600624 [821248/1237259]
loss: -3.603700 [1026048/1237259]
loss: -3.601710 [1230848/1237259]
Train Avg loss: -3.608246
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0489  
ndcg@20: 0.0398  
diversity: 0.2787  


Epoch 3
-------------------------------
loss: -3.628797 [ 2048/1237259]
loss: -3.625602 [206848/1237259]
loss: -3.619913 [411648/1237259]
loss: -3.614517 [616448/1237259]
loss: -3.609449 [821248/1237259]
loss: -3.607947 [1026048/1237259]
loss: -3.605400 [1230848/1237259]
Train Avg loss: -3.615784
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0513  
ndcg@20: 0.0416  
diversity: 0.2755  


Epoch 4
-------------------------------
loss: -3.636562 [ 2048/1237259]
loss: -3.627536 [206848/1237259]
loss: -3.620069 [411648/1237259]
loss: -3.616968 [616448/1237259]
loss: -3.614524 [821248/1237259]
loss: -3.612885 [1026048/1237259]
loss: -3.609894 [1230848/1237259]
Train Avg loss: -3.619406
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0524  
ndcg@20: 0.0425  
diversity: 0.2733  


Epoch 5
-------------------------------
loss: -3.634534 [ 2048/1237259]
loss: -3.631189 [206848/1237259]
loss: -3.624846 [411648/1237259]
loss: -3.621670 [616448/1237259]
loss: -3.619167 [821248/1237259]
loss: -3.614549 [1026048/1237259]
loss: -3.612134 [1230848/1237259]
Train Avg loss: -3.621591
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0533  
ndcg@20: 0.0434  
diversity: 0.2712  


Epoch 6
-------------------------------
loss: -3.634589 [ 2048/1237259]
loss: -3.632290 [206848/1237259]
loss: -3.628912 [411648/1237259]
loss: -3.622852 [616448/1237259]
loss: -3.620063 [821248/1237259]
loss: -3.618133 [1026048/1237259]
loss: -3.616390 [1230848/1237259]
Train Avg loss: -3.623108
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0540  
ndcg@20: 0.0442  
diversity: 0.2695  


Epoch 7
-------------------------------
loss: -3.636208 [ 2048/1237259]
loss: -3.632788 [206848/1237259]
loss: -3.628813 [411648/1237259]
loss: -3.622532 [616448/1237259]
loss: -3.620912 [821248/1237259]
loss: -3.617575 [1026048/1237259]
loss: -3.613840 [1230848/1237259]
Train Avg loss: -3.624291
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0548  
ndcg@20: 0.0448  
diversity: 0.2678  


Epoch 8
-------------------------------
loss: -3.636419 [ 2048/1237259]
loss: -3.630687 [206848/1237259]
loss: -3.628837 [411648/1237259]
loss: -3.625119 [616448/1237259]
loss: -3.622636 [821248/1237259]
loss: -3.620004 [1026048/1237259]
loss: -3.616770 [1230848/1237259]
Train Avg loss: -3.625231
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0558  
ndcg@20: 0.0455  
diversity: 0.2665  


Epoch 9
-------------------------------
loss: -3.638109 [ 2048/1237259]
loss: -3.631381 [206848/1237259]
loss: -3.630502 [411648/1237259]
loss: -3.625512 [616448/1237259]
loss: -3.623840 [821248/1237259]
loss: -3.618253 [1026048/1237259]
loss: -3.617455 [1230848/1237259]
Train Avg loss: -3.626094
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0557  
ndcg@20: 0.0458  
diversity: 0.2652  


Epoch 10
-------------------------------
loss: -3.635444 [ 2048/1237259]
loss: -3.633485 [206848/1237259]
loss: -3.626745 [411648/1237259]
loss: -3.629920 [616448/1237259]
loss: -3.623095 [821248/1237259]
loss: -3.621133 [1026048/1237259]
loss: -3.618040 [1230848/1237259]
Train Avg loss: -3.626812
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0561  
ndcg@20: 0.0460  
diversity: 0.2640  


Epoch 11
-------------------------------
loss: -3.636295 [ 2048/1237259]
loss: -3.632591 [206848/1237259]
loss: -3.629953 [411648/1237259]
loss: -3.628387 [616448/1237259]
loss: -3.624335 [821248/1237259]
loss: -3.622133 [1026048/1237259]
loss: -3.617680 [1230848/1237259]
Train Avg loss: -3.627451
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0569  
ndcg@20: 0.0466  
diversity: 0.2630  


Epoch 12
-------------------------------
loss: -3.638691 [ 2048/1237259]
loss: -3.632282 [206848/1237259]
loss: -3.630072 [411648/1237259]
loss: -3.626685 [616448/1237259]
loss: -3.626657 [821248/1237259]
loss: -3.624774 [1026048/1237259]
loss: -3.620824 [1230848/1237259]
Train Avg loss: -3.628001
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0571  
ndcg@20: 0.0468  
diversity: 0.2621  


Epoch 13
-------------------------------
loss: -3.638491 [ 2048/1237259]
loss: -3.633197 [206848/1237259]
loss: -3.631659 [411648/1237259]
loss: -3.628769 [616448/1237259]
loss: -3.623014 [821248/1237259]
loss: -3.623019 [1026048/1237259]
loss: -3.622364 [1230848/1237259]
Train Avg loss: -3.628529
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0577  
ndcg@20: 0.0474  
diversity: 0.2615  


Epoch 14
-------------------------------
loss: -3.635974 [ 2048/1237259]
loss: -3.634783 [206848/1237259]
loss: -3.633279 [411648/1237259]
loss: -3.631754 [616448/1237259]
loss: -3.625396 [821248/1237259]
loss: -3.624046 [1026048/1237259]
loss: -3.620736 [1230848/1237259]
Train Avg loss: -3.629045
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0581  
ndcg@20: 0.0477  
diversity: 0.2607  


Epoch 15
-------------------------------
loss: -3.636685 [ 2048/1237259]
loss: -3.635572 [206848/1237259]
loss: -3.631059 [411648/1237259]
loss: -3.629198 [616448/1237259]
loss: -3.626423 [821248/1237259]
loss: -3.626092 [1026048/1237259]
loss: -3.623535 [1230848/1237259]
Train Avg loss: -3.629499
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0581  
ndcg@20: 0.0477  
diversity: 0.2600  


Epoch 16
-------------------------------
loss: -3.640690 [ 2048/1237259]
loss: -3.633969 [206848/1237259]
loss: -3.633235 [411648/1237259]
loss: -3.630607 [616448/1237259]
loss: -3.626235 [821248/1237259]
loss: -3.624190 [1026048/1237259]
loss: -3.624041 [1230848/1237259]
Train Avg loss: -3.629863
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0583  
ndcg@20: 0.0478  
diversity: 0.2594  


Epoch 17
-------------------------------
loss: -3.640401 [ 2048/1237259]
loss: -3.634335 [206848/1237259]
loss: -3.631397 [411648/1237259]
loss: -3.628956 [616448/1237259]
loss: -3.625674 [821248/1237259]
loss: -3.625171 [1026048/1237259]
loss: -3.624024 [1230848/1237259]
Train Avg loss: -3.630300
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0586  
ndcg@20: 0.0481  
diversity: 0.2589  


Epoch 18
-------------------------------
loss: -3.640542 [ 2048/1237259]
loss: -3.634628 [206848/1237259]
loss: -3.630729 [411648/1237259]
loss: -3.630243 [616448/1237259]
loss: -3.628836 [821248/1237259]
loss: -3.624465 [1026048/1237259]
loss: -3.624981 [1230848/1237259]
Train Avg loss: -3.630598
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0588  
ndcg@20: 0.0482  
diversity: 0.2584  


Epoch 19
-------------------------------
loss: -3.636663 [ 2048/1237259]
loss: -3.636424 [206848/1237259]
loss: -3.634228 [411648/1237259]
loss: -3.629693 [616448/1237259]
loss: -3.627954 [821248/1237259]
loss: -3.627187 [1026048/1237259]
loss: -3.622604 [1230848/1237259]
Train Avg loss: -3.630908
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0590  
ndcg@20: 0.0485  
diversity: 0.2581  


Epoch 20
-------------------------------
loss: -3.638564 [ 2048/1237259]
loss: -3.633136 [206848/1237259]
loss: -3.634044 [411648/1237259]
loss: -3.632194 [616448/1237259]
loss: -3.628526 [821248/1237259]
loss: -3.627945 [1026048/1237259]
loss: -3.623434 [1230848/1237259]
Train Avg loss: -3.631221
Eval results: 
recall@20: 0.0588  
ndcg@20: 0.0485  
diversity: 0.2575  


Epoch 21
-------------------------------
loss: -3.639414 [ 2048/1237259]
loss: -3.635722 [206848/1237259]
loss: -3.632491 [411648/1237259]
loss: -3.630706 [616448/1237259]
loss: -3.626950 [821248/1237259]
loss: -3.627785 [1026048/1237259]
loss: -3.624125 [1230848/1237259]
Train Avg loss: -3.631494
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0591  
ndcg@20: 0.0487  
diversity: 0.2572  


Epoch 22
-------------------------------
loss: -3.640345 [ 2048/1237259]
loss: -3.635724 [206848/1237259]
loss: -3.634348 [411648/1237259]
loss: -3.630358 [616448/1237259]
loss: -3.627679 [821248/1237259]
loss: -3.627800 [1026048/1237259]
loss: -3.625019 [1230848/1237259]
Train Avg loss: -3.631718
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0593  
ndcg@20: 0.0488  
diversity: 0.2568  


Epoch 23
-------------------------------
loss: -3.640889 [ 2048/1237259]
loss: -3.635242 [206848/1237259]
loss: -3.632928 [411648/1237259]
loss: -3.632530 [616448/1237259]
loss: -3.630867 [821248/1237259]
loss: -3.626763 [1026048/1237259]
loss: -3.627998 [1230848/1237259]
Train Avg loss: -3.631979
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0593  
ndcg@20: 0.0489  
diversity: 0.2566  


Epoch 24
-------------------------------
loss: -3.640009 [ 2048/1237259]
loss: -3.633966 [206848/1237259]
loss: -3.634304 [411648/1237259]
loss: -3.632941 [616448/1237259]
loss: -3.629166 [821248/1237259]
loss: -3.628626 [1026048/1237259]
loss: -3.625681 [1230848/1237259]
Train Avg loss: -3.632199
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0595  
ndcg@20: 0.0490  
diversity: 0.2562  


Epoch 25
-------------------------------
loss: -3.641157 [ 2048/1237259]
loss: -3.635087 [206848/1237259]
loss: -3.632403 [411648/1237259]
loss: -3.633450 [616448/1237259]
loss: -3.630633 [821248/1237259]
loss: -3.627483 [1026048/1237259]
loss: -3.625474 [1230848/1237259]
Train Avg loss: -3.632410
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0594  
ndcg@20: 0.0490  
diversity: 0.2559  


Epoch 26
-------------------------------
loss: -3.639921 [ 2048/1237259]
loss: -3.635895 [206848/1237259]
loss: -3.632987 [411648/1237259]
loss: -3.629800 [616448/1237259]
loss: -3.630639 [821248/1237259]
loss: -3.629123 [1026048/1237259]
loss: -3.627028 [1230848/1237259]
Train Avg loss: -3.632612
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0595  
ndcg@20: 0.0490  
diversity: 0.2557  


Epoch 27
-------------------------------
loss: -3.639324 [ 2048/1237259]
loss: -3.636358 [206848/1237259]
loss: -3.635477 [411648/1237259]
loss: -3.634594 [616448/1237259]
loss: -3.631862 [821248/1237259]
loss: -3.628404 [1026048/1237259]
loss: -3.624830 [1230848/1237259]
Train Avg loss: -3.632775
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0596  
ndcg@20: 0.0492  
diversity: 0.2554  


Epoch 28
-------------------------------
loss: -3.639093 [ 2048/1237259]
loss: -3.636721 [206848/1237259]
loss: -3.633615 [411648/1237259]
loss: -3.633640 [616448/1237259]
loss: -3.630317 [821248/1237259]
loss: -3.629659 [1026048/1237259]
loss: -3.626091 [1230848/1237259]
Train Avg loss: -3.632975
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0601  
ndcg@20: 0.0495  
diversity: 0.2550  


Epoch 29
-------------------------------
loss: -3.636049 [ 2048/1237259]
loss: -3.635664 [206848/1237259]
loss: -3.635873 [411648/1237259]
loss: -3.634914 [616448/1237259]
loss: -3.631022 [821248/1237259]
loss: -3.628481 [1026048/1237259]
loss: -3.627556 [1230848/1237259]
Train Avg loss: -3.633138
Eval results: 
recall@20: 0.0598  
ndcg@20: 0.0493  
diversity: 0.2549  


Epoch 30
-------------------------------
loss: -3.640864 [ 2048/1237259]
loss: -3.638472 [206848/1237259]
loss: -3.637908 [411648/1237259]
loss: -3.631608 [616448/1237259]
loss: -3.628740 [821248/1237259]
loss: -3.628103 [1026048/1237259]
loss: -3.628073 [1230848/1237259]
Train Avg loss: -3.633283
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0601  
ndcg@20: 0.0495  
diversity: 0.2545  


Epoch 31
-------------------------------
loss: -3.640280 [ 2048/1237259]
loss: -3.636513 [206848/1237259]
loss: -3.634480 [411648/1237259]
loss: -3.635218 [616448/1237259]
loss: -3.629624 [821248/1237259]
loss: -3.629381 [1026048/1237259]
loss: -3.629300 [1230848/1237259]
Train Avg loss: -3.633442
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0600  
ndcg@20: 0.0495  
diversity: 0.2545  


Epoch 32
-------------------------------
loss: -3.639223 [ 2048/1237259]
loss: -3.637671 [206848/1237259]
loss: -3.633204 [411648/1237259]
loss: -3.635019 [616448/1237259]
loss: -3.632261 [821248/1237259]
loss: -3.630037 [1026048/1237259]
loss: -3.628322 [1230848/1237259]
Train Avg loss: -3.633564
Eval results: 
recall@20: 0.0599  
ndcg@20: 0.0494  
diversity: 0.2544  


Epoch 33
-------------------------------
loss: -3.637355 [ 2048/1237259]
loss: -3.637617 [206848/1237259]
loss: -3.635451 [411648/1237259]
loss: -3.634408 [616448/1237259]
loss: -3.632514 [821248/1237259]
loss: -3.630754 [1026048/1237259]
loss: -3.630454 [1230848/1237259]
Train Avg loss: -3.633711
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0601  
ndcg@20: 0.0495  
diversity: 0.2542  


Epoch 34
-------------------------------
loss: -3.639952 [ 2048/1237259]
loss: -3.638228 [206848/1237259]
loss: -3.638005 [411648/1237259]
loss: -3.632637 [616448/1237259]
loss: -3.632544 [821248/1237259]
loss: -3.631912 [1026048/1237259]
loss: -3.627282 [1230848/1237259]
Train Avg loss: -3.633838
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0600  
ndcg@20: 0.0495  
diversity: 0.2541  


Epoch 35
-------------------------------
loss: -3.639735 [ 2048/1237259]
loss: -3.638003 [206848/1237259]
loss: -3.635365 [411648/1237259]
loss: -3.632349 [616448/1237259]
loss: -3.634060 [821248/1237259]
loss: -3.632849 [1026048/1237259]
loss: -3.628130 [1230848/1237259]
Train Avg loss: -3.633950
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0602  
ndcg@20: 0.0495  
diversity: 0.2539  


Epoch 36
-------------------------------
loss: -3.638946 [ 2048/1237259]
loss: -3.636879 [206848/1237259]
loss: -3.635620 [411648/1237259]
loss: -3.631933 [616448/1237259]
loss: -3.630594 [821248/1237259]
loss: -3.629330 [1026048/1237259]
loss: -3.630022 [1230848/1237259]
Train Avg loss: -3.634040
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0603  
ndcg@20: 0.0497  
diversity: 0.2538  


Epoch 37
-------------------------------
loss: -3.640046 [ 2048/1237259]
loss: -3.636487 [206848/1237259]
loss: -3.633204 [411648/1237259]
loss: -3.632925 [616448/1237259]
loss: -3.633013 [821248/1237259]
loss: -3.629117 [1026048/1237259]
loss: -3.629895 [1230848/1237259]
Train Avg loss: -3.634177
Eval results: 
recall@20: 0.0601  
ndcg@20: 0.0496  
diversity: 0.2535  


Epoch 38
-------------------------------
loss: -3.641276 [ 2048/1237259]
loss: -3.637247 [206848/1237259]
loss: -3.634647 [411648/1237259]
loss: -3.631922 [616448/1237259]
loss: -3.631427 [821248/1237259]
loss: -3.628796 [1026048/1237259]
loss: -3.630597 [1230848/1237259]
Train Avg loss: -3.634198
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0602  
ndcg@20: 0.0497  
diversity: 0.2535  


Epoch 39
-------------------------------
loss: -3.637882 [ 2048/1237259]
loss: -3.636909 [206848/1237259]
loss: -3.638649 [411648/1237259]
loss: -3.636174 [616448/1237259]
loss: -3.633960 [821248/1237259]
loss: -3.628869 [1026048/1237259]
loss: -3.628113 [1230848/1237259]
Train Avg loss: -3.634342
Eval results: 
recall@20: 0.0602  
ndcg@20: 0.0496  
diversity: 0.2534  


Epoch 40
-------------------------------
loss: -3.639028 [ 2048/1237259]
loss: -3.638111 [206848/1237259]
loss: -3.635869 [411648/1237259]
loss: -3.635765 [616448/1237259]
loss: -3.634834 [821248/1237259]
loss: -3.631737 [1026048/1237259]
loss: -3.630747 [1230848/1237259]
Train Avg loss: -3.634454
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0606  
ndcg@20: 0.0499  
diversity: 0.2531  


Epoch 41
-------------------------------
loss: -3.638705 [ 2048/1237259]
loss: -3.640749 [206848/1237259]
loss: -3.636518 [411648/1237259]
loss: -3.635486 [616448/1237259]
loss: -3.635432 [821248/1237259]
loss: -3.629604 [1026048/1237259]
loss: -3.631006 [1230848/1237259]
Train Avg loss: -3.634517
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0606  
ndcg@20: 0.0499  
diversity: 0.2531  


Epoch 42
-------------------------------
loss: -3.638836 [ 2048/1237259]
loss: -3.638087 [206848/1237259]
loss: -3.634896 [411648/1237259]
loss: -3.634191 [616448/1237259]
loss: -3.632907 [821248/1237259]
loss: -3.629690 [1026048/1237259]
loss: -3.630871 [1230848/1237259]
Train Avg loss: -3.634694
Eval results: 
recall@20: 0.0604  
ndcg@20: 0.0498  
diversity: 0.2530  


Epoch 43
-------------------------------
loss: -3.641042 [ 2048/1237259]
loss: -3.635349 [206848/1237259]
loss: -3.633522 [411648/1237259]
loss: -3.637011 [616448/1237259]
loss: -3.632461 [821248/1237259]
loss: -3.633509 [1026048/1237259]
loss: -3.632094 [1230848/1237259]
Train Avg loss: -3.634712
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0500  
diversity: 0.2528  


Epoch 44
-------------------------------
loss: -3.640748 [ 2048/1237259]
loss: -3.636987 [206848/1237259]
loss: -3.636288 [411648/1237259]
loss: -3.634158 [616448/1237259]
loss: -3.633034 [821248/1237259]
loss: -3.631241 [1026048/1237259]
loss: -3.630894 [1230848/1237259]
Train Avg loss: -3.634801
Eval results: 
recall@20: 0.0605  
ndcg@20: 0.0499  
diversity: 0.2526  


Epoch 45
-------------------------------
loss: -3.642134 [ 2048/1237259]
loss: -3.636854 [206848/1237259]
loss: -3.635661 [411648/1237259]
loss: -3.633159 [616448/1237259]
loss: -3.632888 [821248/1237259]
loss: -3.630898 [1026048/1237259]
loss: -3.627346 [1230848/1237259]
Train Avg loss: -3.634842
Eval results: 
recall@20: 0.0606  
ndcg@20: 0.0499  
diversity: 0.2525  


Epoch 46
-------------------------------
loss: -3.639798 [ 2048/1237259]
loss: -3.641034 [206848/1237259]
loss: -3.635175 [411648/1237259]
loss: -3.634261 [616448/1237259]
loss: -3.630378 [821248/1237259]
loss: -3.630046 [1026048/1237259]
loss: -3.630973 [1230848/1237259]
Train Avg loss: -3.634939
Eval results: 
recall@20: 0.0605  
ndcg@20: 0.0499  
diversity: 0.2524  


Epoch 47
-------------------------------
loss: -3.637329 [ 2048/1237259]
loss: -3.638135 [206848/1237259]
loss: -3.636702 [411648/1237259]
loss: -3.636730 [616448/1237259]
loss: -3.633576 [821248/1237259]
loss: -3.632187 [1026048/1237259]
loss: -3.627111 [1230848/1237259]
Train Avg loss: -3.635011
Eval results: 
recall@20: 0.0606  
ndcg@20: 0.0499  
diversity: 0.2524  


Epoch 48
-------------------------------
loss: -3.638208 [ 2048/1237259]
loss: -3.640505 [206848/1237259]
loss: -3.636930 [411648/1237259]
loss: -3.638650 [616448/1237259]
loss: -3.634488 [821248/1237259]
loss: -3.632293 [1026048/1237259]
loss: -3.632678 [1230848/1237259]
Train Avg loss: -3.635120
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0501  
diversity: 0.2523  


Epoch 49
-------------------------------
loss: -3.640965 [ 2048/1237259]
loss: -3.636439 [206848/1237259]
loss: -3.637564 [411648/1237259]
loss: -3.634014 [616448/1237259]
loss: -3.632604 [821248/1237259]
loss: -3.631233 [1026048/1237259]
loss: -3.630407 [1230848/1237259]
Train Avg loss: -3.635157
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0501  
diversity: 0.2522  


Epoch 50
-------------------------------
loss: -3.640302 [ 2048/1237259]
loss: -3.639875 [206848/1237259]
loss: -3.635129 [411648/1237259]
loss: -3.636541 [616448/1237259]
loss: -3.632865 [821248/1237259]
loss: -3.630176 [1026048/1237259]
loss: -3.632632 [1230848/1237259]
Train Avg loss: -3.635238
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0501  
diversity: 0.2520  


Epoch 51
-------------------------------
loss: -3.639546 [ 2048/1237259]
loss: -3.636342 [206848/1237259]
loss: -3.636627 [411648/1237259]
loss: -3.634979 [616448/1237259]
loss: -3.632001 [821248/1237259]
loss: -3.633689 [1026048/1237259]
loss: -3.630418 [1230848/1237259]
Train Avg loss: -3.635270
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0501  
diversity: 0.2519  


Epoch 52
-------------------------------
loss: -3.641076 [ 2048/1237259]
loss: -3.636744 [206848/1237259]
loss: -3.636154 [411648/1237259]
loss: -3.631115 [616448/1237259]
loss: -3.633916 [821248/1237259]
loss: -3.628657 [1026048/1237259]
loss: -3.632158 [1230848/1237259]
Train Avg loss: -3.635341
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0501  
diversity: 0.2518  


Epoch 53
-------------------------------
loss: -3.637954 [ 2048/1237259]
loss: -3.639385 [206848/1237259]
loss: -3.637331 [411648/1237259]
loss: -3.635948 [616448/1237259]
loss: -3.632379 [821248/1237259]
loss: -3.631967 [1026048/1237259]
loss: -3.630942 [1230848/1237259]
Train Avg loss: -3.635410
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0501  
diversity: 0.2517  


Epoch 54
-------------------------------
loss: -3.642270 [ 2048/1237259]
loss: -3.635556 [206848/1237259]
loss: -3.636781 [411648/1237259]
loss: -3.635241 [616448/1237259]
loss: -3.634746 [821248/1237259]
loss: -3.633255 [1026048/1237259]
loss: -3.632794 [1230848/1237259]
Train Avg loss: -3.635472
Eval results: 
recall@20: 0.0607  
ndcg@20: 0.0500  
diversity: 0.2517  


Epoch 55
-------------------------------
loss: -3.639456 [ 2048/1237259]
loss: -3.638564 [206848/1237259]
loss: -3.635847 [411648/1237259]
loss: -3.638150 [616448/1237259]
loss: -3.634426 [821248/1237259]
loss: -3.632428 [1026048/1237259]
loss: -3.630171 [1230848/1237259]
Train Avg loss: -3.635527
Eval results: 
recall@20: 0.0606  
ndcg@20: 0.0500  
diversity: 0.2516  


Epoch 56
-------------------------------
loss: -3.641727 [ 2048/1237259]
loss: -3.640899 [206848/1237259]
loss: -3.636232 [411648/1237259]
loss: -3.633642 [616448/1237259]
loss: -3.632497 [821248/1237259]
loss: -3.630530 [1026048/1237259]
loss: -3.631835 [1230848/1237259]
Train Avg loss: -3.635582
Eval results: 
recall@20: 0.0607  
ndcg@20: 0.0499  
diversity: 0.2516  


Epoch 57
-------------------------------
loss: -3.641519 [ 2048/1237259]
loss: -3.636475 [206848/1237259]
loss: -3.638303 [411648/1237259]
loss: -3.637297 [616448/1237259]
loss: -3.635142 [821248/1237259]
loss: -3.632940 [1026048/1237259]
loss: -3.631647 [1230848/1237259]
Train Avg loss: -3.635629
Eval results: 
recall@20: 0.0605  
ndcg@20: 0.0499  
diversity: 0.2514  


Epoch 58
-------------------------------
loss: -3.641245 [ 2048/1237259]
loss: -3.637406 [206848/1237259]
loss: -3.636395 [411648/1237259]
loss: -3.634480 [616448/1237259]
loss: -3.633604 [821248/1237259]
loss: -3.633392 [1026048/1237259]
loss: -3.632346 [1230848/1237259]
Train Avg loss: -3.635672
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0500  
diversity: 0.2514  


Epoch 59
-------------------------------
loss: -3.641162 [ 2048/1237259]
loss: -3.640709 [206848/1237259]
loss: -3.637309 [411648/1237259]
loss: -3.637999 [616448/1237259]
loss: -3.632472 [821248/1237259]
loss: -3.635662 [1026048/1237259]
loss: -3.631710 [1230848/1237259]
Train Avg loss: -3.635740
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0609  
ndcg@20: 0.0502  
diversity: 0.2513  


Epoch 60
-------------------------------
loss: -3.638104 [ 2048/1237259]
loss: -3.639638 [206848/1237259]
loss: -3.634201 [411648/1237259]
loss: -3.634161 [616448/1237259]
loss: -3.637111 [821248/1237259]
loss: -3.631359 [1026048/1237259]
loss: -3.631431 [1230848/1237259]
Train Avg loss: -3.635776
Eval results: 
recall@20: 0.0607  
ndcg@20: 0.0500  
diversity: 0.2512  


Epoch 61
-------------------------------
loss: -3.641122 [ 2048/1237259]
loss: -3.638403 [206848/1237259]
loss: -3.636469 [411648/1237259]
loss: -3.636508 [616448/1237259]
loss: -3.633660 [821248/1237259]
loss: -3.632897 [1026048/1237259]
loss: -3.629841 [1230848/1237259]
Train Avg loss: -3.635832
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0502  
diversity: 0.2510  


Epoch 62
-------------------------------
loss: -3.641732 [ 2048/1237259]
loss: -3.639559 [206848/1237259]
loss: -3.635903 [411648/1237259]
loss: -3.636547 [616448/1237259]
loss: -3.635599 [821248/1237259]
loss: -3.634323 [1026048/1237259]
loss: -3.632172 [1230848/1237259]
Train Avg loss: -3.635847
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0501  
diversity: 0.2511  


Epoch 63
-------------------------------
loss: -3.638160 [ 2048/1237259]
loss: -3.638062 [206848/1237259]
loss: -3.636224 [411648/1237259]
loss: -3.633764 [616448/1237259]
loss: -3.632987 [821248/1237259]
loss: -3.633341 [1026048/1237259]
loss: -3.633230 [1230848/1237259]
Train Avg loss: -3.635894
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0503  
diversity: 0.2509  


Epoch 64
-------------------------------
loss: -3.638954 [ 2048/1237259]
loss: -3.637640 [206848/1237259]
loss: -3.639780 [411648/1237259]
loss: -3.637313 [616448/1237259]
loss: -3.634522 [821248/1237259]
loss: -3.634587 [1026048/1237259]
loss: -3.632426 [1230848/1237259]
Train Avg loss: -3.635979
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0502  
diversity: 0.2509  


Epoch 65
-------------------------------
loss: -3.638944 [ 2048/1237259]
loss: -3.640933 [206848/1237259]
loss: -3.638414 [411648/1237259]
loss: -3.636235 [616448/1237259]
loss: -3.637681 [821248/1237259]
loss: -3.635198 [1026048/1237259]
loss: -3.631517 [1230848/1237259]
Train Avg loss: -3.635991
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0609  
ndcg@20: 0.0503  
diversity: 0.2508  


Epoch 66
-------------------------------
loss: -3.639434 [ 2048/1237259]
loss: -3.639742 [206848/1237259]
loss: -3.638765 [411648/1237259]
loss: -3.634615 [616448/1237259]
loss: -3.635127 [821248/1237259]
loss: -3.637239 [1026048/1237259]
loss: -3.632110 [1230848/1237259]
Train Avg loss: -3.636052
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0501  
diversity: 0.2508  


Epoch 67
-------------------------------
loss: -3.641693 [ 2048/1237259]
loss: -3.637826 [206848/1237259]
loss: -3.641664 [411648/1237259]
loss: -3.636532 [616448/1237259]
loss: -3.634654 [821248/1237259]
loss: -3.634287 [1026048/1237259]
loss: -3.633006 [1230848/1237259]
Train Avg loss: -3.636077
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0502  
diversity: 0.2508  


Epoch 68
-------------------------------
loss: -3.639443 [ 2048/1237259]
loss: -3.639480 [206848/1237259]
loss: -3.637514 [411648/1237259]
loss: -3.635459 [616448/1237259]
loss: -3.634187 [821248/1237259]
loss: -3.632806 [1026048/1237259]
loss: -3.633280 [1230848/1237259]
Train Avg loss: -3.636125
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0503  
diversity: 0.2506  


Epoch 69
-------------------------------
loss: -3.638166 [ 2048/1237259]
loss: -3.638422 [206848/1237259]
loss: -3.634774 [411648/1237259]
loss: -3.635084 [616448/1237259]
loss: -3.634106 [821248/1237259]
loss: -3.632177 [1026048/1237259]
loss: -3.633093 [1230848/1237259]
Train Avg loss: -3.636166
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0502  
diversity: 0.2506  


Epoch 70
-------------------------------
loss: -3.641124 [ 2048/1237259]
loss: -3.638838 [206848/1237259]
loss: -3.637593 [411648/1237259]
loss: -3.636469 [616448/1237259]
loss: -3.633130 [821248/1237259]
loss: -3.632708 [1026048/1237259]
loss: -3.634555 [1230848/1237259]
Train Avg loss: -3.636239
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0503  
diversity: 0.2505  


Epoch 71
-------------------------------
loss: -3.640825 [ 2048/1237259]
loss: -3.640982 [206848/1237259]
loss: -3.635607 [411648/1237259]
loss: -3.635662 [616448/1237259]
loss: -3.635676 [821248/1237259]
loss: -3.634138 [1026048/1237259]
loss: -3.632917 [1230848/1237259]
Train Avg loss: -3.636266
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0503  
diversity: 0.2505  


Epoch 72
-------------------------------
loss: -3.640791 [ 2048/1237259]
loss: -3.639312 [206848/1237259]
loss: -3.638287 [411648/1237259]
loss: -3.637498 [616448/1237259]
loss: -3.636302 [821248/1237259]
loss: -3.634078 [1026048/1237259]
loss: -3.633337 [1230848/1237259]
Train Avg loss: -3.636297
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0505  
diversity: 0.2504  


Epoch 73
-------------------------------
loss: -3.638107 [ 2048/1237259]
loss: -3.639892 [206848/1237259]
loss: -3.637503 [411648/1237259]
loss: -3.635978 [616448/1237259]
loss: -3.633836 [821248/1237259]
loss: -3.633615 [1026048/1237259]
loss: -3.634647 [1230848/1237259]
Train Avg loss: -3.636319
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0503  
diversity: 0.2503  


Epoch 74
-------------------------------
loss: -3.639968 [ 2048/1237259]
loss: -3.636642 [206848/1237259]
loss: -3.638517 [411648/1237259]
loss: -3.636635 [616448/1237259]
loss: -3.635780 [821248/1237259]
loss: -3.633344 [1026048/1237259]
loss: -3.636428 [1230848/1237259]
Train Avg loss: -3.636388
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0503  
diversity: 0.2503  


Epoch 75
-------------------------------
loss: -3.641708 [ 2048/1237259]
loss: -3.638743 [206848/1237259]
loss: -3.637477 [411648/1237259]
loss: -3.636135 [616448/1237259]
loss: -3.635370 [821248/1237259]
loss: -3.633796 [1026048/1237259]
loss: -3.631147 [1230848/1237259]
Train Avg loss: -3.636396
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0501  
diversity: 0.2502  


Epoch 76
-------------------------------
loss: -3.639107 [ 2048/1237259]
loss: -3.640759 [206848/1237259]
loss: -3.637294 [411648/1237259]
loss: -3.637062 [616448/1237259]
loss: -3.634647 [821248/1237259]
loss: -3.634281 [1026048/1237259]
loss: -3.632457 [1230848/1237259]
Train Avg loss: -3.636441
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0503  
diversity: 0.2501  


Epoch 77
-------------------------------
loss: -3.641736 [ 2048/1237259]
loss: -3.641834 [206848/1237259]
loss: -3.637202 [411648/1237259]
loss: -3.634860 [616448/1237259]
loss: -3.634666 [821248/1237259]
loss: -3.636787 [1026048/1237259]
loss: -3.632271 [1230848/1237259]
Train Avg loss: -3.636474
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0502  
diversity: 0.2501  


Epoch 78
-------------------------------
loss: -3.640829 [ 2048/1237259]
loss: -3.636991 [206848/1237259]
loss: -3.638341 [411648/1237259]
loss: -3.633971 [616448/1237259]
loss: -3.635128 [821248/1237259]
loss: -3.634108 [1026048/1237259]
loss: -3.633363 [1230848/1237259]
Train Avg loss: -3.636506
Eval results: 
recall@20: 0.0609  
ndcg@20: 0.0502  
diversity: 0.2500  


Epoch 79
-------------------------------
loss: -3.641117 [ 2048/1237259]
loss: -3.637394 [206848/1237259]
loss: -3.639776 [411648/1237259]
loss: -3.634795 [616448/1237259]
loss: -3.636193 [821248/1237259]
loss: -3.633904 [1026048/1237259]
loss: -3.634510 [1230848/1237259]
Train Avg loss: -3.636549
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0503  
diversity: 0.2500  


Epoch 80
-------------------------------
loss: -3.641856 [ 2048/1237259]
loss: -3.636651 [206848/1237259]
loss: -3.637912 [411648/1237259]
loss: -3.637853 [616448/1237259]
loss: -3.634749 [821248/1237259]
loss: -3.634804 [1026048/1237259]
loss: -3.633744 [1230848/1237259]
Train Avg loss: -3.636579
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0503  
diversity: 0.2499  


Epoch 81
-------------------------------
loss: -3.638861 [ 2048/1237259]
loss: -3.637503 [206848/1237259]
loss: -3.642209 [411648/1237259]
loss: -3.637046 [616448/1237259]
loss: -3.637319 [821248/1237259]
loss: -3.632617 [1026048/1237259]
loss: -3.632537 [1230848/1237259]
Train Avg loss: -3.636568
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0503  
diversity: 0.2499  


Epoch 82
-------------------------------
loss: -3.638115 [ 2048/1237259]
loss: -3.638802 [206848/1237259]
loss: -3.639816 [411648/1237259]
loss: -3.635323 [616448/1237259]
loss: -3.633567 [821248/1237259]
loss: -3.634519 [1026048/1237259]
loss: -3.633102 [1230848/1237259]
Train Avg loss: -3.636619
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0503  
diversity: 0.2499  


Epoch 83
-------------------------------
loss: -3.641579 [ 2048/1237259]
loss: -3.638904 [206848/1237259]
loss: -3.637482 [411648/1237259]
loss: -3.637132 [616448/1237259]
loss: -3.635406 [821248/1237259]
loss: -3.632015 [1026048/1237259]
loss: -3.633885 [1230848/1237259]
Train Avg loss: -3.636710
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0616  
ndcg@20: 0.0505  
diversity: 0.2498  


Epoch 84
-------------------------------
loss: -3.640169 [ 2048/1237259]
loss: -3.638096 [206848/1237259]
loss: -3.635483 [411648/1237259]
loss: -3.636594 [616448/1237259]
loss: -3.634861 [821248/1237259]
loss: -3.632982 [1026048/1237259]
loss: -3.634981 [1230848/1237259]
Train Avg loss: -3.636656
Eval results: 
recall@20: 0.0615  
ndcg@20: 0.0505  
diversity: 0.2498  


Epoch 85
-------------------------------
loss: -3.638029 [ 2048/1237259]
loss: -3.639702 [206848/1237259]
loss: -3.636729 [411648/1237259]
loss: -3.636984 [616448/1237259]
loss: -3.635403 [821248/1237259]
loss: -3.636409 [1026048/1237259]
loss: -3.632719 [1230848/1237259]
Train Avg loss: -3.636689
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0504  
diversity: 0.2497  


Epoch 86
-------------------------------
loss: -3.641200 [ 2048/1237259]
loss: -3.638626 [206848/1237259]
loss: -3.638580 [411648/1237259]
loss: -3.635163 [616448/1237259]
loss: -3.636446 [821248/1237259]
loss: -3.634420 [1026048/1237259]
loss: -3.636065 [1230848/1237259]
Train Avg loss: -3.636741
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0504  
diversity: 0.2497  


Epoch 87
-------------------------------
loss: -3.640974 [ 2048/1237259]
loss: -3.638915 [206848/1237259]
loss: -3.637364 [411648/1237259]
loss: -3.638464 [616448/1237259]
loss: -3.638197 [821248/1237259]
loss: -3.633812 [1026048/1237259]
loss: -3.633870 [1230848/1237259]
Train Avg loss: -3.636788
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0504  
diversity: 0.2496  


Epoch 88
-------------------------------
loss: -3.639896 [ 2048/1237259]
loss: -3.639132 [206848/1237259]
loss: -3.638704 [411648/1237259]
loss: -3.635132 [616448/1237259]
loss: -3.636393 [821248/1237259]
loss: -3.637016 [1026048/1237259]
loss: -3.634461 [1230848/1237259]
Train Avg loss: -3.636770
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0504  
diversity: 0.2496  


Epoch 89
-------------------------------
loss: -3.641521 [ 2048/1237259]
loss: -3.638527 [206848/1237259]
loss: -3.636575 [411648/1237259]
loss: -3.635818 [616448/1237259]
loss: -3.635176 [821248/1237259]
loss: -3.633965 [1026048/1237259]
loss: -3.632275 [1230848/1237259]
Train Avg loss: -3.636761
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0504  
diversity: 0.2494  


Epoch 90
-------------------------------
loss: -3.640268 [ 2048/1237259]
loss: -3.638464 [206848/1237259]
loss: -3.640319 [411648/1237259]
loss: -3.636531 [616448/1237259]
loss: -3.634232 [821248/1237259]
loss: -3.634701 [1026048/1237259]
loss: -3.633278 [1230848/1237259]
Train Avg loss: -3.636843
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0504  
diversity: 0.2496  


Epoch 91
-------------------------------
loss: -3.642280 [ 2048/1237259]
loss: -3.640671 [206848/1237259]
loss: -3.639788 [411648/1237259]
loss: -3.637784 [616448/1237259]
loss: -3.634809 [821248/1237259]
loss: -3.632856 [1026048/1237259]
loss: -3.633434 [1230848/1237259]
Train Avg loss: -3.636875
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0504  
diversity: 0.2494  


Epoch 92
-------------------------------
loss: -3.639647 [ 2048/1237259]
loss: -3.640527 [206848/1237259]
loss: -3.636597 [411648/1237259]
loss: -3.638525 [616448/1237259]
loss: -3.635501 [821248/1237259]
loss: -3.636909 [1026048/1237259]
loss: -3.634350 [1230848/1237259]
Train Avg loss: -3.636880
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0505  
diversity: 0.2494  


Epoch 93
-------------------------------
loss: -3.644202 [ 2048/1237259]
loss: -3.640869 [206848/1237259]
loss: -3.637692 [411648/1237259]
loss: -3.638268 [616448/1237259]
loss: -3.636663 [821248/1237259]
loss: -3.632758 [1026048/1237259]
loss: -3.632655 [1230848/1237259]
Train Avg loss: -3.636933
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0505  
diversity: 0.2494  


Epoch 94
-------------------------------
loss: -3.636970 [ 2048/1237259]
loss: -3.638638 [206848/1237259]
loss: -3.637208 [411648/1237259]
loss: -3.637980 [616448/1237259]
loss: -3.636109 [821248/1237259]
loss: -3.633834 [1026048/1237259]
loss: -3.631791 [1230848/1237259]
Train Avg loss: -3.636955
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0505  
diversity: 0.2493  


Epoch 95
-------------------------------
loss: -3.639054 [ 2048/1237259]
loss: -3.639888 [206848/1237259]
loss: -3.633012 [411648/1237259]
loss: -3.634159 [616448/1237259]
loss: -3.634721 [821248/1237259]
loss: -3.635513 [1026048/1237259]
loss: -3.634940 [1230848/1237259]
Train Avg loss: -3.636912
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0504  
diversity: 0.2492  


Epoch 96
-------------------------------
loss: -3.640925 [ 2048/1237259]
loss: -3.640425 [206848/1237259]
loss: -3.639403 [411648/1237259]
loss: -3.635587 [616448/1237259]
loss: -3.637286 [821248/1237259]
loss: -3.637425 [1026048/1237259]
loss: -3.637356 [1230848/1237259]
Train Avg loss: -3.636983
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0505  
diversity: 0.2492  


Epoch 97
-------------------------------
loss: -3.641168 [ 2048/1237259]
loss: -3.637536 [206848/1237259]
loss: -3.639523 [411648/1237259]
loss: -3.637321 [616448/1237259]
loss: -3.635749 [821248/1237259]
loss: -3.633526 [1026048/1237259]
loss: -3.634573 [1230848/1237259]
Train Avg loss: -3.636959
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0504  
diversity: 0.2492  


Epoch 98
-------------------------------
loss: -3.640501 [ 2048/1237259]
loss: -3.639169 [206848/1237259]
loss: -3.640259 [411648/1237259]
loss: -3.635833 [616448/1237259]
loss: -3.637140 [821248/1237259]
loss: -3.633487 [1026048/1237259]
loss: -3.633717 [1230848/1237259]
Train Avg loss: -3.637013
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0505  
diversity: 0.2491  


Epoch 99
-------------------------------
loss: -3.640432 [ 2048/1237259]
loss: -3.641851 [206848/1237259]
loss: -3.637653 [411648/1237259]
loss: -3.634485 [616448/1237259]
loss: -3.636211 [821248/1237259]
loss: -3.635404 [1026048/1237259]
loss: -3.633982 [1230848/1237259]
Train Avg loss: -3.637041
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0504  
diversity: 0.2491  


Epoch 100
-------------------------------
loss: -3.642813 [ 2048/1237259]
loss: -3.638235 [206848/1237259]
loss: -3.639685 [411648/1237259]
loss: -3.636591 [616448/1237259]
loss: -3.636801 [821248/1237259]
loss: -3.633266 [1026048/1237259]
loss: -3.633801 [1230848/1237259]
Train Avg loss: -3.637058
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0505  
diversity: 0.2491  


