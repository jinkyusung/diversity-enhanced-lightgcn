Folder created at: ./our-layer3/best_model
Your Device: cuda
Yelp2018

#user = 31668
#item = 38048

#interactions
    (train) 1237259
    (test)  324147
    (total) 1561406

Sparsity = 0.0012958757851778645

Epoch 0
-------------------------------
loss: -3.475339 [ 2048/1237259]
loss: -3.559721 [206848/1237259]
loss: -3.575737 [411648/1237259]
loss: -3.579585 [616448/1237259]
loss: -3.580129 [821248/1237259]
loss: -3.583101 [1026048/1237259]
loss: -3.583256 [1230848/1237259]
train_time_one_epoch = 94.81055402755737
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0437  
ndcg@20: 0.0355  
diversity: 0.2847  


Epoch 1
-------------------------------
loss: -3.622786 [ 2048/1237259]
loss: -3.617176 [206848/1237259]
loss: -3.608707 [411648/1237259]
loss: -3.604105 [616448/1237259]
loss: -3.605844 [821248/1237259]
loss: -3.602162 [1026048/1237259]
loss: -3.599506 [1230848/1237259]
train_time_one_epoch = 94.93092679977417
Epoch 2
-------------------------------
loss: -3.631003 [ 2048/1237259]
loss: -3.625402 [206848/1237259]
loss: -3.615576 [411648/1237259]
loss: -3.618100 [616448/1237259]
loss: -3.606540 [821248/1237259]
loss: -3.608773 [1026048/1237259]
loss: -3.607064 [1230848/1237259]
train_time_one_epoch = 95.39095044136047
Epoch 3
-------------------------------
loss: -3.634651 [ 2048/1237259]
loss: -3.628018 [206848/1237259]
loss: -3.623711 [411648/1237259]
loss: -3.617378 [616448/1237259]
loss: -3.616552 [821248/1237259]
loss: -3.611719 [1026048/1237259]
loss: -3.609516 [1230848/1237259]
train_time_one_epoch = 95.54251551628113
Epoch 4
-------------------------------
loss: -3.636970 [ 2048/1237259]
loss: -3.630568 [206848/1237259]
loss: -3.626671 [411648/1237259]
loss: -3.621205 [616448/1237259]
loss: -3.615333 [821248/1237259]
loss: -3.614862 [1026048/1237259]
loss: -3.608740 [1230848/1237259]
train_time_one_epoch = 95.54265594482422
Epoch 5
-------------------------------
loss: -3.635164 [ 2048/1237259]
loss: -3.627419 [206848/1237259]
loss: -3.628399 [411648/1237259]
loss: -3.622572 [616448/1237259]
loss: -3.619557 [821248/1237259]
loss: -3.615807 [1026048/1237259]
loss: -3.615927 [1230848/1237259]
train_time_one_epoch = 95.48603749275208
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0547  
ndcg@20: 0.0445  
diversity: 0.2679  


Epoch 6
-------------------------------
loss: -3.634525 [ 2048/1237259]
loss: -3.631352 [206848/1237259]
loss: -3.628333 [411648/1237259]
loss: -3.622472 [616448/1237259]
loss: -3.619731 [821248/1237259]
loss: -3.616241 [1026048/1237259]
loss: -3.615927 [1230848/1237259]
train_time_one_epoch = 95.09262418746948
Epoch 7
-------------------------------
loss: -3.637872 [ 2048/1237259]
loss: -3.632655 [206848/1237259]
loss: -3.627265 [411648/1237259]
loss: -3.623630 [616448/1237259]
loss: -3.622525 [821248/1237259]
loss: -3.619366 [1026048/1237259]
loss: -3.617748 [1230848/1237259]
train_time_one_epoch = 95.42807030677795
Epoch 8
-------------------------------
loss: -3.639400 [ 2048/1237259]
loss: -3.633192 [206848/1237259]
loss: -3.631054 [411648/1237259]
loss: -3.625778 [616448/1237259]
loss: -3.623915 [821248/1237259]
loss: -3.619437 [1026048/1237259]
loss: -3.616507 [1230848/1237259]
train_time_one_epoch = 95.53480434417725
Epoch 9
-------------------------------
loss: -3.635490 [ 2048/1237259]
loss: -3.634488 [206848/1237259]
loss: -3.630038 [411648/1237259]
loss: -3.626268 [616448/1237259]
loss: -3.624214 [821248/1237259]
loss: -3.619942 [1026048/1237259]
loss: -3.618507 [1230848/1237259]
train_time_one_epoch = 95.4439799785614
Epoch 10
-------------------------------
loss: -3.640356 [ 2048/1237259]
loss: -3.632738 [206848/1237259]
loss: -3.628774 [411648/1237259]
loss: -3.628471 [616448/1237259]
loss: -3.620362 [821248/1237259]
loss: -3.621070 [1026048/1237259]
loss: -3.620444 [1230848/1237259]
train_time_one_epoch = 95.49507761001587
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0572  
ndcg@20: 0.0469  
diversity: 0.2619  


Epoch 11
-------------------------------
loss: -3.640082 [ 2048/1237259]
loss: -3.635632 [206848/1237259]
loss: -3.630770 [411648/1237259]
loss: -3.627054 [616448/1237259]
loss: -3.625025 [821248/1237259]
loss: -3.620629 [1026048/1237259]
loss: -3.624717 [1230848/1237259]
train_time_one_epoch = 95.05454063415527
Epoch 12
-------------------------------
loss: -3.636724 [ 2048/1237259]
loss: -3.634522 [206848/1237259]
loss: -3.632523 [411648/1237259]
loss: -3.628011 [616448/1237259]
loss: -3.626794 [821248/1237259]
loss: -3.623742 [1026048/1237259]
loss: -3.618766 [1230848/1237259]
train_time_one_epoch = 95.51412582397461
Epoch 13
-------------------------------
loss: -3.635886 [ 2048/1237259]
loss: -3.636800 [206848/1237259]
loss: -3.631910 [411648/1237259]
loss: -3.626767 [616448/1237259]
loss: -3.626166 [821248/1237259]
loss: -3.625167 [1026048/1237259]
loss: -3.623621 [1230848/1237259]
train_time_one_epoch = 95.36878895759583
Epoch 14
-------------------------------
loss: -3.636959 [ 2048/1237259]
loss: -3.633036 [206848/1237259]
loss: -3.633465 [411648/1237259]
loss: -3.630148 [616448/1237259]
loss: -3.624397 [821248/1237259]
loss: -3.623632 [1026048/1237259]
loss: -3.621300 [1230848/1237259]
train_time_one_epoch = 95.4608645439148
Epoch 15
-------------------------------
loss: -3.638634 [ 2048/1237259]
loss: -3.636529 [206848/1237259]
loss: -3.635130 [411648/1237259]
loss: -3.629314 [616448/1237259]
loss: -3.629646 [821248/1237259]
loss: -3.624823 [1026048/1237259]
loss: -3.624413 [1230848/1237259]
train_time_one_epoch = 95.41603207588196
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0589  
ndcg@20: 0.0483  
diversity: 0.2585  


Epoch 16
-------------------------------
loss: -3.637203 [ 2048/1237259]
loss: -3.636142 [206848/1237259]
loss: -3.633964 [411648/1237259]
loss: -3.630435 [616448/1237259]
loss: -3.627927 [821248/1237259]
loss: -3.628193 [1026048/1237259]
loss: -3.623376 [1230848/1237259]
train_time_one_epoch = 94.98382019996643
Epoch 17
-------------------------------
loss: -3.641869 [ 2048/1237259]
loss: -3.635345 [206848/1237259]
loss: -3.633083 [411648/1237259]
loss: -3.629049 [616448/1237259]
loss: -3.628632 [821248/1237259]
loss: -3.623934 [1026048/1237259]
loss: -3.626485 [1230848/1237259]
train_time_one_epoch = 95.49439811706543
Epoch 18
-------------------------------
loss: -3.638660 [ 2048/1237259]
loss: -3.634978 [206848/1237259]
loss: -3.634020 [411648/1237259]
loss: -3.631581 [616448/1237259]
loss: -3.631335 [821248/1237259]
loss: -3.625213 [1026048/1237259]
loss: -3.627018 [1230848/1237259]
train_time_one_epoch = 95.50069451332092
Epoch 19
-------------------------------
loss: -3.635643 [ 2048/1237259]
loss: -3.636324 [206848/1237259]
loss: -3.633704 [411648/1237259]
loss: -3.631832 [616448/1237259]
loss: -3.629218 [821248/1237259]
loss: -3.624778 [1026048/1237259]
loss: -3.625199 [1230848/1237259]
train_time_one_epoch = 95.4422857761383
Epoch 20
-------------------------------
loss: -3.639373 [ 2048/1237259]
loss: -3.636806 [206848/1237259]
loss: -3.634064 [411648/1237259]
loss: -3.634858 [616448/1237259]
loss: -3.628046 [821248/1237259]
loss: -3.625892 [1026048/1237259]
loss: -3.627930 [1230848/1237259]
train_time_one_epoch = 95.35487818717957
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0595  
ndcg@20: 0.0488  
diversity: 0.2565  


Epoch 21
-------------------------------
loss: -3.638743 [ 2048/1237259]
loss: -3.636712 [206848/1237259]
loss: -3.633085 [411648/1237259]
loss: -3.632910 [616448/1237259]
loss: -3.628322 [821248/1237259]
loss: -3.628989 [1026048/1237259]
loss: -3.624393 [1230848/1237259]
train_time_one_epoch = 95.00419330596924
Epoch 22
-------------------------------
loss: -3.640528 [ 2048/1237259]
loss: -3.638490 [206848/1237259]
loss: -3.632508 [411648/1237259]
loss: -3.631904 [616448/1237259]
loss: -3.629963 [821248/1237259]
loss: -3.629414 [1026048/1237259]
loss: -3.623683 [1230848/1237259]
train_time_one_epoch = 95.35743594169617
Epoch 23
-------------------------------
loss: -3.639536 [ 2048/1237259]
loss: -3.635083 [206848/1237259]
loss: -3.635989 [411648/1237259]
loss: -3.633796 [616448/1237259]
loss: -3.627047 [821248/1237259]
loss: -3.625078 [1026048/1237259]
loss: -3.627032 [1230848/1237259]
train_time_one_epoch = 95.46509742736816
Epoch 24
-------------------------------
loss: -3.640088 [ 2048/1237259]
loss: -3.638187 [206848/1237259]
loss: -3.632336 [411648/1237259]
loss: -3.633574 [616448/1237259]
loss: -3.631215 [821248/1237259]
loss: -3.628048 [1026048/1237259]
loss: -3.626122 [1230848/1237259]
train_time_one_epoch = 95.44113326072693
Epoch 25
-------------------------------
loss: -3.640056 [ 2048/1237259]
loss: -3.635582 [206848/1237259]
loss: -3.632963 [411648/1237259]
loss: -3.630663 [616448/1237259]
loss: -3.632529 [821248/1237259]
loss: -3.628896 [1026048/1237259]
loss: -3.624954 [1230848/1237259]
train_time_one_epoch = 95.38464212417603
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0596  
ndcg@20: 0.0490  
diversity: 0.2551  


Epoch 26
-------------------------------
loss: -3.639914 [ 2048/1237259]
loss: -3.635634 [206848/1237259]
loss: -3.632766 [411648/1237259]
loss: -3.633904 [616448/1237259]
loss: -3.628812 [821248/1237259]
loss: -3.627536 [1026048/1237259]
loss: -3.626705 [1230848/1237259]
train_time_one_epoch = 94.99095416069031
Epoch 27
-------------------------------
loss: -3.640258 [ 2048/1237259]
loss: -3.636640 [206848/1237259]
loss: -3.634105 [411648/1237259]
loss: -3.633028 [616448/1237259]
loss: -3.632748 [821248/1237259]
loss: -3.627630 [1026048/1237259]
loss: -3.628342 [1230848/1237259]
train_time_one_epoch = 95.37855529785156
Epoch 28
-------------------------------
loss: -3.641937 [ 2048/1237259]
loss: -3.637393 [206848/1237259]
loss: -3.636043 [411648/1237259]
loss: -3.632786 [616448/1237259]
loss: -3.629112 [821248/1237259]
loss: -3.629536 [1026048/1237259]
loss: -3.627775 [1230848/1237259]
train_time_one_epoch = 95.45001888275146
Epoch 29
-------------------------------
loss: -3.639335 [ 2048/1237259]
loss: -3.636559 [206848/1237259]
loss: -3.633498 [411648/1237259]
loss: -3.632441 [616448/1237259]
loss: -3.631457 [821248/1237259]
loss: -3.628599 [1026048/1237259]
loss: -3.627318 [1230848/1237259]
train_time_one_epoch = 95.36169862747192
Epoch 30
-------------------------------
loss: -3.639181 [ 2048/1237259]
loss: -3.636896 [206848/1237259]
loss: -3.634727 [411648/1237259]
loss: -3.634055 [616448/1237259]
loss: -3.630756 [821248/1237259]
loss: -3.631455 [1026048/1237259]
loss: -3.629157 [1230848/1237259]
train_time_one_epoch = 95.45897054672241
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0602  
ndcg@20: 0.0497  
diversity: 0.2542  


Epoch 31
-------------------------------
loss: -3.639567 [ 2048/1237259]
loss: -3.639145 [206848/1237259]
loss: -3.634703 [411648/1237259]
loss: -3.633458 [616448/1237259]
loss: -3.631938 [821248/1237259]
loss: -3.629121 [1026048/1237259]
loss: -3.630094 [1230848/1237259]
train_time_one_epoch = 94.90198016166687
Epoch 32
-------------------------------
loss: -3.638812 [ 2048/1237259]
loss: -3.637296 [206848/1237259]
loss: -3.635140 [411648/1237259]
loss: -3.631507 [616448/1237259]
loss: -3.631913 [821248/1237259]
loss: -3.631413 [1026048/1237259]
loss: -3.629337 [1230848/1237259]
train_time_one_epoch = 95.47329950332642
Epoch 33
-------------------------------
loss: -3.640487 [ 2048/1237259]
loss: -3.639231 [206848/1237259]
loss: -3.636541 [411648/1237259]
loss: -3.633934 [616448/1237259]
loss: -3.634607 [821248/1237259]
loss: -3.631594 [1026048/1237259]
loss: -3.629860 [1230848/1237259]
train_time_one_epoch = 95.45282578468323
Epoch 34
-------------------------------
loss: -3.637621 [ 2048/1237259]
loss: -3.637037 [206848/1237259]
loss: -3.637027 [411648/1237259]
loss: -3.633097 [616448/1237259]
loss: -3.629582 [821248/1237259]
loss: -3.632562 [1026048/1237259]
loss: -3.629810 [1230848/1237259]
train_time_one_epoch = 95.4011881351471
Epoch 35
-------------------------------
loss: -3.640403 [ 2048/1237259]
loss: -3.640470 [206848/1237259]
loss: -3.636839 [411648/1237259]
loss: -3.633540 [616448/1237259]
loss: -3.632629 [821248/1237259]
loss: -3.632416 [1026048/1237259]
loss: -3.630151 [1230848/1237259]
train_time_one_epoch = 95.26842260360718
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0605  
ndcg@20: 0.0498  
diversity: 0.2534  


Epoch 36
-------------------------------
loss: -3.638807 [ 2048/1237259]
loss: -3.638456 [206848/1237259]
loss: -3.635970 [411648/1237259]
loss: -3.634852 [616448/1237259]
loss: -3.632174 [821248/1237259]
loss: -3.630704 [1026048/1237259]
loss: -3.629260 [1230848/1237259]
train_time_one_epoch = 94.96002125740051
Epoch 37
-------------------------------
loss: -3.639956 [ 2048/1237259]
loss: -3.638710 [206848/1237259]
loss: -3.637374 [411648/1237259]
loss: -3.636483 [616448/1237259]
loss: -3.635412 [821248/1237259]
loss: -3.631077 [1026048/1237259]
loss: -3.630086 [1230848/1237259]
train_time_one_epoch = 95.31579661369324
Epoch 38
-------------------------------
loss: -3.640814 [ 2048/1237259]
loss: -3.639033 [206848/1237259]
loss: -3.636811 [411648/1237259]
loss: -3.633772 [616448/1237259]
loss: -3.633918 [821248/1237259]
loss: -3.628098 [1026048/1237259]
loss: -3.626285 [1230848/1237259]
train_time_one_epoch = 95.36604833602905
Epoch 39
-------------------------------
loss: -3.639758 [ 2048/1237259]
loss: -3.637058 [206848/1237259]
loss: -3.636028 [411648/1237259]
loss: -3.635141 [616448/1237259]
loss: -3.633611 [821248/1237259]
loss: -3.630898 [1026048/1237259]
loss: -3.629592 [1230848/1237259]
train_time_one_epoch = 95.32301664352417
Epoch 40
-------------------------------
loss: -3.643079 [ 2048/1237259]
loss: -3.636752 [206848/1237259]
loss: -3.637236 [411648/1237259]
loss: -3.633063 [616448/1237259]
loss: -3.632312 [821248/1237259]
loss: -3.632300 [1026048/1237259]
loss: -3.629074 [1230848/1237259]
train_time_one_epoch = 95.368155002594
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0606  
ndcg@20: 0.0500  
diversity: 0.2527  


Epoch 41
-------------------------------
loss: -3.636790 [ 2048/1237259]
loss: -3.636504 [206848/1237259]
loss: -3.634223 [411648/1237259]
loss: -3.633530 [616448/1237259]
loss: -3.636098 [821248/1237259]
loss: -3.631252 [1026048/1237259]
loss: -3.629768 [1230848/1237259]
train_time_one_epoch = 94.95904850959778
Epoch 42
-------------------------------
loss: -3.636653 [ 2048/1237259]
loss: -3.638320 [206848/1237259]
loss: -3.634912 [411648/1237259]
loss: -3.635055 [616448/1237259]
loss: -3.633228 [821248/1237259]
loss: -3.632071 [1026048/1237259]
loss: -3.631601 [1230848/1237259]
train_time_one_epoch = 95.44896817207336
Epoch 43
-------------------------------
loss: -3.638469 [ 2048/1237259]
loss: -3.637473 [206848/1237259]
loss: -3.638350 [411648/1237259]
loss: -3.635273 [616448/1237259]
loss: -3.631224 [821248/1237259]
loss: -3.633868 [1026048/1237259]
loss: -3.630661 [1230848/1237259]
train_time_one_epoch = 95.39092326164246
Epoch 44
-------------------------------
loss: -3.640318 [ 2048/1237259]
loss: -3.637012 [206848/1237259]
loss: -3.636002 [411648/1237259]
loss: -3.636545 [616448/1237259]
loss: -3.632990 [821248/1237259]
loss: -3.632434 [1026048/1237259]
loss: -3.627498 [1230848/1237259]
train_time_one_epoch = 95.41786050796509
Epoch 45
-------------------------------
loss: -3.638261 [ 2048/1237259]
loss: -3.637696 [206848/1237259]
loss: -3.638587 [411648/1237259]
loss: -3.635980 [616448/1237259]
loss: -3.634452 [821248/1237259]
loss: -3.632309 [1026048/1237259]
loss: -3.627563 [1230848/1237259]
train_time_one_epoch = 95.39113426208496
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0607  
ndcg@20: 0.0502  
diversity: 0.2522  


Epoch 46
-------------------------------
loss: -3.639595 [ 2048/1237259]
loss: -3.637969 [206848/1237259]
loss: -3.634641 [411648/1237259]
loss: -3.634980 [616448/1237259]
loss: -3.634609 [821248/1237259]
loss: -3.631098 [1026048/1237259]
loss: -3.631829 [1230848/1237259]
train_time_one_epoch = 94.98661351203918
Epoch 47
-------------------------------
loss: -3.640176 [ 2048/1237259]
loss: -3.639918 [206848/1237259]
loss: -3.640177 [411648/1237259]
loss: -3.635403 [616448/1237259]
loss: -3.633618 [821248/1237259]
loss: -3.633403 [1026048/1237259]
loss: -3.633125 [1230848/1237259]
train_time_one_epoch = 95.38866233825684
Epoch 48
-------------------------------
loss: -3.639860 [ 2048/1237259]
loss: -3.638658 [206848/1237259]
loss: -3.637697 [411648/1237259]
loss: -3.635319 [616448/1237259]
loss: -3.632794 [821248/1237259]
loss: -3.633681 [1026048/1237259]
loss: -3.632572 [1230848/1237259]
train_time_one_epoch = 95.34843564033508
Epoch 49
-------------------------------
loss: -3.638950 [ 2048/1237259]
loss: -3.639834 [206848/1237259]
loss: -3.636976 [411648/1237259]
loss: -3.634329 [616448/1237259]
loss: -3.633045 [821248/1237259]
loss: -3.633411 [1026048/1237259]
loss: -3.628747 [1230848/1237259]
train_time_one_epoch = 95.37107348442078
Epoch 50
-------------------------------
loss: -3.638987 [ 2048/1237259]
loss: -3.636321 [206848/1237259]
loss: -3.638325 [411648/1237259]
loss: -3.634478 [616448/1237259]
loss: -3.633405 [821248/1237259]
loss: -3.632831 [1026048/1237259]
loss: -3.632215 [1230848/1237259]
train_time_one_epoch = 95.44689536094666
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0607  
ndcg@20: 0.0502  
diversity: 0.2517  


Epoch 51
-------------------------------
loss: -3.639882 [ 2048/1237259]
loss: -3.639736 [206848/1237259]
loss: -3.638677 [411648/1237259]
loss: -3.634958 [616448/1237259]
loss: -3.633434 [821248/1237259]
loss: -3.631733 [1026048/1237259]
loss: -3.629627 [1230848/1237259]
train_time_one_epoch = 94.83595395088196
Epoch 52
-------------------------------
loss: -3.641724 [ 2048/1237259]
loss: -3.636665 [206848/1237259]
loss: -3.636681 [411648/1237259]
loss: -3.635761 [616448/1237259]
loss: -3.636373 [821248/1237259]
loss: -3.635233 [1026048/1237259]
loss: -3.631557 [1230848/1237259]
train_time_one_epoch = 95.45996952056885
Epoch 53
-------------------------------
loss: -3.638377 [ 2048/1237259]
loss: -3.639698 [206848/1237259]
loss: -3.634354 [411648/1237259]
loss: -3.635569 [616448/1237259]
loss: -3.632909 [821248/1237259]
loss: -3.635265 [1026048/1237259]
loss: -3.632792 [1230848/1237259]
train_time_one_epoch = 95.41566205024719
Epoch 54
-------------------------------
loss: -3.640019 [ 2048/1237259]
loss: -3.636461 [206848/1237259]
loss: -3.637115 [411648/1237259]
loss: -3.636061 [616448/1237259]
loss: -3.634524 [821248/1237259]
loss: -3.630738 [1026048/1237259]
loss: -3.631622 [1230848/1237259]
train_time_one_epoch = 95.37476468086243
Epoch 55
-------------------------------
loss: -3.640006 [ 2048/1237259]
loss: -3.638977 [206848/1237259]
loss: -3.635487 [411648/1237259]
loss: -3.637214 [616448/1237259]
loss: -3.636226 [821248/1237259]
loss: -3.632852 [1026048/1237259]
loss: -3.631944 [1230848/1237259]
train_time_one_epoch = 95.43605399131775
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0503  
diversity: 0.2513  


Epoch 56
-------------------------------
loss: -3.637545 [ 2048/1237259]
loss: -3.638270 [206848/1237259]
loss: -3.640123 [411648/1237259]
loss: -3.635597 [616448/1237259]
loss: -3.632970 [821248/1237259]
loss: -3.631375 [1026048/1237259]
loss: -3.630525 [1230848/1237259]
train_time_one_epoch = 94.94429755210876
Epoch 57
-------------------------------
loss: -3.639316 [ 2048/1237259]
loss: -3.638008 [206848/1237259]
loss: -3.635432 [411648/1237259]
loss: -3.637752 [616448/1237259]
loss: -3.634416 [821248/1237259]
loss: -3.633518 [1026048/1237259]
loss: -3.633374 [1230848/1237259]
train_time_one_epoch = 95.45444083213806
Epoch 58
-------------------------------
loss: -3.641250 [ 2048/1237259]
loss: -3.636104 [206848/1237259]
loss: -3.635690 [411648/1237259]
loss: -3.637273 [616448/1237259]
loss: -3.631247 [821248/1237259]
loss: -3.633707 [1026048/1237259]
loss: -3.633119 [1230848/1237259]
train_time_one_epoch = 95.41557049751282
Epoch 59
-------------------------------
loss: -3.637837 [ 2048/1237259]
loss: -3.637071 [206848/1237259]
loss: -3.639649 [411648/1237259]
loss: -3.637414 [616448/1237259]
loss: -3.632093 [821248/1237259]
loss: -3.632598 [1026048/1237259]
loss: -3.631543 [1230848/1237259]
train_time_one_epoch = 95.50151109695435
Epoch 60
-------------------------------
loss: -3.641472 [ 2048/1237259]
loss: -3.638119 [206848/1237259]
loss: -3.636720 [411648/1237259]
loss: -3.636393 [616448/1237259]
loss: -3.633589 [821248/1237259]
loss: -3.634239 [1026048/1237259]
loss: -3.630097 [1230848/1237259]
train_time_one_epoch = 95.39262819290161
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0503  
diversity: 0.2509  


Epoch 61
-------------------------------
loss: -3.640280 [ 2048/1237259]
loss: -3.639367 [206848/1237259]
loss: -3.637097 [411648/1237259]
loss: -3.634061 [616448/1237259]
loss: -3.634140 [821248/1237259]
loss: -3.633610 [1026048/1237259]
loss: -3.632911 [1230848/1237259]
train_time_one_epoch = 94.97124028205872
Epoch 62
-------------------------------
loss: -3.641117 [ 2048/1237259]
loss: -3.637079 [206848/1237259]
loss: -3.638439 [411648/1237259]
loss: -3.634922 [616448/1237259]
loss: -3.634507 [821248/1237259]
loss: -3.634499 [1026048/1237259]
loss: -3.634704 [1230848/1237259]
train_time_one_epoch = 95.34915328025818
Epoch 63
-------------------------------
loss: -3.641230 [ 2048/1237259]
loss: -3.640962 [206848/1237259]
loss: -3.637295 [411648/1237259]
loss: -3.637206 [616448/1237259]
loss: -3.635159 [821248/1237259]
loss: -3.633298 [1026048/1237259]
loss: -3.630913 [1230848/1237259]
train_time_one_epoch = 95.45063161849976
Epoch 64
-------------------------------
loss: -3.636499 [ 2048/1237259]
loss: -3.637252 [206848/1237259]
loss: -3.635429 [411648/1237259]
loss: -3.637362 [616448/1237259]
loss: -3.636501 [821248/1237259]
loss: -3.631267 [1026048/1237259]
loss: -3.634181 [1230848/1237259]
train_time_one_epoch = 95.4157063961029
Epoch 65
-------------------------------
loss: -3.640625 [ 2048/1237259]
loss: -3.637291 [206848/1237259]
loss: -3.634737 [411648/1237259]
loss: -3.637617 [616448/1237259]
loss: -3.633620 [821248/1237259]
loss: -3.634990 [1026048/1237259]
loss: -3.631838 [1230848/1237259]
train_time_one_epoch = 95.3505265712738
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0504  
diversity: 0.2506  


Epoch 66
-------------------------------
loss: -3.640997 [ 2048/1237259]
loss: -3.638517 [206848/1237259]
loss: -3.636011 [411648/1237259]
loss: -3.637255 [616448/1237259]
loss: -3.636446 [821248/1237259]
loss: -3.632947 [1026048/1237259]
loss: -3.633967 [1230848/1237259]
train_time_one_epoch = 94.99337482452393
Epoch 67
-------------------------------
loss: -3.641228 [ 2048/1237259]
loss: -3.640937 [206848/1237259]
loss: -3.638446 [411648/1237259]
loss: -3.635805 [616448/1237259]
loss: -3.632965 [821248/1237259]
loss: -3.636219 [1026048/1237259]
loss: -3.632372 [1230848/1237259]
train_time_one_epoch = 95.4442138671875
Epoch 68
-------------------------------
loss: -3.639739 [ 2048/1237259]
loss: -3.638194 [206848/1237259]
loss: -3.636546 [411648/1237259]
loss: -3.636202 [616448/1237259]
loss: -3.635108 [821248/1237259]
loss: -3.636234 [1026048/1237259]
loss: -3.634485 [1230848/1237259]
train_time_one_epoch = 95.54780793190002
Epoch 69
-------------------------------
loss: -3.643014 [ 2048/1237259]
loss: -3.639469 [206848/1237259]
loss: -3.638587 [411648/1237259]
loss: -3.635140 [616448/1237259]
loss: -3.633658 [821248/1237259]
loss: -3.635606 [1026048/1237259]
loss: -3.631804 [1230848/1237259]
train_time_one_epoch = 95.41560626029968
Epoch 70
-------------------------------
loss: -3.640134 [ 2048/1237259]
loss: -3.640144 [206848/1237259]
loss: -3.638139 [411648/1237259]
loss: -3.636526 [616448/1237259]
loss: -3.633150 [821248/1237259]
loss: -3.634879 [1026048/1237259]
loss: -3.631994 [1230848/1237259]
train_time_one_epoch = 95.4309720993042
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0504  
diversity: 0.2502  


Epoch 71
-------------------------------
loss: -3.641140 [ 2048/1237259]
loss: -3.638392 [206848/1237259]
loss: -3.638800 [411648/1237259]
loss: -3.636161 [616448/1237259]
loss: -3.636542 [821248/1237259]
loss: -3.632398 [1026048/1237259]
loss: -3.633562 [1230848/1237259]
train_time_one_epoch = 94.80471181869507
Epoch 72
-------------------------------
loss: -3.639760 [ 2048/1237259]
loss: -3.638688 [206848/1237259]
loss: -3.640261 [411648/1237259]
loss: -3.636621 [616448/1237259]
loss: -3.634861 [821248/1237259]
loss: -3.633899 [1026048/1237259]
loss: -3.632726 [1230848/1237259]
train_time_one_epoch = 95.41001033782959
Epoch 73
-------------------------------
loss: -3.641677 [ 2048/1237259]
loss: -3.639567 [206848/1237259]
loss: -3.637944 [411648/1237259]
loss: -3.635656 [616448/1237259]
loss: -3.636609 [821248/1237259]
loss: -3.633220 [1026048/1237259]
loss: -3.633109 [1230848/1237259]
train_time_one_epoch = 95.37626028060913
Epoch 74
-------------------------------
loss: -3.640863 [ 2048/1237259]
loss: -3.638211 [206848/1237259]
loss: -3.640590 [411648/1237259]
loss: -3.637899 [616448/1237259]
loss: -3.636070 [821248/1237259]
loss: -3.637600 [1026048/1237259]
loss: -3.633245 [1230848/1237259]
train_time_one_epoch = 95.4093565940857
Epoch 75
-------------------------------
loss: -3.639323 [ 2048/1237259]
loss: -3.636304 [206848/1237259]
loss: -3.636012 [411648/1237259]
loss: -3.634402 [616448/1237259]
loss: -3.635326 [821248/1237259]
loss: -3.634296 [1026048/1237259]
loss: -3.630338 [1230848/1237259]
train_time_one_epoch = 95.28810715675354
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0505  
diversity: 0.2500  


Epoch 76
-------------------------------
loss: -3.642533 [ 2048/1237259]
loss: -3.640634 [206848/1237259]
loss: -3.639898 [411648/1237259]
loss: -3.636698 [616448/1237259]
loss: -3.638167 [821248/1237259]
loss: -3.634454 [1026048/1237259]
loss: -3.632419 [1230848/1237259]
train_time_one_epoch = 94.8984043598175
Epoch 77
-------------------------------
loss: -3.640795 [ 2048/1237259]
loss: -3.640036 [206848/1237259]
loss: -3.637359 [411648/1237259]
loss: -3.636319 [616448/1237259]
loss: -3.634560 [821248/1237259]
loss: -3.636317 [1026048/1237259]
loss: -3.630725 [1230848/1237259]
train_time_one_epoch = 95.36854410171509
Epoch 78
-------------------------------
loss: -3.641162 [ 2048/1237259]
loss: -3.639732 [206848/1237259]
loss: -3.639966 [411648/1237259]
loss: -3.636600 [616448/1237259]
loss: -3.636333 [821248/1237259]
loss: -3.634145 [1026048/1237259]
loss: -3.632930 [1230848/1237259]
train_time_one_epoch = 95.29239249229431
Epoch 79
-------------------------------
loss: -3.642450 [ 2048/1237259]
loss: -3.640131 [206848/1237259]
loss: -3.638650 [411648/1237259]
loss: -3.637337 [616448/1237259]
loss: -3.633834 [821248/1237259]
loss: -3.635796 [1026048/1237259]
loss: -3.630345 [1230848/1237259]
train_time_one_epoch = 95.41536164283752
Epoch 80
-------------------------------
loss: -3.640952 [ 2048/1237259]
loss: -3.638424 [206848/1237259]
loss: -3.637162 [411648/1237259]
loss: -3.636415 [616448/1237259]
loss: -3.633768 [821248/1237259]
loss: -3.636502 [1026048/1237259]
loss: -3.631716 [1230848/1237259]
train_time_one_epoch = 95.31471109390259
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0505  
diversity: 0.2498  


Epoch 81
-------------------------------
loss: -3.639508 [ 2048/1237259]
loss: -3.640175 [206848/1237259]
loss: -3.635728 [411648/1237259]
loss: -3.637463 [616448/1237259]
loss: -3.633646 [821248/1237259]
loss: -3.634776 [1026048/1237259]
loss: -3.632987 [1230848/1237259]
train_time_one_epoch = 94.83297443389893
Epoch 82
-------------------------------
loss: -3.639789 [ 2048/1237259]
loss: -3.637689 [206848/1237259]
loss: -3.639568 [411648/1237259]
loss: -3.635123 [616448/1237259]
loss: -3.633402 [821248/1237259]
loss: -3.634785 [1026048/1237259]
loss: -3.631013 [1230848/1237259]
train_time_one_epoch = 95.35069847106934
Epoch 83
-------------------------------
loss: -3.643001 [ 2048/1237259]
loss: -3.636980 [206848/1237259]
loss: -3.639717 [411648/1237259]
loss: -3.636057 [616448/1237259]
loss: -3.635757 [821248/1237259]
loss: -3.632592 [1026048/1237259]
loss: -3.632117 [1230848/1237259]
train_time_one_epoch = 95.2822527885437
Epoch 84
-------------------------------
loss: -3.639100 [ 2048/1237259]
loss: -3.639355 [206848/1237259]
loss: -3.637975 [411648/1237259]
loss: -3.637686 [616448/1237259]
loss: -3.637094 [821248/1237259]
loss: -3.635168 [1026048/1237259]
loss: -3.633926 [1230848/1237259]
train_time_one_epoch = 95.3395164012909
Epoch 85
-------------------------------
loss: -3.639054 [ 2048/1237259]
loss: -3.638939 [206848/1237259]
loss: -3.638932 [411648/1237259]
loss: -3.636166 [616448/1237259]
loss: -3.635388 [821248/1237259]
loss: -3.634359 [1026048/1237259]
loss: -3.633316 [1230848/1237259]
train_time_one_epoch = 95.33449029922485
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0506  
diversity: 0.2496  


Epoch 86
-------------------------------
loss: -3.639077 [ 2048/1237259]
loss: -3.639288 [206848/1237259]
loss: -3.637211 [411648/1237259]
loss: -3.634670 [616448/1237259]
loss: -3.633709 [821248/1237259]
loss: -3.633746 [1026048/1237259]
loss: -3.632330 [1230848/1237259]
train_time_one_epoch = 94.84059071540833
Epoch 87
-------------------------------
loss: -3.642324 [ 2048/1237259]
loss: -3.641921 [206848/1237259]
loss: -3.638115 [411648/1237259]
loss: -3.637036 [616448/1237259]
loss: -3.635965 [821248/1237259]
loss: -3.635544 [1026048/1237259]
loss: -3.633300 [1230848/1237259]
train_time_one_epoch = 95.36806154251099
Epoch 88
-------------------------------
loss: -3.639554 [ 2048/1237259]
loss: -3.636242 [206848/1237259]
loss: -3.635893 [411648/1237259]
loss: -3.635757 [616448/1237259]
loss: -3.633012 [821248/1237259]
loss: -3.634783 [1026048/1237259]
loss: -3.634597 [1230848/1237259]
train_time_one_epoch = 95.37638211250305
Epoch 89
-------------------------------
loss: -3.637739 [ 2048/1237259]
loss: -3.636547 [206848/1237259]
loss: -3.635793 [411648/1237259]
loss: -3.635054 [616448/1237259]
loss: -3.637690 [821248/1237259]
loss: -3.636565 [1026048/1237259]
loss: -3.637083 [1230848/1237259]
train_time_one_epoch = 95.45515608787537
Epoch 90
-------------------------------
loss: -3.639000 [ 2048/1237259]
loss: -3.638913 [206848/1237259]
loss: -3.637851 [411648/1237259]
loss: -3.636489 [616448/1237259]
loss: -3.634265 [821248/1237259]
loss: -3.633137 [1026048/1237259]
loss: -3.633879 [1230848/1237259]
train_time_one_epoch = 95.40668439865112
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0506  
diversity: 0.2492  


Epoch 91
-------------------------------
loss: -3.639633 [ 2048/1237259]
loss: -3.639482 [206848/1237259]
loss: -3.640349 [411648/1237259]
loss: -3.635487 [616448/1237259]
loss: -3.636191 [821248/1237259]
loss: -3.634942 [1026048/1237259]
loss: -3.631702 [1230848/1237259]
train_time_one_epoch = 94.90898013114929
Epoch 92
-------------------------------
loss: -3.639388 [ 2048/1237259]
loss: -3.640294 [206848/1237259]
loss: -3.637909 [411648/1237259]
loss: -3.636781 [616448/1237259]
loss: -3.635712 [821248/1237259]
loss: -3.637043 [1026048/1237259]
loss: -3.634942 [1230848/1237259]
train_time_one_epoch = 95.39761304855347
Epoch 93
-------------------------------
loss: -3.640717 [ 2048/1237259]
loss: -3.639076 [206848/1237259]
loss: -3.639611 [411648/1237259]
loss: -3.637486 [616448/1237259]
loss: -3.636271 [821248/1237259]
loss: -3.633319 [1026048/1237259]
loss: -3.636062 [1230848/1237259]
train_time_one_epoch = 95.30926442146301
Epoch 94
-------------------------------
loss: -3.639579 [ 2048/1237259]
loss: -3.639921 [206848/1237259]
loss: -3.637971 [411648/1237259]
loss: -3.637686 [616448/1237259]
loss: -3.636036 [821248/1237259]
loss: -3.632789 [1026048/1237259]
loss: -3.634034 [1230848/1237259]
train_time_one_epoch = 95.27907872200012
Epoch 95
-------------------------------
loss: -3.639748 [ 2048/1237259]
loss: -3.636802 [206848/1237259]
loss: -3.637211 [411648/1237259]
loss: -3.638316 [616448/1237259]
loss: -3.635430 [821248/1237259]
loss: -3.637452 [1026048/1237259]
loss: -3.631236 [1230848/1237259]
train_time_one_epoch = 95.3765218257904
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0504  
diversity: 0.2491  


Epoch 96
-------------------------------
loss: -3.643489 [ 2048/1237259]
loss: -3.638838 [206848/1237259]
loss: -3.637899 [411648/1237259]
loss: -3.635062 [616448/1237259]
loss: -3.634880 [821248/1237259]
loss: -3.638394 [1026048/1237259]
loss: -3.634426 [1230848/1237259]
train_time_one_epoch = 94.91942596435547
Epoch 97
-------------------------------
loss: -3.639621 [ 2048/1237259]
loss: -3.637469 [206848/1237259]
loss: -3.637230 [411648/1237259]
loss: -3.637746 [616448/1237259]
loss: -3.633923 [821248/1237259]
loss: -3.633663 [1026048/1237259]
loss: -3.637231 [1230848/1237259]
train_time_one_epoch = 95.42011761665344
Epoch 98
-------------------------------
loss: -3.639635 [ 2048/1237259]
loss: -3.637722 [206848/1237259]
loss: -3.639159 [411648/1237259]
loss: -3.639265 [616448/1237259]
loss: -3.635750 [821248/1237259]
loss: -3.636478 [1026048/1237259]
loss: -3.633179 [1230848/1237259]
train_time_one_epoch = 95.4210274219513
Epoch 99
-------------------------------
loss: -3.640847 [ 2048/1237259]
loss: -3.639837 [206848/1237259]
loss: -3.641352 [411648/1237259]
loss: -3.634791 [616448/1237259]
loss: -3.637478 [821248/1237259]
loss: -3.632674 [1026048/1237259]
loss: -3.633886 [1230848/1237259]
train_time_one_epoch = 95.42506051063538
Epoch 100
-------------------------------
loss: -3.638565 [ 2048/1237259]
loss: -3.638834 [206848/1237259]
loss: -3.637617 [411648/1237259]
loss: -3.636583 [616448/1237259]
loss: -3.635578 [821248/1237259]
loss: -3.635152 [1026048/1237259]
loss: -3.633690 [1230848/1237259]
train_time_one_epoch = 95.36640620231628
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0507  
diversity: 0.2488  


Epoch 101
-------------------------------
loss: -3.637489 [ 2048/1237259]
loss: -3.639798 [206848/1237259]
loss: -3.636453 [411648/1237259]
loss: -3.634624 [616448/1237259]
loss: -3.636803 [821248/1237259]
loss: -3.633849 [1026048/1237259]
loss: -3.635269 [1230848/1237259]
train_time_one_epoch = 95.00987815856934
Epoch 102
-------------------------------
loss: -3.638358 [ 2048/1237259]
loss: -3.638500 [206848/1237259]
loss: -3.637513 [411648/1237259]
loss: -3.636159 [616448/1237259]
loss: -3.634082 [821248/1237259]
loss: -3.633585 [1026048/1237259]
loss: -3.636209 [1230848/1237259]
train_time_one_epoch = 95.3459141254425
Epoch 103
-------------------------------
loss: -3.640908 [ 2048/1237259]
loss: -3.640853 [206848/1237259]
loss: -3.641211 [411648/1237259]
loss: -3.636433 [616448/1237259]
loss: -3.638633 [821248/1237259]
loss: -3.635435 [1026048/1237259]
loss: -3.633342 [1230848/1237259]
train_time_one_epoch = 95.30631136894226
Epoch 104
-------------------------------
loss: -3.641320 [ 2048/1237259]
loss: -3.639433 [206848/1237259]
loss: -3.640132 [411648/1237259]
loss: -3.637000 [616448/1237259]
loss: -3.636307 [821248/1237259]
loss: -3.635027 [1026048/1237259]
loss: -3.634436 [1230848/1237259]
train_time_one_epoch = 95.40461945533752
Epoch 105
-------------------------------
loss: -3.639762 [ 2048/1237259]
loss: -3.639599 [206848/1237259]
loss: -3.639558 [411648/1237259]
loss: -3.636184 [616448/1237259]
loss: -3.637151 [821248/1237259]
loss: -3.633880 [1026048/1237259]
loss: -3.633706 [1230848/1237259]
train_time_one_epoch = 95.36486983299255
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0615  
ndcg@20: 0.0508  
diversity: 0.2487  


Epoch 106
-------------------------------
loss: -3.640549 [ 2048/1237259]
loss: -3.640858 [206848/1237259]
loss: -3.638658 [411648/1237259]
loss: -3.635977 [616448/1237259]
loss: -3.636792 [821248/1237259]
loss: -3.635192 [1026048/1237259]
loss: -3.634318 [1230848/1237259]
train_time_one_epoch = 94.96346545219421
Epoch 107
-------------------------------
loss: -3.640962 [ 2048/1237259]
loss: -3.640657 [206848/1237259]
loss: -3.640050 [411648/1237259]
loss: -3.640237 [616448/1237259]
loss: -3.637508 [821248/1237259]
loss: -3.636222 [1026048/1237259]
loss: -3.630603 [1230848/1237259]
train_time_one_epoch = 95.31254625320435
Epoch 108
-------------------------------
loss: -3.640548 [ 2048/1237259]
loss: -3.636335 [206848/1237259]
loss: -3.638091 [411648/1237259]
loss: -3.637336 [616448/1237259]
loss: -3.634292 [821248/1237259]
loss: -3.635972 [1026048/1237259]
loss: -3.633850 [1230848/1237259]
train_time_one_epoch = 95.22963166236877
Epoch 109
-------------------------------
loss: -3.642034 [ 2048/1237259]
loss: -3.642284 [206848/1237259]
loss: -3.637321 [411648/1237259]
loss: -3.639624 [616448/1237259]
loss: -3.635947 [821248/1237259]
loss: -3.635653 [1026048/1237259]
loss: -3.633795 [1230848/1237259]
train_time_one_epoch = 95.2867705821991
Epoch 110
-------------------------------
loss: -3.637462 [ 2048/1237259]
loss: -3.640233 [206848/1237259]
loss: -3.639020 [411648/1237259]
loss: -3.638546 [616448/1237259]
loss: -3.636252 [821248/1237259]
loss: -3.636448 [1026048/1237259]
loss: -3.635229 [1230848/1237259]
train_time_one_epoch = 95.47583794593811
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0616  
ndcg@20: 0.0508  
diversity: 0.2485  


Epoch 111
-------------------------------
loss: -3.639234 [ 2048/1237259]
loss: -3.640905 [206848/1237259]
loss: -3.637114 [411648/1237259]
loss: -3.639029 [616448/1237259]
loss: -3.636938 [821248/1237259]
loss: -3.635009 [1026048/1237259]
loss: -3.635103 [1230848/1237259]
train_time_one_epoch = 94.83018207550049
Epoch 112
-------------------------------
loss: -3.641467 [ 2048/1237259]
loss: -3.637761 [206848/1237259]
loss: -3.637296 [411648/1237259]
loss: -3.636292 [616448/1237259]
loss: -3.637002 [821248/1237259]
loss: -3.635490 [1026048/1237259]
loss: -3.635297 [1230848/1237259]
train_time_one_epoch = 95.33288908004761
Epoch 113
-------------------------------
loss: -3.640279 [ 2048/1237259]
loss: -3.639118 [206848/1237259]
loss: -3.640945 [411648/1237259]
loss: -3.638734 [616448/1237259]
loss: -3.634188 [821248/1237259]
loss: -3.636256 [1026048/1237259]
loss: -3.635787 [1230848/1237259]
train_time_one_epoch = 95.36391711235046
Epoch 114
-------------------------------
loss: -3.638694 [ 2048/1237259]
loss: -3.639845 [206848/1237259]
loss: -3.636951 [411648/1237259]
loss: -3.637633 [616448/1237259]
loss: -3.635716 [821248/1237259]
loss: -3.635168 [1026048/1237259]
loss: -3.634197 [1230848/1237259]
train_time_one_epoch = 95.4011161327362
Epoch 115
-------------------------------
loss: -3.639606 [ 2048/1237259]
loss: -3.638372 [206848/1237259]
loss: -3.639400 [411648/1237259]
loss: -3.636221 [616448/1237259]
loss: -3.637768 [821248/1237259]
loss: -3.634669 [1026048/1237259]
loss: -3.631279 [1230848/1237259]
train_time_one_epoch = 95.2782232761383
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0619  
ndcg@20: 0.0510  
diversity: 0.2483  


Epoch 116
-------------------------------
loss: -3.639924 [ 2048/1237259]
loss: -3.641298 [206848/1237259]
loss: -3.636668 [411648/1237259]
loss: -3.637819 [616448/1237259]
loss: -3.635288 [821248/1237259]
loss: -3.639416 [1026048/1237259]
loss: -3.635218 [1230848/1237259]
train_time_one_epoch = 94.87952136993408
Epoch 117
-------------------------------
loss: -3.642621 [ 2048/1237259]
loss: -3.638874 [206848/1237259]
loss: -3.638604 [411648/1237259]
loss: -3.637868 [616448/1237259]
loss: -3.637025 [821248/1237259]
loss: -3.635626 [1026048/1237259]
loss: -3.634416 [1230848/1237259]
train_time_one_epoch = 95.36940217018127
Epoch 118
-------------------------------
loss: -3.641885 [ 2048/1237259]
loss: -3.642491 [206848/1237259]
loss: -3.638398 [411648/1237259]
loss: -3.637361 [616448/1237259]
loss: -3.635270 [821248/1237259]
loss: -3.635457 [1026048/1237259]
loss: -3.632905 [1230848/1237259]
train_time_one_epoch = 95.36457371711731
Epoch 119
-------------------------------
loss: -3.639978 [ 2048/1237259]
loss: -3.637971 [206848/1237259]
loss: -3.638008 [411648/1237259]
loss: -3.636072 [616448/1237259]
loss: -3.637094 [821248/1237259]
loss: -3.631368 [1026048/1237259]
loss: -3.634827 [1230848/1237259]
train_time_one_epoch = 95.27901411056519
Epoch 120
-------------------------------
loss: -3.639966 [ 2048/1237259]
loss: -3.638854 [206848/1237259]
loss: -3.639096 [411648/1237259]
loss: -3.634127 [616448/1237259]
loss: -3.637408 [821248/1237259]
loss: -3.632514 [1026048/1237259]
loss: -3.633052 [1230848/1237259]
train_time_one_epoch = 95.33144903182983
Eval results: 
recall@20: 0.0618  
ndcg@20: 0.0509  
diversity: 0.2482  


Epoch 121
-------------------------------
loss: -3.641917 [ 2048/1237259]
loss: -3.638975 [206848/1237259]
loss: -3.636891 [411648/1237259]
loss: -3.638636 [616448/1237259]
loss: -3.636055 [821248/1237259]
loss: -3.636388 [1026048/1237259]
loss: -3.633075 [1230848/1237259]
train_time_one_epoch = 94.7817873954773
Epoch 122
-------------------------------
loss: -3.641245 [ 2048/1237259]
loss: -3.638866 [206848/1237259]
loss: -3.635978 [411648/1237259]
loss: -3.638251 [616448/1237259]
loss: -3.633457 [821248/1237259]
loss: -3.638617 [1026048/1237259]
loss: -3.634607 [1230848/1237259]
train_time_one_epoch = 95.22937536239624
Epoch 123
-------------------------------
loss: -3.641987 [ 2048/1237259]
loss: -3.638435 [206848/1237259]
loss: -3.635781 [411648/1237259]
loss: -3.638680 [616448/1237259]
loss: -3.636993 [821248/1237259]
loss: -3.635214 [1026048/1237259]
loss: -3.637548 [1230848/1237259]
train_time_one_epoch = 95.20049524307251
Epoch 124
-------------------------------
loss: -3.639795 [ 2048/1237259]
loss: -3.639570 [206848/1237259]
loss: -3.638039 [411648/1237259]
loss: -3.635089 [616448/1237259]
loss: -3.637542 [821248/1237259]
loss: -3.636008 [1026048/1237259]
loss: -3.636085 [1230848/1237259]
train_time_one_epoch = 95.1499125957489
Epoch 125
-------------------------------
loss: -3.640226 [ 2048/1237259]
loss: -3.640725 [206848/1237259]
loss: -3.638964 [411648/1237259]
loss: -3.638239 [616448/1237259]
loss: -3.635962 [821248/1237259]
loss: -3.634661 [1026048/1237259]
loss: -3.636897 [1230848/1237259]
train_time_one_epoch = 95.12640404701233
Eval results: 
recall@20: 0.0618  
ndcg@20: 0.0510  
diversity: 0.2480  


Epoch 126
-------------------------------
loss: -3.641043 [ 2048/1237259]
loss: -3.638807 [206848/1237259]
loss: -3.637811 [411648/1237259]
loss: -3.636919 [616448/1237259]
loss: -3.640336 [821248/1237259]
loss: -3.633471 [1026048/1237259]
loss: -3.633940 [1230848/1237259]
train_time_one_epoch = 94.64365696907043
Epoch 127
-------------------------------
loss: -3.640821 [ 2048/1237259]
loss: -3.639650 [206848/1237259]
loss: -3.638830 [411648/1237259]
loss: -3.636779 [616448/1237259]
loss: -3.637909 [821248/1237259]
loss: -3.632823 [1026048/1237259]
loss: -3.631239 [1230848/1237259]
train_time_one_epoch = 95.12124419212341
Epoch 128
-------------------------------
loss: -3.639647 [ 2048/1237259]
loss: -3.641144 [206848/1237259]
loss: -3.637892 [411648/1237259]
loss: -3.637723 [616448/1237259]
loss: -3.636091 [821248/1237259]
loss: -3.636040 [1026048/1237259]
loss: -3.635286 [1230848/1237259]
train_time_one_epoch = 95.30972027778625
Epoch 129
-------------------------------
loss: -3.643156 [ 2048/1237259]
loss: -3.640748 [206848/1237259]
loss: -3.638299 [411648/1237259]
loss: -3.634626 [616448/1237259]
loss: -3.636998 [821248/1237259]
loss: -3.635859 [1026048/1237259]
loss: -3.633816 [1230848/1237259]
train_time_one_epoch = 95.44333696365356
Epoch 130
-------------------------------
loss: -3.640959 [ 2048/1237259]
loss: -3.639201 [206848/1237259]
loss: -3.640658 [411648/1237259]
loss: -3.640059 [616448/1237259]
loss: -3.638939 [821248/1237259]
loss: -3.637188 [1026048/1237259]
loss: -3.635882 [1230848/1237259]
train_time_one_epoch = 95.31742572784424
Eval results: 
recall@20: 0.0618  
ndcg@20: 0.0509  
diversity: 0.2479  


Epoch 131
-------------------------------
loss: -3.640562 [ 2048/1237259]
loss: -3.637810 [206848/1237259]
loss: -3.639433 [411648/1237259]
loss: -3.636039 [616448/1237259]
loss: -3.636079 [821248/1237259]
loss: -3.636984 [1026048/1237259]
loss: -3.636409 [1230848/1237259]
train_time_one_epoch = 94.82270812988281
Epoch 132
-------------------------------
loss: -3.641656 [ 2048/1237259]
loss: -3.641560 [206848/1237259]
loss: -3.638225 [411648/1237259]
loss: -3.635991 [616448/1237259]
loss: -3.637751 [821248/1237259]
loss: -3.634354 [1026048/1237259]
loss: -3.633860 [1230848/1237259]
train_time_one_epoch = 95.42017865180969
Epoch 133
-------------------------------
loss: -3.640633 [ 2048/1237259]
loss: -3.640294 [206848/1237259]
loss: -3.636509 [411648/1237259]
loss: -3.636362 [616448/1237259]
loss: -3.635200 [821248/1237259]
loss: -3.635176 [1026048/1237259]
loss: -3.634193 [1230848/1237259]
train_time_one_epoch = 95.36971473693848
Epoch 134
-------------------------------
loss: -3.639794 [ 2048/1237259]
loss: -3.636247 [206848/1237259]
loss: -3.639215 [411648/1237259]
loss: -3.638258 [616448/1237259]
loss: -3.635848 [821248/1237259]
loss: -3.635478 [1026048/1237259]
loss: -3.637089 [1230848/1237259]
train_time_one_epoch = 95.41110324859619
Epoch 135
-------------------------------
loss: -3.638017 [ 2048/1237259]
loss: -3.639354 [206848/1237259]
loss: -3.641268 [411648/1237259]
loss: -3.635306 [616448/1237259]
loss: -3.638730 [821248/1237259]
loss: -3.633937 [1026048/1237259]
loss: -3.634320 [1230848/1237259]
train_time_one_epoch = 95.22312641143799
Eval results: 
recall@20: 0.0617  
ndcg@20: 0.0509  
diversity: 0.2477  


Epoch 136
-------------------------------
loss: -3.641897 [ 2048/1237259]
loss: -3.638796 [206848/1237259]
loss: -3.637833 [411648/1237259]
loss: -3.635927 [616448/1237259]
loss: -3.636978 [821248/1237259]
loss: -3.635849 [1026048/1237259]
loss: -3.632850 [1230848/1237259]
train_time_one_epoch = 94.9534740447998
Epoch 137
-------------------------------
loss: -3.639792 [ 2048/1237259]
loss: -3.640692 [206848/1237259]
loss: -3.639743 [411648/1237259]
loss: -3.636933 [616448/1237259]
loss: -3.633925 [821248/1237259]
loss: -3.634897 [1026048/1237259]
loss: -3.632641 [1230848/1237259]
train_time_one_epoch = 95.32331800460815
Epoch 138
-------------------------------
loss: -3.637189 [ 2048/1237259]
loss: -3.640229 [206848/1237259]
loss: -3.640558 [411648/1237259]
loss: -3.636047 [616448/1237259]
loss: -3.638594 [821248/1237259]
loss: -3.636418 [1026048/1237259]
loss: -3.637213 [1230848/1237259]
train_time_one_epoch = 95.5129554271698
Epoch 139
-------------------------------
loss: -3.643271 [ 2048/1237259]
loss: -3.640584 [206848/1237259]
loss: -3.637254 [411648/1237259]
loss: -3.636246 [616448/1237259]
loss: -3.636347 [821248/1237259]
loss: -3.635822 [1026048/1237259]
loss: -3.634535 [1230848/1237259]
train_time_one_epoch = 95.46484088897705
Epoch 140
-------------------------------
loss: -3.641976 [ 2048/1237259]
loss: -3.638780 [206848/1237259]
loss: -3.639346 [411648/1237259]
loss: -3.637617 [616448/1237259]
loss: -3.637250 [821248/1237259]
loss: -3.635293 [1026048/1237259]
loss: -3.636615 [1230848/1237259]
train_time_one_epoch = 95.40445494651794
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0619  
ndcg@20: 0.0510  
diversity: 0.2476  


Epoch 141
-------------------------------
loss: -3.639643 [ 2048/1237259]
loss: -3.638402 [206848/1237259]
loss: -3.637717 [411648/1237259]
loss: -3.633496 [616448/1237259]
loss: -3.639039 [821248/1237259]
loss: -3.638675 [1026048/1237259]
loss: -3.635865 [1230848/1237259]
train_time_one_epoch = 94.97995281219482
Epoch 142
-------------------------------
loss: -3.642595 [ 2048/1237259]
loss: -3.636617 [206848/1237259]
loss: -3.639677 [411648/1237259]
loss: -3.637740 [616448/1237259]
loss: -3.633153 [821248/1237259]
loss: -3.638081 [1026048/1237259]
loss: -3.635149 [1230848/1237259]
train_time_one_epoch = 95.43752002716064
Epoch 143
-------------------------------
loss: -3.640332 [ 2048/1237259]
loss: -3.639430 [206848/1237259]
loss: -3.636530 [411648/1237259]
loss: -3.638539 [616448/1237259]
loss: -3.638156 [821248/1237259]
loss: -3.637040 [1026048/1237259]
loss: -3.633889 [1230848/1237259]
train_time_one_epoch = 95.46784162521362
Epoch 144
-------------------------------
loss: -3.639260 [ 2048/1237259]
loss: -3.638776 [206848/1237259]
loss: -3.639511 [411648/1237259]
loss: -3.637951 [616448/1237259]
loss: -3.635984 [821248/1237259]
loss: -3.634883 [1026048/1237259]
loss: -3.636636 [1230848/1237259]
train_time_one_epoch = 95.38973093032837
Epoch 145
-------------------------------
loss: -3.639904 [ 2048/1237259]
loss: -3.639586 [206848/1237259]
loss: -3.641828 [411648/1237259]
loss: -3.639604 [616448/1237259]
loss: -3.636438 [821248/1237259]
loss: -3.633771 [1026048/1237259]
loss: -3.634165 [1230848/1237259]
train_time_one_epoch = 95.48722195625305
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0619  
ndcg@20: 0.0510  
diversity: 0.2475  


Epoch 146
-------------------------------
loss: -3.639849 [ 2048/1237259]
loss: -3.642235 [206848/1237259]
loss: -3.640841 [411648/1237259]
loss: -3.637959 [616448/1237259]
loss: -3.638622 [821248/1237259]
loss: -3.635592 [1026048/1237259]
loss: -3.633325 [1230848/1237259]
train_time_one_epoch = 94.93554329872131
Epoch 147
-------------------------------
loss: -3.641769 [ 2048/1237259]
loss: -3.640007 [206848/1237259]
loss: -3.639764 [411648/1237259]
loss: -3.639333 [616448/1237259]
loss: -3.635836 [821248/1237259]
loss: -3.636060 [1026048/1237259]
loss: -3.638044 [1230848/1237259]
train_time_one_epoch = 95.40874457359314
Epoch 148
-------------------------------
loss: -3.640671 [ 2048/1237259]
loss: -3.638756 [206848/1237259]
loss: -3.639274 [411648/1237259]
loss: -3.637313 [616448/1237259]
loss: -3.636813 [821248/1237259]
loss: -3.635214 [1026048/1237259]
loss: -3.635130 [1230848/1237259]
train_time_one_epoch = 95.38537645339966
Epoch 149
-------------------------------
loss: -3.641455 [ 2048/1237259]
loss: -3.639838 [206848/1237259]
loss: -3.639305 [411648/1237259]
loss: -3.639724 [616448/1237259]
loss: -3.637394 [821248/1237259]
loss: -3.638358 [1026048/1237259]
loss: -3.635958 [1230848/1237259]
train_time_one_epoch = 95.45888304710388
Epoch 150
-------------------------------
loss: -3.640692 [ 2048/1237259]
loss: -3.638873 [206848/1237259]
loss: -3.639153 [411648/1237259]
loss: -3.635346 [616448/1237259]
loss: -3.636786 [821248/1237259]
loss: -3.635177 [1026048/1237259]
loss: -3.633365 [1230848/1237259]
train_time_one_epoch = 95.41029596328735
Eval results: 
recall@20: 0.0619  
ndcg@20: 0.0510  
diversity: 0.2474  


Epoch 151
-------------------------------
loss: -3.642869 [ 2048/1237259]
loss: -3.642357 [206848/1237259]
loss: -3.640421 [411648/1237259]
loss: -3.635982 [616448/1237259]
loss: -3.637132 [821248/1237259]
loss: -3.637689 [1026048/1237259]
loss: -3.637375 [1230848/1237259]
train_time_one_epoch = 95.00961136817932
Epoch 152
-------------------------------
loss: -3.638684 [ 2048/1237259]
loss: -3.640096 [206848/1237259]
loss: -3.638333 [411648/1237259]
loss: -3.637700 [616448/1237259]
loss: -3.635977 [821248/1237259]
loss: -3.636457 [1026048/1237259]
loss: -3.632896 [1230848/1237259]
train_time_one_epoch = 95.69069623947144
Epoch 153
-------------------------------
loss: -3.641473 [ 2048/1237259]
loss: -3.638963 [206848/1237259]
loss: -3.638120 [411648/1237259]
loss: -3.638465 [616448/1237259]
loss: -3.635772 [821248/1237259]
loss: -3.638151 [1026048/1237259]
loss: -3.634817 [1230848/1237259]
train_time_one_epoch = 95.54040455818176
Epoch 154
-------------------------------
loss: -3.641148 [ 2048/1237259]
loss: -3.640160 [206848/1237259]
loss: -3.638367 [411648/1237259]
loss: -3.638391 [616448/1237259]
loss: -3.635547 [821248/1237259]
loss: -3.637886 [1026048/1237259]
loss: -3.636372 [1230848/1237259]
train_time_one_epoch = 95.43505549430847
Epoch 155
-------------------------------
loss: -3.641678 [ 2048/1237259]
loss: -3.638324 [206848/1237259]
loss: -3.638315 [411648/1237259]
loss: -3.638005 [616448/1237259]
loss: -3.636159 [821248/1237259]
loss: -3.640106 [1026048/1237259]
loss: -3.633737 [1230848/1237259]
train_time_one_epoch = 95.29354929924011
Eval results: 
recall@20: 0.0619  
ndcg@20: 0.0509  
diversity: 0.2472  


Epoch 156
-------------------------------
loss: -3.640418 [ 2048/1237259]
loss: -3.638577 [206848/1237259]
loss: -3.637402 [411648/1237259]
loss: -3.637805 [616448/1237259]
loss: -3.637218 [821248/1237259]
loss: -3.638313 [1026048/1237259]
loss: -3.634946 [1230848/1237259]
train_time_one_epoch = 94.96819424629211
Epoch 157
-------------------------------
loss: -3.641025 [ 2048/1237259]
loss: -3.637375 [206848/1237259]
loss: -3.635330 [411648/1237259]
loss: -3.637904 [616448/1237259]
loss: -3.636179 [821248/1237259]
loss: -3.636890 [1026048/1237259]
loss: -3.636589 [1230848/1237259]
train_time_one_epoch = 95.3387999534607
Epoch 158
-------------------------------
loss: -3.640610 [ 2048/1237259]
loss: -3.640160 [206848/1237259]
loss: -3.639996 [411648/1237259]
loss: -3.640690 [616448/1237259]
loss: -3.637748 [821248/1237259]
loss: -3.633612 [1026048/1237259]
loss: -3.636649 [1230848/1237259]
train_time_one_epoch = 95.48601460456848
Epoch 159
-------------------------------
loss: -3.641547 [ 2048/1237259]
loss: -3.642045 [206848/1237259]
loss: -3.636156 [411648/1237259]
loss: -3.638152 [616448/1237259]
loss: -3.637532 [821248/1237259]
loss: -3.632597 [1026048/1237259]
loss: -3.635209 [1230848/1237259]
train_time_one_epoch = 95.49297332763672
Epoch 160
-------------------------------
loss: -3.641151 [ 2048/1237259]
loss: -3.639944 [206848/1237259]
loss: -3.639606 [411648/1237259]
loss: -3.639241 [616448/1237259]
loss: -3.634216 [821248/1237259]
loss: -3.636304 [1026048/1237259]
loss: -3.636197 [1230848/1237259]
train_time_one_epoch = 95.36336040496826
Eval results: 
recall@20: 0.0619  
ndcg@20: 0.0510  
diversity: 0.2471  


Epoch 161
-------------------------------
loss: -3.639881 [ 2048/1237259]
loss: -3.638390 [206848/1237259]
loss: -3.637402 [411648/1237259]
loss: -3.638365 [616448/1237259]
loss: -3.636785 [821248/1237259]
loss: -3.635944 [1026048/1237259]
loss: -3.635679 [1230848/1237259]
train_time_one_epoch = 95.02634382247925
Epoch 162
-------------------------------
loss: -3.639859 [ 2048/1237259]
loss: -3.639310 [206848/1237259]
loss: -3.638909 [411648/1237259]
loss: -3.637382 [616448/1237259]
loss: -3.638387 [821248/1237259]
loss: -3.634603 [1026048/1237259]
loss: -3.633364 [1230848/1237259]
train_time_one_epoch = 95.38895797729492
Epoch 163
-------------------------------
loss: -3.644486 [ 2048/1237259]
loss: -3.640707 [206848/1237259]
loss: -3.637558 [411648/1237259]
loss: -3.637952 [616448/1237259]
loss: -3.637448 [821248/1237259]
loss: -3.635163 [1026048/1237259]
loss: -3.634564 [1230848/1237259]
train_time_one_epoch = 95.35622477531433
Epoch 164
-------------------------------
loss: -3.641403 [ 2048/1237259]
loss: -3.638124 [206848/1237259]
loss: -3.639040 [411648/1237259]
loss: -3.637135 [616448/1237259]
loss: -3.636991 [821248/1237259]
loss: -3.635534 [1026048/1237259]
