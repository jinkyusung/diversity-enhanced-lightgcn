Folder created at: ./dw-bpr/best_model
Your Device: cuda
Yelp2018

#user = 31668
#item = 38048

#interactions
    (train) 1237259
    (test)  324147
    (total) 1561406

Sparsity = 0.0012958757851778645

Epoch 0
-------------------------------
loss: 0.145647 [ 2048/1237259]
loss: 0.114038 [206848/1237259]
loss: 0.066250 [411648/1237259]
loss: 0.057656 [616448/1237259]
loss: 0.047498 [821248/1237259]
loss: 0.040588 [1026048/1237259]
loss: 0.043130 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0315  
ndcg@20: 0.0258  
diversity: 0.1612  


Epoch 1
-------------------------------
loss: 0.043117 [ 2048/1237259]
loss: 0.040939 [206848/1237259]
loss: 0.043506 [411648/1237259]
loss: 0.041922 [616448/1237259]
loss: 0.045867 [821248/1237259]
loss: 0.039378 [1026048/1237259]
loss: 0.037269 [1230848/1237259]
Epoch 2
-------------------------------
loss: 0.040674 [ 2048/1237259]
loss: 0.041173 [206848/1237259]
loss: 0.040488 [411648/1237259]
loss: 0.037142 [616448/1237259]
loss: 0.036394 [821248/1237259]
loss: 0.038467 [1026048/1237259]
loss: 0.036439 [1230848/1237259]
Epoch 3
-------------------------------
loss: 0.037351 [ 2048/1237259]
loss: 0.037361 [206848/1237259]
loss: 0.037255 [411648/1237259]
loss: 0.036430 [616448/1237259]
loss: 0.036981 [821248/1237259]
loss: 0.033180 [1026048/1237259]
loss: 0.043468 [1230848/1237259]
Epoch 4
-------------------------------
loss: 0.034211 [ 2048/1237259]
loss: 0.036899 [206848/1237259]
loss: 0.033532 [411648/1237259]
loss: 0.036190 [616448/1237259]
loss: 0.035503 [821248/1237259]
loss: 0.034242 [1026048/1237259]
loss: 0.032271 [1230848/1237259]
Epoch 5
-------------------------------
loss: 0.034256 [ 2048/1237259]
loss: 0.035148 [206848/1237259]
loss: 0.035669 [411648/1237259]
loss: 0.037790 [616448/1237259]
loss: 0.037598 [821248/1237259]
loss: 0.036310 [1026048/1237259]
loss: 0.033009 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0355  
ndcg@20: 0.0288  
diversity: 0.1627  


Epoch 6
-------------------------------
loss: 0.033366 [ 2048/1237259]
loss: 0.037675 [206848/1237259]
loss: 0.033024 [411648/1237259]
loss: 0.032818 [616448/1237259]
loss: 0.036189 [821248/1237259]
loss: 0.030970 [1026048/1237259]
loss: 0.033270 [1230848/1237259]
Epoch 7
-------------------------------
loss: 0.032407 [ 2048/1237259]
loss: 0.033937 [206848/1237259]
loss: 0.031135 [411648/1237259]
loss: 0.031192 [616448/1237259]
loss: 0.037600 [821248/1237259]
loss: 0.033520 [1026048/1237259]
loss: 0.031815 [1230848/1237259]
Epoch 8
-------------------------------
loss: 0.030084 [ 2048/1237259]
loss: 0.032732 [206848/1237259]
loss: 0.033333 [411648/1237259]
loss: 0.033011 [616448/1237259]
loss: 0.030858 [821248/1237259]
loss: 0.031926 [1026048/1237259]
loss: 0.031669 [1230848/1237259]
Epoch 9
-------------------------------
loss: 0.032052 [ 2048/1237259]
loss: 0.029664 [206848/1237259]
loss: 0.030334 [411648/1237259]
loss: 0.031466 [616448/1237259]
loss: 0.028901 [821248/1237259]
loss: 0.030326 [1026048/1237259]
loss: 0.027866 [1230848/1237259]
Epoch 10
-------------------------------
loss: 0.026234 [ 2048/1237259]
loss: 0.029315 [206848/1237259]
loss: 0.024776 [411648/1237259]
loss: 0.030556 [616448/1237259]
loss: 0.028337 [821248/1237259]
loss: 0.031467 [1026048/1237259]
loss: 0.028375 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0397  
ndcg@20: 0.0320  
diversity: 0.1653  


Epoch 11
-------------------------------
loss: 0.030169 [ 2048/1237259]
loss: 0.029596 [206848/1237259]
loss: 0.026843 [411648/1237259]
loss: 0.029090 [616448/1237259]
loss: 0.027936 [821248/1237259]
loss: 0.027244 [1026048/1237259]
loss: 0.025190 [1230848/1237259]
Epoch 12
-------------------------------
loss: 0.026918 [ 2048/1237259]
loss: 0.026792 [206848/1237259]
loss: 0.027290 [411648/1237259]
loss: 0.025140 [616448/1237259]
loss: 0.028062 [821248/1237259]
loss: 0.023121 [1026048/1237259]
loss: 0.026909 [1230848/1237259]
Epoch 13
-------------------------------
loss: 0.028065 [ 2048/1237259]
loss: 0.029560 [206848/1237259]
loss: 0.027610 [411648/1237259]
loss: 0.028839 [616448/1237259]
loss: 0.026309 [821248/1237259]
loss: 0.025493 [1026048/1237259]
loss: 0.026511 [1230848/1237259]
Epoch 14
-------------------------------
loss: 0.027694 [ 2048/1237259]
loss: 0.025158 [206848/1237259]
loss: 0.030121 [411648/1237259]
loss: 0.026357 [616448/1237259]
loss: 0.028964 [821248/1237259]
loss: 0.022432 [1026048/1237259]
loss: 0.025719 [1230848/1237259]
Epoch 15
-------------------------------
loss: 0.025759 [ 2048/1237259]
loss: 0.029250 [206848/1237259]
loss: 0.026286 [411648/1237259]
loss: 0.025866 [616448/1237259]
loss: 0.028252 [821248/1237259]
loss: 0.027818 [1026048/1237259]
loss: 0.027879 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0412  
ndcg@20: 0.0333  
diversity: 0.1671  


Epoch 16
-------------------------------
loss: 0.026355 [ 2048/1237259]
loss: 0.022687 [206848/1237259]
loss: 0.025290 [411648/1237259]
loss: 0.028956 [616448/1237259]
loss: 0.025936 [821248/1237259]
loss: 0.030026 [1026048/1237259]
loss: 0.024441 [1230848/1237259]
Epoch 17
-------------------------------
loss: 0.028887 [ 2048/1237259]
loss: 0.027032 [206848/1237259]
loss: 0.026452 [411648/1237259]
loss: 0.025205 [616448/1237259]
loss: 0.023685 [821248/1237259]
loss: 0.024757 [1026048/1237259]
loss: 0.024912 [1230848/1237259]
Epoch 18
-------------------------------
loss: 0.026195 [ 2048/1237259]
loss: 0.025075 [206848/1237259]
loss: 0.022611 [411648/1237259]
loss: 0.026519 [616448/1237259]
loss: 0.022792 [821248/1237259]
loss: 0.026477 [1026048/1237259]
loss: 0.028235 [1230848/1237259]
Epoch 19
-------------------------------
loss: 0.021613 [ 2048/1237259]
loss: 0.027319 [206848/1237259]
loss: 0.025889 [411648/1237259]
loss: 0.026178 [616448/1237259]
loss: 0.022619 [821248/1237259]
loss: 0.022021 [1026048/1237259]
loss: 0.029108 [1230848/1237259]
Epoch 20
-------------------------------
loss: 0.024930 [ 2048/1237259]
loss: 0.022281 [206848/1237259]
loss: 0.022339 [411648/1237259]
loss: 0.024219 [616448/1237259]
loss: 0.023337 [821248/1237259]
loss: 0.023567 [1026048/1237259]
loss: 0.025790 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0420  
ndcg@20: 0.0339  
diversity: 0.1677  


Epoch 21
-------------------------------
loss: 0.021818 [ 2048/1237259]
loss: 0.024281 [206848/1237259]
loss: 0.027311 [411648/1237259]
loss: 0.026686 [616448/1237259]
loss: 0.023837 [821248/1237259]
loss: 0.023563 [1026048/1237259]
loss: 0.025455 [1230848/1237259]
Epoch 22
-------------------------------
loss: 0.024085 [ 2048/1237259]
loss: 0.024522 [206848/1237259]
loss: 0.021548 [411648/1237259]
loss: 0.021836 [616448/1237259]
loss: 0.020020 [821248/1237259]
loss: 0.022850 [1026048/1237259]
loss: 0.023442 [1230848/1237259]
Epoch 23
-------------------------------
loss: 0.022124 [ 2048/1237259]
loss: 0.020246 [206848/1237259]
loss: 0.026937 [411648/1237259]
loss: 0.029097 [616448/1237259]
loss: 0.023919 [821248/1237259]
loss: 0.023212 [1026048/1237259]
loss: 0.024326 [1230848/1237259]
Epoch 24
-------------------------------
loss: 0.022426 [ 2048/1237259]
loss: 0.023516 [206848/1237259]
loss: 0.024726 [411648/1237259]
loss: 0.020223 [616448/1237259]
loss: 0.022790 [821248/1237259]
loss: 0.023483 [1026048/1237259]
loss: 0.029862 [1230848/1237259]
Epoch 25
-------------------------------
loss: 0.024436 [ 2048/1237259]
loss: 0.023418 [206848/1237259]
loss: 0.021235 [411648/1237259]
loss: 0.022062 [616448/1237259]
loss: 0.023370 [821248/1237259]
loss: 0.022928 [1026048/1237259]
loss: 0.021472 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0435  
ndcg@20: 0.0351  
diversity: 0.1683  


Epoch 26
-------------------------------
loss: 0.025076 [ 2048/1237259]
loss: 0.019892 [206848/1237259]
loss: 0.024484 [411648/1237259]
loss: 0.021511 [616448/1237259]
loss: 0.023207 [821248/1237259]
loss: 0.021594 [1026048/1237259]
loss: 0.023361 [1230848/1237259]
Epoch 27
-------------------------------
loss: 0.024365 [ 2048/1237259]
loss: 0.021392 [206848/1237259]
loss: 0.024307 [411648/1237259]
loss: 0.022060 [616448/1237259]
loss: 0.020589 [821248/1237259]
loss: 0.020476 [1026048/1237259]
loss: 0.025582 [1230848/1237259]
Epoch 28
-------------------------------
loss: 0.022955 [ 2048/1237259]
loss: 0.021824 [206848/1237259]
loss: 0.023898 [411648/1237259]
loss: 0.020326 [616448/1237259]
loss: 0.023134 [821248/1237259]
loss: 0.020042 [1026048/1237259]
loss: 0.023401 [1230848/1237259]
Epoch 29
-------------------------------
loss: 0.023965 [ 2048/1237259]
loss: 0.021461 [206848/1237259]
loss: 0.023566 [411648/1237259]
loss: 0.020614 [616448/1237259]
loss: 0.024214 [821248/1237259]
loss: 0.022259 [1026048/1237259]
loss: 0.024400 [1230848/1237259]
Epoch 30
-------------------------------
loss: 0.020602 [ 2048/1237259]
loss: 0.020983 [206848/1237259]
loss: 0.020423 [411648/1237259]
loss: 0.021070 [616448/1237259]
loss: 0.021632 [821248/1237259]
loss: 0.022008 [1026048/1237259]
loss: 0.024576 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0443  
ndcg@20: 0.0361  
diversity: 0.1694  


Epoch 31
-------------------------------
loss: 0.021787 [ 2048/1237259]
loss: 0.021926 [206848/1237259]
loss: 0.022144 [411648/1237259]
loss: 0.023920 [616448/1237259]
loss: 0.022227 [821248/1237259]
loss: 0.022570 [1026048/1237259]
loss: 0.021421 [1230848/1237259]
Epoch 32
-------------------------------
loss: 0.018641 [ 2048/1237259]
loss: 0.022155 [206848/1237259]
loss: 0.018673 [411648/1237259]
loss: 0.021177 [616448/1237259]
loss: 0.022199 [821248/1237259]
loss: 0.019943 [1026048/1237259]
loss: 0.023886 [1230848/1237259]
Epoch 33
-------------------------------
loss: 0.018625 [ 2048/1237259]
loss: 0.018890 [206848/1237259]
loss: 0.019486 [411648/1237259]
loss: 0.019686 [616448/1237259]
loss: 0.019669 [821248/1237259]
loss: 0.021125 [1026048/1237259]
loss: 0.017689 [1230848/1237259]
Epoch 34
-------------------------------
loss: 0.019025 [ 2048/1237259]
loss: 0.021472 [206848/1237259]
loss: 0.019555 [411648/1237259]
loss: 0.024103 [616448/1237259]
loss: 0.021413 [821248/1237259]
loss: 0.018750 [1026048/1237259]
loss: 0.020292 [1230848/1237259]
Epoch 35
-------------------------------
loss: 0.020071 [ 2048/1237259]
loss: 0.021058 [206848/1237259]
loss: 0.018226 [411648/1237259]
loss: 0.022429 [616448/1237259]
loss: 0.019790 [821248/1237259]
loss: 0.020624 [1026048/1237259]
loss: 0.019902 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0453  
ndcg@20: 0.0369  
diversity: 0.1702  


Epoch 36
-------------------------------
loss: 0.021680 [ 2048/1237259]
loss: 0.021556 [206848/1237259]
loss: 0.019009 [411648/1237259]
loss: 0.018418 [616448/1237259]
loss: 0.019879 [821248/1237259]
loss: 0.021315 [1026048/1237259]
loss: 0.018874 [1230848/1237259]
Epoch 37
-------------------------------
loss: 0.015294 [ 2048/1237259]
loss: 0.014258 [206848/1237259]
loss: 0.022570 [411648/1237259]
loss: 0.018662 [616448/1237259]
loss: 0.019270 [821248/1237259]
loss: 0.021884 [1026048/1237259]
loss: 0.021941 [1230848/1237259]
Epoch 38
-------------------------------
loss: 0.021407 [ 2048/1237259]
loss: 0.021961 [206848/1237259]
loss: 0.020936 [411648/1237259]
loss: 0.022222 [616448/1237259]
loss: 0.018664 [821248/1237259]
loss: 0.017172 [1026048/1237259]
loss: 0.019196 [1230848/1237259]
Epoch 39
-------------------------------
loss: 0.023544 [ 2048/1237259]
loss: 0.018435 [206848/1237259]
loss: 0.018838 [411648/1237259]
loss: 0.018951 [616448/1237259]
loss: 0.021260 [821248/1237259]
loss: 0.020771 [1026048/1237259]
loss: 0.017055 [1230848/1237259]
Epoch 40
-------------------------------
loss: 0.021305 [ 2048/1237259]
loss: 0.021410 [206848/1237259]
loss: 0.021610 [411648/1237259]
loss: 0.019591 [616448/1237259]
loss: 0.019708 [821248/1237259]
loss: 0.019431 [1026048/1237259]
loss: 0.020881 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0460  
ndcg@20: 0.0375  
diversity: 0.1709  


Epoch 41
-------------------------------
loss: 0.016739 [ 2048/1237259]
loss: 0.018553 [206848/1237259]
loss: 0.020991 [411648/1237259]
loss: 0.020009 [616448/1237259]
loss: 0.019318 [821248/1237259]
loss: 0.021012 [1026048/1237259]
loss: 0.017567 [1230848/1237259]
Epoch 42
-------------------------------
loss: 0.019653 [ 2048/1237259]
loss: 0.022046 [206848/1237259]
loss: 0.019330 [411648/1237259]
loss: 0.018267 [616448/1237259]
loss: 0.020463 [821248/1237259]
loss: 0.018823 [1026048/1237259]
loss: 0.018331 [1230848/1237259]
Epoch 43
-------------------------------
loss: 0.019011 [ 2048/1237259]
loss: 0.020105 [206848/1237259]
loss: 0.020539 [411648/1237259]
loss: 0.020122 [616448/1237259]
loss: 0.021076 [821248/1237259]
loss: 0.020423 [1026048/1237259]
loss: 0.020101 [1230848/1237259]
Epoch 44
-------------------------------
loss: 0.016994 [ 2048/1237259]
loss: 0.017755 [206848/1237259]
loss: 0.019813 [411648/1237259]
loss: 0.019830 [616448/1237259]
loss: 0.018680 [821248/1237259]
loss: 0.016614 [1026048/1237259]
loss: 0.022672 [1230848/1237259]
Epoch 45
-------------------------------
loss: 0.019710 [ 2048/1237259]
loss: 0.020976 [206848/1237259]
loss: 0.019979 [411648/1237259]
loss: 0.018318 [616448/1237259]
loss: 0.018551 [821248/1237259]
loss: 0.016336 [1026048/1237259]
loss: 0.021192 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0466  
ndcg@20: 0.0381  
diversity: 0.1716  


Epoch 46
-------------------------------
loss: 0.019105 [ 2048/1237259]
loss: 0.017321 [206848/1237259]
loss: 0.017077 [411648/1237259]
loss: 0.021468 [616448/1237259]
loss: 0.018780 [821248/1237259]
loss: 0.019413 [1026048/1237259]
loss: 0.018679 [1230848/1237259]
Epoch 47
-------------------------------
loss: 0.021302 [ 2048/1237259]
loss: 0.018812 [206848/1237259]
loss: 0.019160 [411648/1237259]
loss: 0.019631 [616448/1237259]
loss: 0.017686 [821248/1237259]
loss: 0.020167 [1026048/1237259]
loss: 0.020664 [1230848/1237259]
Epoch 48
-------------------------------
loss: 0.017243 [ 2048/1237259]
loss: 0.020138 [206848/1237259]
loss: 0.019908 [411648/1237259]
loss: 0.017242 [616448/1237259]
loss: 0.021789 [821248/1237259]
loss: 0.017267 [1026048/1237259]
loss: 0.018787 [1230848/1237259]
Epoch 49
-------------------------------
loss: 0.017973 [ 2048/1237259]
loss: 0.016728 [206848/1237259]
loss: 0.017416 [411648/1237259]
loss: 0.020055 [616448/1237259]
loss: 0.019447 [821248/1237259]
loss: 0.017411 [1026048/1237259]
loss: 0.019068 [1230848/1237259]
Epoch 50
-------------------------------
loss: 0.017723 [ 2048/1237259]
loss: 0.018107 [206848/1237259]
loss: 0.018238 [411648/1237259]
loss: 0.019217 [616448/1237259]
loss: 0.017248 [821248/1237259]
loss: 0.018276 [1026048/1237259]
loss: 0.017886 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0473  
ndcg@20: 0.0385  
diversity: 0.1720  


Epoch 51
-------------------------------
loss: 0.017631 [ 2048/1237259]
loss: 0.019379 [206848/1237259]
loss: 0.018994 [411648/1237259]
loss: 0.019238 [616448/1237259]
loss: 0.021254 [821248/1237259]
loss: 0.015345 [1026048/1237259]
loss: 0.017540 [1230848/1237259]
Epoch 52
-------------------------------
loss: 0.016587 [ 2048/1237259]
loss: 0.015582 [206848/1237259]
loss: 0.016400 [411648/1237259]
loss: 0.018939 [616448/1237259]
loss: 0.019406 [821248/1237259]
loss: 0.020541 [1026048/1237259]
loss: 0.016783 [1230848/1237259]
Epoch 53
-------------------------------
loss: 0.017416 [ 2048/1237259]
loss: 0.017735 [206848/1237259]
loss: 0.017777 [411648/1237259]
loss: 0.015759 [616448/1237259]
loss: 0.018336 [821248/1237259]
loss: 0.016411 [1026048/1237259]
loss: 0.017756 [1230848/1237259]
Epoch 54
-------------------------------
loss: 0.017001 [ 2048/1237259]
loss: 0.017001 [206848/1237259]
loss: 0.015079 [411648/1237259]
loss: 0.018517 [616448/1237259]
loss: 0.019108 [821248/1237259]
loss: 0.018143 [1026048/1237259]
loss: 0.017797 [1230848/1237259]
Epoch 55
-------------------------------
loss: 0.018291 [ 2048/1237259]
loss: 0.019465 [206848/1237259]
loss: 0.017782 [411648/1237259]
loss: 0.016948 [616448/1237259]
loss: 0.017775 [821248/1237259]
loss: 0.020511 [1026048/1237259]
loss: 0.017760 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0480  
ndcg@20: 0.0391  
diversity: 0.1725  


Epoch 56
-------------------------------
loss: 0.016436 [ 2048/1237259]
loss: 0.017679 [206848/1237259]
loss: 0.018108 [411648/1237259]
loss: 0.017790 [616448/1237259]
loss: 0.015949 [821248/1237259]
loss: 0.016217 [1026048/1237259]
loss: 0.018733 [1230848/1237259]
Epoch 57
-------------------------------
loss: 0.018358 [ 2048/1237259]
loss: 0.017104 [206848/1237259]
loss: 0.018217 [411648/1237259]
loss: 0.018477 [616448/1237259]
loss: 0.020355 [821248/1237259]
loss: 0.020725 [1026048/1237259]
loss: 0.016372 [1230848/1237259]
Epoch 58
-------------------------------
loss: 0.015878 [ 2048/1237259]
loss: 0.018476 [206848/1237259]
loss: 0.020045 [411648/1237259]
loss: 0.018442 [616448/1237259]
loss: 0.017200 [821248/1237259]
loss: 0.015382 [1026048/1237259]
loss: 0.019640 [1230848/1237259]
Epoch 59
-------------------------------
loss: 0.018434 [ 2048/1237259]
loss: 0.016467 [206848/1237259]
loss: 0.017936 [411648/1237259]
loss: 0.016524 [616448/1237259]
loss: 0.017277 [821248/1237259]
loss: 0.020806 [1026048/1237259]
loss: 0.016303 [1230848/1237259]
Epoch 60
-------------------------------
loss: 0.016862 [ 2048/1237259]
loss: 0.018123 [206848/1237259]
loss: 0.017580 [411648/1237259]
loss: 0.016487 [616448/1237259]
loss: 0.020673 [821248/1237259]
loss: 0.017697 [1026048/1237259]
loss: 0.017522 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0489  
ndcg@20: 0.0397  
diversity: 0.1731  


Epoch 61
-------------------------------
loss: 0.016361 [ 2048/1237259]
loss: 0.016609 [206848/1237259]
loss: 0.018851 [411648/1237259]
loss: 0.015224 [616448/1237259]
loss: 0.017278 [821248/1237259]
loss: 0.018092 [1026048/1237259]
loss: 0.017037 [1230848/1237259]
Epoch 62
-------------------------------
loss: 0.017829 [ 2048/1237259]
loss: 0.016017 [206848/1237259]
loss: 0.018343 [411648/1237259]
loss: 0.015345 [616448/1237259]
loss: 0.020053 [821248/1237259]
loss: 0.016104 [1026048/1237259]
loss: 0.020939 [1230848/1237259]
Epoch 63
-------------------------------
loss: 0.016425 [ 2048/1237259]
loss: 0.016629 [206848/1237259]
loss: 0.018444 [411648/1237259]
loss: 0.016615 [616448/1237259]
loss: 0.017036 [821248/1237259]
loss: 0.020001 [1026048/1237259]
loss: 0.018084 [1230848/1237259]
Epoch 64
-------------------------------
loss: 0.018748 [ 2048/1237259]
loss: 0.016942 [206848/1237259]
loss: 0.018737 [411648/1237259]
loss: 0.019781 [616448/1237259]
loss: 0.019071 [821248/1237259]
loss: 0.018356 [1026048/1237259]
loss: 0.018690 [1230848/1237259]
Epoch 65
-------------------------------
loss: 0.019936 [ 2048/1237259]
loss: 0.017771 [206848/1237259]
loss: 0.016006 [411648/1237259]
loss: 0.018811 [616448/1237259]
loss: 0.018860 [821248/1237259]
loss: 0.019644 [1026048/1237259]
loss: 0.019174 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0496  
ndcg@20: 0.0402  
diversity: 0.1735  


Epoch 66
-------------------------------
loss: 0.018523 [ 2048/1237259]
loss: 0.016485 [206848/1237259]
loss: 0.020059 [411648/1237259]
loss: 0.018772 [616448/1237259]
loss: 0.018232 [821248/1237259]
loss: 0.015200 [1026048/1237259]
loss: 0.017695 [1230848/1237259]
Epoch 67
-------------------------------
loss: 0.015963 [ 2048/1237259]
loss: 0.018028 [206848/1237259]
loss: 0.014531 [411648/1237259]
loss: 0.016031 [616448/1237259]
loss: 0.017078 [821248/1237259]
loss: 0.018743 [1026048/1237259]
loss: 0.017192 [1230848/1237259]
Epoch 68
-------------------------------
loss: 0.015428 [ 2048/1237259]
loss: 0.017413 [206848/1237259]
loss: 0.020122 [411648/1237259]
loss: 0.015633 [616448/1237259]
loss: 0.020436 [821248/1237259]
loss: 0.019452 [1026048/1237259]
loss: 0.014951 [1230848/1237259]
Epoch 69
-------------------------------
loss: 0.018682 [ 2048/1237259]
loss: 0.016148 [206848/1237259]
loss: 0.015586 [411648/1237259]
loss: 0.016026 [616448/1237259]
loss: 0.016735 [821248/1237259]
loss: 0.016844 [1026048/1237259]
loss: 0.018041 [1230848/1237259]
Epoch 70
-------------------------------
loss: 0.018023 [ 2048/1237259]
loss: 0.016081 [206848/1237259]
loss: 0.016769 [411648/1237259]
loss: 0.018683 [616448/1237259]
loss: 0.017752 [821248/1237259]
loss: 0.016544 [1026048/1237259]
loss: 0.017878 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0503  
ndcg@20: 0.0408  
diversity: 0.1741  


Epoch 71
-------------------------------
loss: 0.018507 [ 2048/1237259]
loss: 0.018419 [206848/1237259]
loss: 0.017981 [411648/1237259]
loss: 0.016486 [616448/1237259]
loss: 0.015673 [821248/1237259]
loss: 0.018349 [1026048/1237259]
loss: 0.017685 [1230848/1237259]
Epoch 72
-------------------------------
loss: 0.019568 [ 2048/1237259]
loss: 0.016143 [206848/1237259]
loss: 0.017451 [411648/1237259]
loss: 0.017167 [616448/1237259]
loss: 0.016088 [821248/1237259]
loss: 0.019197 [1026048/1237259]
loss: 0.018172 [1230848/1237259]
Epoch 73
-------------------------------
loss: 0.016303 [ 2048/1237259]
loss: 0.016747 [206848/1237259]
loss: 0.016751 [411648/1237259]
loss: 0.015269 [616448/1237259]
loss: 0.015753 [821248/1237259]
loss: 0.014939 [1026048/1237259]
loss: 0.017770 [1230848/1237259]
Epoch 74
-------------------------------
loss: 0.014855 [ 2048/1237259]
loss: 0.018736 [206848/1237259]
loss: 0.017022 [411648/1237259]
loss: 0.018871 [616448/1237259]
loss: 0.015992 [821248/1237259]
loss: 0.017230 [1026048/1237259]
loss: 0.018078 [1230848/1237259]
Epoch 75
-------------------------------
loss: 0.015599 [ 2048/1237259]
loss: 0.017467 [206848/1237259]
loss: 0.019292 [411648/1237259]
loss: 0.018298 [616448/1237259]
loss: 0.017426 [821248/1237259]
loss: 0.019041 [1026048/1237259]
loss: 0.018167 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0511  
ndcg@20: 0.0413  
diversity: 0.1745  


Epoch 76
-------------------------------
loss: 0.016290 [ 2048/1237259]
loss: 0.016867 [206848/1237259]
loss: 0.019944 [411648/1237259]
loss: 0.016024 [616448/1237259]
loss: 0.015622 [821248/1237259]
loss: 0.016494 [1026048/1237259]
loss: 0.014329 [1230848/1237259]
Epoch 77
-------------------------------
loss: 0.015737 [ 2048/1237259]
loss: 0.018653 [206848/1237259]
loss: 0.017559 [411648/1237259]
loss: 0.016008 [616448/1237259]
loss: 0.017326 [821248/1237259]
loss: 0.016835 [1026048/1237259]
loss: 0.015118 [1230848/1237259]
Epoch 78
-------------------------------
loss: 0.019123 [ 2048/1237259]
loss: 0.016741 [206848/1237259]
loss: 0.015220 [411648/1237259]
loss: 0.016119 [616448/1237259]
loss: 0.016657 [821248/1237259]
loss: 0.016159 [1026048/1237259]
loss: 0.016097 [1230848/1237259]
Epoch 79
-------------------------------
loss: 0.015307 [ 2048/1237259]
loss: 0.015831 [206848/1237259]
loss: 0.019485 [411648/1237259]
loss: 0.016659 [616448/1237259]
loss: 0.016732 [821248/1237259]
loss: 0.017013 [1026048/1237259]
loss: 0.017942 [1230848/1237259]
Epoch 80
-------------------------------
loss: 0.017286 [ 2048/1237259]
loss: 0.017522 [206848/1237259]
loss: 0.017347 [411648/1237259]
loss: 0.017759 [616448/1237259]
loss: 0.016187 [821248/1237259]
loss: 0.016531 [1026048/1237259]
loss: 0.018680 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0518  
ndcg@20: 0.0417  
diversity: 0.1748  


Epoch 81
-------------------------------
loss: 0.017653 [ 2048/1237259]
loss: 0.015052 [206848/1237259]
loss: 0.015486 [411648/1237259]
loss: 0.017434 [616448/1237259]
loss: 0.016374 [821248/1237259]
loss: 0.016391 [1026048/1237259]
loss: 0.015363 [1230848/1237259]
Epoch 82
-------------------------------
loss: 0.015573 [ 2048/1237259]
loss: 0.014313 [206848/1237259]
loss: 0.017134 [411648/1237259]
loss: 0.017037 [616448/1237259]
loss: 0.015135 [821248/1237259]
loss: 0.015785 [1026048/1237259]
loss: 0.014323 [1230848/1237259]
Epoch 83
-------------------------------
loss: 0.016117 [ 2048/1237259]
loss: 0.015518 [206848/1237259]
loss: 0.015115 [411648/1237259]
loss: 0.017522 [616448/1237259]
loss: 0.016469 [821248/1237259]
loss: 0.014575 [1026048/1237259]
loss: 0.017821 [1230848/1237259]
Epoch 84
-------------------------------
loss: 0.016397 [ 2048/1237259]
loss: 0.016608 [206848/1237259]
loss: 0.014910 [411648/1237259]
loss: 0.016960 [616448/1237259]
loss: 0.017625 [821248/1237259]
loss: 0.016912 [1026048/1237259]
loss: 0.017684 [1230848/1237259]
Epoch 85
-------------------------------
loss: 0.013554 [ 2048/1237259]
loss: 0.018395 [206848/1237259]
loss: 0.018575 [411648/1237259]
loss: 0.017889 [616448/1237259]
loss: 0.017177 [821248/1237259]
loss: 0.013507 [1026048/1237259]
loss: 0.016860 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0523  
ndcg@20: 0.0421  
diversity: 0.1752  


Epoch 86
-------------------------------
loss: 0.017442 [ 2048/1237259]
loss: 0.017072 [206848/1237259]
loss: 0.017255 [411648/1237259]
loss: 0.016605 [616448/1237259]
loss: 0.018065 [821248/1237259]
loss: 0.018586 [1026048/1237259]
loss: 0.016208 [1230848/1237259]
Epoch 87
-------------------------------
loss: 0.015075 [ 2048/1237259]
loss: 0.014852 [206848/1237259]
loss: 0.015772 [411648/1237259]
loss: 0.013867 [616448/1237259]
loss: 0.018986 [821248/1237259]
loss: 0.015226 [1026048/1237259]
loss: 0.014487 [1230848/1237259]
Epoch 88
-------------------------------
loss: 0.014330 [ 2048/1237259]
loss: 0.018039 [206848/1237259]
loss: 0.015608 [411648/1237259]
loss: 0.018679 [616448/1237259]
loss: 0.017990 [821248/1237259]
loss: 0.017042 [1026048/1237259]
loss: 0.016432 [1230848/1237259]
Epoch 89
-------------------------------
loss: 0.016205 [ 2048/1237259]
loss: 0.017606 [206848/1237259]
loss: 0.016895 [411648/1237259]
loss: 0.016003 [616448/1237259]
loss: 0.015999 [821248/1237259]
loss: 0.017368 [1026048/1237259]
loss: 0.014604 [1230848/1237259]
Epoch 90
-------------------------------
loss: 0.014759 [ 2048/1237259]
loss: 0.016917 [206848/1237259]
loss: 0.016811 [411648/1237259]
loss: 0.018140 [616448/1237259]
loss: 0.012737 [821248/1237259]
loss: 0.015141 [1026048/1237259]
loss: 0.016965 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0528  
ndcg@20: 0.0425  
diversity: 0.1756  


Epoch 91
-------------------------------
loss: 0.016152 [ 2048/1237259]
loss: 0.015231 [206848/1237259]
loss: 0.015410 [411648/1237259]
loss: 0.015482 [616448/1237259]
loss: 0.018368 [821248/1237259]
loss: 0.015748 [1026048/1237259]
loss: 0.017657 [1230848/1237259]
Epoch 92
-------------------------------
loss: 0.014399 [ 2048/1237259]
loss: 0.015338 [206848/1237259]
loss: 0.016076 [411648/1237259]
loss: 0.015389 [616448/1237259]
loss: 0.017957 [821248/1237259]
loss: 0.015770 [1026048/1237259]
loss: 0.015937 [1230848/1237259]
Epoch 93
-------------------------------
loss: 0.015180 [ 2048/1237259]
loss: 0.014915 [206848/1237259]
loss: 0.014152 [411648/1237259]
loss: 0.015262 [616448/1237259]
loss: 0.017287 [821248/1237259]
loss: 0.016954 [1026048/1237259]
loss: 0.013553 [1230848/1237259]
Epoch 94
-------------------------------
loss: 0.016397 [ 2048/1237259]
loss: 0.016875 [206848/1237259]
loss: 0.014187 [411648/1237259]
loss: 0.016088 [616448/1237259]
loss: 0.013832 [821248/1237259]
loss: 0.014325 [1026048/1237259]
loss: 0.015205 [1230848/1237259]
Epoch 95
-------------------------------
loss: 0.013837 [ 2048/1237259]
loss: 0.015823 [206848/1237259]
loss: 0.016253 [411648/1237259]
loss: 0.016482 [616448/1237259]
loss: 0.014851 [821248/1237259]
loss: 0.016166 [1026048/1237259]
loss: 0.014946 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0531  
ndcg@20: 0.0427  
diversity: 0.1760  


Epoch 96
-------------------------------
loss: 0.015833 [ 2048/1237259]
loss: 0.016213 [206848/1237259]
loss: 0.017069 [411648/1237259]
loss: 0.016262 [616448/1237259]
loss: 0.018235 [821248/1237259]
loss: 0.014213 [1026048/1237259]
loss: 0.016341 [1230848/1237259]
Epoch 97
-------------------------------
loss: 0.014773 [ 2048/1237259]
loss: 0.016377 [206848/1237259]
loss: 0.013916 [411648/1237259]
loss: 0.014997 [616448/1237259]
loss: 0.017658 [821248/1237259]
loss: 0.017767 [1026048/1237259]
loss: 0.019956 [1230848/1237259]
Epoch 98
-------------------------------
loss: 0.018194 [ 2048/1237259]
loss: 0.016463 [206848/1237259]
loss: 0.016499 [411648/1237259]
loss: 0.014412 [616448/1237259]
loss: 0.015369 [821248/1237259]
loss: 0.014861 [1026048/1237259]
loss: 0.016277 [1230848/1237259]
Epoch 99
-------------------------------
loss: 0.016960 [ 2048/1237259]
loss: 0.015505 [206848/1237259]
loss: 0.015566 [411648/1237259]
loss: 0.015885 [616448/1237259]
loss: 0.016669 [821248/1237259]
loss: 0.015622 [1026048/1237259]
loss: 0.016614 [1230848/1237259]
Epoch 100
-------------------------------
loss: 0.013478 [ 2048/1237259]
loss: 0.015319 [206848/1237259]
loss: 0.016374 [411648/1237259]
loss: 0.014587 [616448/1237259]
loss: 0.014135 [821248/1237259]
loss: 0.014968 [1026048/1237259]
loss: 0.016573 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0536  
ndcg@20: 0.0433  
diversity: 0.1763  


Epoch 101
-------------------------------
loss: 0.014723 [ 2048/1237259]
loss: 0.016179 [206848/1237259]
loss: 0.016177 [411648/1237259]
loss: 0.016200 [616448/1237259]
loss: 0.013450 [821248/1237259]
loss: 0.013602 [1026048/1237259]
loss: 0.016985 [1230848/1237259]
Epoch 102
-------------------------------
loss: 0.016266 [ 2048/1237259]
loss: 0.015761 [206848/1237259]
loss: 0.013761 [411648/1237259]
loss: 0.017390 [616448/1237259]
loss: 0.015368 [821248/1237259]
loss: 0.015250 [1026048/1237259]
loss: 0.016392 [1230848/1237259]
Epoch 103
-------------------------------
loss: 0.015092 [ 2048/1237259]
loss: 0.014903 [206848/1237259]
loss: 0.015772 [411648/1237259]
loss: 0.015860 [616448/1237259]
loss: 0.019540 [821248/1237259]
loss: 0.014598 [1026048/1237259]
loss: 0.015636 [1230848/1237259]
Epoch 104
-------------------------------
loss: 0.016647 [ 2048/1237259]
loss: 0.016240 [206848/1237259]
loss: 0.014147 [411648/1237259]
loss: 0.015818 [616448/1237259]
loss: 0.015194 [821248/1237259]
loss: 0.016536 [1026048/1237259]
loss: 0.017731 [1230848/1237259]
Epoch 105
-------------------------------
loss: 0.015738 [ 2048/1237259]
loss: 0.013617 [206848/1237259]
loss: 0.013797 [411648/1237259]
loss: 0.014767 [616448/1237259]
loss: 0.016981 [821248/1237259]
loss: 0.015530 [1026048/1237259]
loss: 0.015442 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0537  
ndcg@20: 0.0434  
diversity: 0.1766  


Epoch 106
-------------------------------
loss: 0.015696 [ 2048/1237259]
loss: 0.014793 [206848/1237259]
loss: 0.014673 [411648/1237259]
loss: 0.014435 [616448/1237259]
loss: 0.015913 [821248/1237259]
loss: 0.014329 [1026048/1237259]
loss: 0.015935 [1230848/1237259]
Epoch 107
-------------------------------
loss: 0.016353 [ 2048/1237259]
loss: 0.014239 [206848/1237259]
loss: 0.016326 [411648/1237259]
loss: 0.016309 [616448/1237259]
loss: 0.014968 [821248/1237259]
loss: 0.018046 [1026048/1237259]
loss: 0.015676 [1230848/1237259]
Epoch 108
-------------------------------
loss: 0.017132 [ 2048/1237259]
loss: 0.016637 [206848/1237259]
loss: 0.014796 [411648/1237259]
loss: 0.018918 [616448/1237259]
loss: 0.016262 [821248/1237259]
loss: 0.015923 [1026048/1237259]
loss: 0.017246 [1230848/1237259]
Epoch 109
-------------------------------
loss: 0.014333 [ 2048/1237259]
loss: 0.016456 [206848/1237259]
loss: 0.015700 [411648/1237259]
loss: 0.014477 [616448/1237259]
loss: 0.019360 [821248/1237259]
loss: 0.015129 [1026048/1237259]
loss: 0.015440 [1230848/1237259]
Epoch 110
-------------------------------
loss: 0.015938 [ 2048/1237259]
loss: 0.015899 [206848/1237259]
loss: 0.015412 [411648/1237259]
loss: 0.015256 [616448/1237259]
loss: 0.017138 [821248/1237259]
loss: 0.017497 [1026048/1237259]
loss: 0.017145 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0539  
ndcg@20: 0.0435  
diversity: 0.1769  


Epoch 111
-------------------------------
loss: 0.013679 [ 2048/1237259]
loss: 0.016680 [206848/1237259]
loss: 0.016418 [411648/1237259]
loss: 0.013661 [616448/1237259]
loss: 0.014625 [821248/1237259]
loss: 0.015774 [1026048/1237259]
loss: 0.013789 [1230848/1237259]
Epoch 112
-------------------------------
loss: 0.013996 [ 2048/1237259]
loss: 0.016094 [206848/1237259]
loss: 0.016424 [411648/1237259]
loss: 0.016110 [616448/1237259]
loss: 0.016276 [821248/1237259]
loss: 0.016646 [1026048/1237259]
loss: 0.016292 [1230848/1237259]
Epoch 113
-------------------------------
loss: 0.015491 [ 2048/1237259]
loss: 0.015325 [206848/1237259]
loss: 0.015589 [411648/1237259]
loss: 0.016125 [616448/1237259]
loss: 0.015178 [821248/1237259]
loss: 0.016245 [1026048/1237259]
loss: 0.013221 [1230848/1237259]
Epoch 114
-------------------------------
loss: 0.014431 [ 2048/1237259]
loss: 0.014372 [206848/1237259]
loss: 0.014271 [411648/1237259]
loss: 0.014516 [616448/1237259]
loss: 0.015678 [821248/1237259]
loss: 0.015042 [1026048/1237259]
loss: 0.016002 [1230848/1237259]
Epoch 115
-------------------------------
loss: 0.014047 [ 2048/1237259]
loss: 0.016540 [206848/1237259]
loss: 0.014291 [411648/1237259]
loss: 0.014177 [616448/1237259]
loss: 0.015281 [821248/1237259]
loss: 0.016160 [1026048/1237259]
loss: 0.014142 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0545  
ndcg@20: 0.0439  
diversity: 0.1772  


Epoch 116
-------------------------------
loss: 0.015982 [ 2048/1237259]
loss: 0.017254 [206848/1237259]
loss: 0.015710 [411648/1237259]
loss: 0.017191 [616448/1237259]
loss: 0.014891 [821248/1237259]
loss: 0.015040 [1026048/1237259]
loss: 0.016670 [1230848/1237259]
Epoch 117
-------------------------------
loss: 0.013537 [ 2048/1237259]
loss: 0.013303 [206848/1237259]
loss: 0.016685 [411648/1237259]
loss: 0.017428 [616448/1237259]
loss: 0.014837 [821248/1237259]
loss: 0.014657 [1026048/1237259]
loss: 0.014299 [1230848/1237259]
Epoch 118
-------------------------------
loss: 0.016119 [ 2048/1237259]
loss: 0.013157 [206848/1237259]
loss: 0.015397 [411648/1237259]
loss: 0.014900 [616448/1237259]
loss: 0.015932 [821248/1237259]
loss: 0.015334 [1026048/1237259]
loss: 0.015512 [1230848/1237259]
Epoch 119
-------------------------------
loss: 0.014745 [ 2048/1237259]
loss: 0.014619 [206848/1237259]
loss: 0.015301 [411648/1237259]
loss: 0.015942 [616448/1237259]
loss: 0.016252 [821248/1237259]
loss: 0.013445 [1026048/1237259]
loss: 0.015063 [1230848/1237259]
Epoch 120
-------------------------------
loss: 0.015008 [ 2048/1237259]
loss: 0.017032 [206848/1237259]
loss: 0.014775 [411648/1237259]
loss: 0.017089 [616448/1237259]
loss: 0.015638 [821248/1237259]
loss: 0.013112 [1026048/1237259]
loss: 0.013529 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0548  
ndcg@20: 0.0440  
diversity: 0.1774  


Epoch 121
-------------------------------
loss: 0.016837 [ 2048/1237259]
loss: 0.015491 [206848/1237259]
loss: 0.015320 [411648/1237259]
loss: 0.014369 [616448/1237259]
loss: 0.016839 [821248/1237259]
loss: 0.014745 [1026048/1237259]
loss: 0.015654 [1230848/1237259]
Epoch 122
-------------------------------
loss: 0.014325 [ 2048/1237259]
loss: 0.016132 [206848/1237259]
loss: 0.013542 [411648/1237259]
loss: 0.014417 [616448/1237259]
loss: 0.013497 [821248/1237259]
loss: 0.016205 [1026048/1237259]
loss: 0.012276 [1230848/1237259]
Epoch 123
-------------------------------
loss: 0.015056 [ 2048/1237259]
loss: 0.014381 [206848/1237259]
loss: 0.015207 [411648/1237259]
loss: 0.014303 [616448/1237259]
loss: 0.013971 [821248/1237259]
loss: 0.016232 [1026048/1237259]
loss: 0.014569 [1230848/1237259]
Epoch 124
-------------------------------
loss: 0.014067 [ 2048/1237259]
loss: 0.015914 [206848/1237259]
loss: 0.014131 [411648/1237259]
loss: 0.015804 [616448/1237259]
loss: 0.015475 [821248/1237259]
loss: 0.015867 [1026048/1237259]
loss: 0.015846 [1230848/1237259]
Epoch 125
-------------------------------
loss: 0.016596 [ 2048/1237259]
loss: 0.014635 [206848/1237259]
loss: 0.013720 [411648/1237259]
loss: 0.016292 [616448/1237259]
loss: 0.013723 [821248/1237259]
loss: 0.015451 [1026048/1237259]
loss: 0.014665 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0549  
ndcg@20: 0.0443  
diversity: 0.1776  


Epoch 126
-------------------------------
loss: 0.015939 [ 2048/1237259]
loss: 0.012961 [206848/1237259]
loss: 0.013540 [411648/1237259]
loss: 0.015361 [616448/1237259]
loss: 0.014329 [821248/1237259]
loss: 0.015186 [1026048/1237259]
loss: 0.016797 [1230848/1237259]
Epoch 127
-------------------------------
loss: 0.014557 [ 2048/1237259]
loss: 0.014511 [206848/1237259]
loss: 0.015518 [411648/1237259]
loss: 0.014061 [616448/1237259]
loss: 0.014160 [821248/1237259]
loss: 0.013328 [1026048/1237259]
loss: 0.015964 [1230848/1237259]
Epoch 128
-------------------------------
loss: 0.014750 [ 2048/1237259]
loss: 0.014127 [206848/1237259]
loss: 0.014149 [411648/1237259]
loss: 0.015382 [616448/1237259]
loss: 0.015924 [821248/1237259]
loss: 0.015643 [1026048/1237259]
loss: 0.016093 [1230848/1237259]
Epoch 129
-------------------------------
loss: 0.014476 [ 2048/1237259]
loss: 0.014700 [206848/1237259]
loss: 0.016137 [411648/1237259]
loss: 0.014456 [616448/1237259]
loss: 0.016712 [821248/1237259]
loss: 0.016124 [1026048/1237259]
loss: 0.017570 [1230848/1237259]
Epoch 130
-------------------------------
loss: 0.014852 [ 2048/1237259]
loss: 0.014467 [206848/1237259]
loss: 0.015061 [411648/1237259]
loss: 0.017891 [616448/1237259]
loss: 0.015834 [821248/1237259]
loss: 0.013652 [1026048/1237259]
loss: 0.016022 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0551  
ndcg@20: 0.0445  
diversity: 0.1780  


Epoch 131
-------------------------------
loss: 0.014501 [ 2048/1237259]
loss: 0.015791 [206848/1237259]
loss: 0.016038 [411648/1237259]
loss: 0.015155 [616448/1237259]
loss: 0.016201 [821248/1237259]
loss: 0.014456 [1026048/1237259]
loss: 0.014409 [1230848/1237259]
Epoch 132
-------------------------------
loss: 0.013484 [ 2048/1237259]
loss: 0.013078 [206848/1237259]
loss: 0.013684 [411648/1237259]
loss: 0.013552 [616448/1237259]
loss: 0.015646 [821248/1237259]
loss: 0.015267 [1026048/1237259]
loss: 0.017277 [1230848/1237259]
Epoch 133
-------------------------------
loss: 0.013791 [ 2048/1237259]
loss: 0.015479 [206848/1237259]
loss: 0.015192 [411648/1237259]
loss: 0.018539 [616448/1237259]
loss: 0.014673 [821248/1237259]
loss: 0.014215 [1026048/1237259]
loss: 0.013267 [1230848/1237259]
Epoch 134
-------------------------------
loss: 0.013031 [ 2048/1237259]
loss: 0.014696 [206848/1237259]
loss: 0.013219 [411648/1237259]
loss: 0.014689 [616448/1237259]
loss: 0.014082 [821248/1237259]
loss: 0.016654 [1026048/1237259]
loss: 0.015395 [1230848/1237259]
Epoch 135
-------------------------------
loss: 0.015913 [ 2048/1237259]
loss: 0.013762 [206848/1237259]
loss: 0.014568 [411648/1237259]
loss: 0.013772 [616448/1237259]
loss: 0.014417 [821248/1237259]
loss: 0.013961 [1026048/1237259]
loss: 0.013773 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0554  
ndcg@20: 0.0447  
diversity: 0.1782  


Epoch 136
-------------------------------
loss: 0.015198 [ 2048/1237259]
loss: 0.014679 [206848/1237259]
loss: 0.013841 [411648/1237259]
loss: 0.014676 [616448/1237259]
loss: 0.014541 [821248/1237259]
loss: 0.015738 [1026048/1237259]
loss: 0.015393 [1230848/1237259]
Epoch 137
-------------------------------
loss: 0.014192 [ 2048/1237259]
loss: 0.014200 [206848/1237259]
loss: 0.014326 [411648/1237259]
loss: 0.014692 [616448/1237259]
loss: 0.017057 [821248/1237259]
loss: 0.017009 [1026048/1237259]
loss: 0.014523 [1230848/1237259]
Epoch 138
-------------------------------
loss: 0.016332 [ 2048/1237259]
loss: 0.014315 [206848/1237259]
loss: 0.015052 [411648/1237259]
loss: 0.015104 [616448/1237259]
loss: 0.016395 [821248/1237259]
loss: 0.015414 [1026048/1237259]
loss: 0.016696 [1230848/1237259]
Epoch 139
-------------------------------
loss: 0.015101 [ 2048/1237259]
loss: 0.015494 [206848/1237259]
loss: 0.015348 [411648/1237259]
loss: 0.015028 [616448/1237259]
loss: 0.013904 [821248/1237259]
loss: 0.014493 [1026048/1237259]
loss: 0.017235 [1230848/1237259]
Epoch 140
-------------------------------
loss: 0.014749 [ 2048/1237259]
loss: 0.016450 [206848/1237259]
loss: 0.012868 [411648/1237259]
loss: 0.017412 [616448/1237259]
loss: 0.014860 [821248/1237259]
loss: 0.014525 [1026048/1237259]
loss: 0.013330 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0555  
ndcg@20: 0.0448  
diversity: 0.1784  


Epoch 141
-------------------------------
loss: 0.014314 [ 2048/1237259]
loss: 0.015047 [206848/1237259]
loss: 0.013484 [411648/1237259]
loss: 0.015168 [616448/1237259]
loss: 0.013372 [821248/1237259]
loss: 0.016000 [1026048/1237259]
loss: 0.015013 [1230848/1237259]
Epoch 142
-------------------------------
loss: 0.014997 [ 2048/1237259]
loss: 0.015309 [206848/1237259]
loss: 0.011763 [411648/1237259]
loss: 0.012735 [616448/1237259]
loss: 0.014679 [821248/1237259]
loss: 0.014067 [1026048/1237259]
loss: 0.012584 [1230848/1237259]
Epoch 143
-------------------------------
loss: 0.013922 [ 2048/1237259]
loss: 0.014565 [206848/1237259]
loss: 0.014964 [411648/1237259]
loss: 0.016295 [616448/1237259]
loss: 0.014857 [821248/1237259]
loss: 0.017054 [1026048/1237259]
loss: 0.014676 [1230848/1237259]
Epoch 144
-------------------------------
loss: 0.013115 [ 2048/1237259]
loss: 0.013400 [206848/1237259]
loss: 0.015117 [411648/1237259]
loss: 0.012742 [616448/1237259]
loss: 0.013605 [821248/1237259]
loss: 0.012758 [1026048/1237259]
loss: 0.012330 [1230848/1237259]
Epoch 145
-------------------------------
loss: 0.014715 [ 2048/1237259]
loss: 0.014640 [206848/1237259]
loss: 0.015408 [411648/1237259]
loss: 0.015738 [616448/1237259]
loss: 0.013074 [821248/1237259]
loss: 0.013165 [1026048/1237259]
loss: 0.017727 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0558  
ndcg@20: 0.0450  
diversity: 0.1787  


Epoch 146
-------------------------------
loss: 0.013489 [ 2048/1237259]
loss: 0.012785 [206848/1237259]
loss: 0.014747 [411648/1237259]
loss: 0.014500 [616448/1237259]
loss: 0.014579 [821248/1237259]
loss: 0.014924 [1026048/1237259]
loss: 0.014271 [1230848/1237259]
Epoch 147
-------------------------------
loss: 0.014345 [ 2048/1237259]
loss: 0.014275 [206848/1237259]
loss: 0.013810 [411648/1237259]
loss: 0.015453 [616448/1237259]
loss: 0.015470 [821248/1237259]
loss: 0.013795 [1026048/1237259]
loss: 0.014065 [1230848/1237259]
Epoch 148
-------------------------------
loss: 0.014011 [ 2048/1237259]
loss: 0.013974 [206848/1237259]
loss: 0.014585 [411648/1237259]
loss: 0.014420 [616448/1237259]
loss: 0.015185 [821248/1237259]
loss: 0.016373 [1026048/1237259]
loss: 0.013177 [1230848/1237259]
Epoch 149
-------------------------------
loss: 0.014096 [ 2048/1237259]
loss: 0.016139 [206848/1237259]
loss: 0.016293 [411648/1237259]
loss: 0.014256 [616448/1237259]
loss: 0.015804 [821248/1237259]
loss: 0.013693 [1026048/1237259]
loss: 0.013400 [1230848/1237259]
Epoch 150
-------------------------------
loss: 0.016167 [ 2048/1237259]
loss: 0.013487 [206848/1237259]
loss: 0.014887 [411648/1237259]
loss: 0.014953 [616448/1237259]
loss: 0.014820 [821248/1237259]
loss: 0.014568 [1026048/1237259]
loss: 0.015085 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0563  
ndcg@20: 0.0454  
diversity: 0.1788  


Epoch 151
-------------------------------
loss: 0.016422 [ 2048/1237259]
loss: 0.013612 [206848/1237259]
loss: 0.013678 [411648/1237259]
loss: 0.012711 [616448/1237259]
loss: 0.014748 [821248/1237259]
loss: 0.015247 [1026048/1237259]
loss: 0.014174 [1230848/1237259]
Epoch 152
-------------------------------
loss: 0.013074 [ 2048/1237259]
loss: 0.012552 [206848/1237259]
loss: 0.014099 [411648/1237259]
loss: 0.013021 [616448/1237259]
loss: 0.015345 [821248/1237259]
loss: 0.014829 [1026048/1237259]
loss: 0.014968 [1230848/1237259]
Epoch 153
-------------------------------
loss: 0.013771 [ 2048/1237259]
loss: 0.013480 [206848/1237259]
loss: 0.016382 [411648/1237259]
loss: 0.014280 [616448/1237259]
loss: 0.013536 [821248/1237259]
loss: 0.013497 [1026048/1237259]
loss: 0.016141 [1230848/1237259]
Epoch 154
-------------------------------
loss: 0.015780 [ 2048/1237259]
loss: 0.014027 [206848/1237259]
loss: 0.015392 [411648/1237259]
loss: 0.013340 [616448/1237259]
loss: 0.015005 [821248/1237259]
loss: 0.015829 [1026048/1237259]
loss: 0.015134 [1230848/1237259]
Epoch 155
-------------------------------
loss: 0.013014 [ 2048/1237259]
loss: 0.015908 [206848/1237259]
loss: 0.014179 [411648/1237259]
loss: 0.013457 [616448/1237259]
loss: 0.016178 [821248/1237259]
loss: 0.012596 [1026048/1237259]
loss: 0.014322 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0560  
ndcg@20: 0.0452  
diversity: 0.1788  


Epoch 156
-------------------------------
loss: 0.013369 [ 2048/1237259]
loss: 0.014446 [206848/1237259]
loss: 0.014724 [411648/1237259]
loss: 0.015450 [616448/1237259]
loss: 0.014424 [821248/1237259]
loss: 0.013495 [1026048/1237259]
loss: 0.013085 [1230848/1237259]
Epoch 157
-------------------------------
loss: 0.015039 [ 2048/1237259]
loss: 0.015226 [206848/1237259]
loss: 0.015670 [411648/1237259]
loss: 0.014017 [616448/1237259]
loss: 0.013452 [821248/1237259]
loss: 0.014340 [1026048/1237259]
loss: 0.014228 [1230848/1237259]
Epoch 158
-------------------------------
loss: 0.014895 [ 2048/1237259]
loss: 0.013640 [206848/1237259]
loss: 0.014320 [411648/1237259]
loss: 0.012730 [616448/1237259]
loss: 0.012107 [821248/1237259]
loss: 0.013297 [1026048/1237259]
loss: 0.015634 [1230848/1237259]
Epoch 159
-------------------------------
loss: 0.014252 [ 2048/1237259]
loss: 0.015372 [206848/1237259]
loss: 0.012869 [411648/1237259]
loss: 0.015865 [616448/1237259]
loss: 0.012290 [821248/1237259]
loss: 0.012760 [1026048/1237259]
loss: 0.014721 [1230848/1237259]
Epoch 160
-------------------------------
loss: 0.015009 [ 2048/1237259]
loss: 0.013463 [206848/1237259]
loss: 0.013874 [411648/1237259]
loss: 0.013730 [616448/1237259]
loss: 0.016660 [821248/1237259]
loss: 0.015243 [1026048/1237259]
loss: 0.014502 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0563  
ndcg@20: 0.0455  
diversity: 0.1791  


Epoch 161
-------------------------------
loss: 0.014789 [ 2048/1237259]
loss: 0.014390 [206848/1237259]
loss: 0.013988 [411648/1237259]
loss: 0.015613 [616448/1237259]
loss: 0.014726 [821248/1237259]
loss: 0.013047 [1026048/1237259]
loss: 0.014109 [1230848/1237259]
Epoch 162
-------------------------------
loss: 0.015303 [ 2048/1237259]
loss: 0.015071 [206848/1237259]
loss: 0.012834 [411648/1237259]
loss: 0.015262 [616448/1237259]
loss: 0.014093 [821248/1237259]
loss: 0.015314 [1026048/1237259]
loss: 0.015762 [1230848/1237259]
Epoch 163
-------------------------------
loss: 0.015432 [ 2048/1237259]
loss: 0.014343 [206848/1237259]
loss: 0.013762 [411648/1237259]
loss: 0.014458 [616448/1237259]
loss: 0.014366 [821248/1237259]
loss: 0.015513 [1026048/1237259]
loss: 0.013906 [1230848/1237259]
Epoch 164
-------------------------------
loss: 0.013552 [ 2048/1237259]
loss: 0.015063 [206848/1237259]
loss: 0.014054 [411648/1237259]
loss: 0.016533 [616448/1237259]
loss: 0.014293 [821248/1237259]
loss: 0.014393 [1026048/1237259]
loss: 0.015514 [1230848/1237259]
Epoch 165
-------------------------------
loss: 0.012025 [ 2048/1237259]
loss: 0.012259 [206848/1237259]
loss: 0.012801 [411648/1237259]
loss: 0.014474 [616448/1237259]
loss: 0.015919 [821248/1237259]
loss: 0.014794 [1026048/1237259]
loss: 0.015993 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0564  
ndcg@20: 0.0456  
diversity: 0.1794  


Epoch 166
-------------------------------
loss: 0.014670 [ 2048/1237259]
loss: 0.015476 [206848/1237259]
loss: 0.013097 [411648/1237259]
loss: 0.012576 [616448/1237259]
loss: 0.014246 [821248/1237259]
loss: 0.014092 [1026048/1237259]
loss: 0.013343 [1230848/1237259]
Epoch 167
-------------------------------
loss: 0.015402 [ 2048/1237259]
loss: 0.013252 [206848/1237259]
loss: 0.012753 [411648/1237259]
loss: 0.014786 [616448/1237259]
loss: 0.013210 [821248/1237259]
loss: 0.014347 [1026048/1237259]
loss: 0.015588 [1230848/1237259]
Epoch 168
-------------------------------
loss: 0.014803 [ 2048/1237259]
loss: 0.012486 [206848/1237259]
loss: 0.013343 [411648/1237259]
loss: 0.016599 [616448/1237259]
loss: 0.014894 [821248/1237259]
loss: 0.013542 [1026048/1237259]
loss: 0.014063 [1230848/1237259]
Epoch 169
-------------------------------
loss: 0.012559 [ 2048/1237259]
loss: 0.013254 [206848/1237259]
loss: 0.015326 [411648/1237259]
loss: 0.012810 [616448/1237259]
loss: 0.013984 [821248/1237259]
loss: 0.013732 [1026048/1237259]
loss: 0.014711 [1230848/1237259]
Epoch 170
-------------------------------
loss: 0.014380 [ 2048/1237259]
loss: 0.013907 [206848/1237259]
loss: 0.014423 [411648/1237259]
loss: 0.014751 [616448/1237259]
loss: 0.012633 [821248/1237259]
loss: 0.015420 [1026048/1237259]
loss: 0.011966 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0564  
ndcg@20: 0.0456  
diversity: 0.1795  


Epoch 171
-------------------------------
loss: 0.014043 [ 2048/1237259]
loss: 0.014865 [206848/1237259]
loss: 0.015491 [411648/1237259]
loss: 0.011843 [616448/1237259]
loss: 0.013807 [821248/1237259]
loss: 0.014809 [1026048/1237259]
loss: 0.014369 [1230848/1237259]
Epoch 172
-------------------------------
loss: 0.013811 [ 2048/1237259]
loss: 0.013840 [206848/1237259]
loss: 0.014793 [411648/1237259]
loss: 0.014295 [616448/1237259]
loss: 0.014988 [821248/1237259]
loss: 0.013219 [1026048/1237259]
loss: 0.013337 [1230848/1237259]
Epoch 173
-------------------------------
loss: 0.012918 [ 2048/1237259]
loss: 0.014845 [206848/1237259]
loss: 0.014135 [411648/1237259]
loss: 0.014192 [616448/1237259]
loss: 0.013758 [821248/1237259]
loss: 0.012758 [1026048/1237259]
loss: 0.013347 [1230848/1237259]
Epoch 174
-------------------------------
loss: 0.012947 [ 2048/1237259]
loss: 0.012259 [206848/1237259]
loss: 0.014931 [411648/1237259]
loss: 0.013475 [616448/1237259]
loss: 0.015099 [821248/1237259]
loss: 0.013900 [1026048/1237259]
loss: 0.014934 [1230848/1237259]
Epoch 175
-------------------------------
loss: 0.013448 [ 2048/1237259]
loss: 0.016665 [206848/1237259]
loss: 0.014504 [411648/1237259]
loss: 0.016430 [616448/1237259]
loss: 0.014937 [821248/1237259]
loss: 0.012650 [1026048/1237259]
loss: 0.014235 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0570  
ndcg@20: 0.0459  
diversity: 0.1795  


Epoch 176
-------------------------------
loss: 0.013198 [ 2048/1237259]
loss: 0.014887 [206848/1237259]
loss: 0.013075 [411648/1237259]
loss: 0.013129 [616448/1237259]
loss: 0.014553 [821248/1237259]
loss: 0.014662 [1026048/1237259]
loss: 0.014538 [1230848/1237259]
Epoch 177
-------------------------------
loss: 0.012241 [ 2048/1237259]
loss: 0.013691 [206848/1237259]
loss: 0.013141 [411648/1237259]
loss: 0.012861 [616448/1237259]
loss: 0.013456 [821248/1237259]
loss: 0.015306 [1026048/1237259]
loss: 0.013121 [1230848/1237259]
Epoch 178
-------------------------------
loss: 0.014049 [ 2048/1237259]
loss: 0.012992 [206848/1237259]
loss: 0.012955 [411648/1237259]
loss: 0.015489 [616448/1237259]
loss: 0.014783 [821248/1237259]
loss: 0.014105 [1026048/1237259]
loss: 0.014463 [1230848/1237259]
Epoch 179
-------------------------------
loss: 0.014776 [ 2048/1237259]
loss: 0.014858 [206848/1237259]
loss: 0.012765 [411648/1237259]
loss: 0.014627 [616448/1237259]
loss: 0.014956 [821248/1237259]
loss: 0.014000 [1026048/1237259]
loss: 0.015941 [1230848/1237259]
Epoch 180
-------------------------------
loss: 0.014387 [ 2048/1237259]
loss: 0.012565 [206848/1237259]
loss: 0.014616 [411648/1237259]
loss: 0.015974 [616448/1237259]
loss: 0.014799 [821248/1237259]
loss: 0.015897 [1026048/1237259]
loss: 0.013587 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0569  
ndcg@20: 0.0460  
diversity: 0.1797  


Epoch 181
-------------------------------
loss: 0.013076 [ 2048/1237259]
loss: 0.015710 [206848/1237259]
loss: 0.013429 [411648/1237259]
loss: 0.014167 [616448/1237259]
loss: 0.014170 [821248/1237259]
loss: 0.014098 [1026048/1237259]
loss: 0.013060 [1230848/1237259]
Epoch 182
-------------------------------
loss: 0.012993 [ 2048/1237259]
loss: 0.013477 [206848/1237259]
loss: 0.013868 [411648/1237259]
loss: 0.013645 [616448/1237259]
loss: 0.015166 [821248/1237259]
loss: 0.014378 [1026048/1237259]
loss: 0.013724 [1230848/1237259]
Epoch 183
-------------------------------
loss: 0.015258 [ 2048/1237259]
loss: 0.014913 [206848/1237259]
loss: 0.017291 [411648/1237259]
loss: 0.012114 [616448/1237259]
loss: 0.014411 [821248/1237259]
loss: 0.015005 [1026048/1237259]
loss: 0.013417 [1230848/1237259]
Epoch 184
-------------------------------
loss: 0.014468 [ 2048/1237259]
loss: 0.013495 [206848/1237259]
loss: 0.014687 [411648/1237259]
loss: 0.012273 [616448/1237259]
loss: 0.015192 [821248/1237259]
loss: 0.013805 [1026048/1237259]
loss: 0.016428 [1230848/1237259]
Epoch 185
-------------------------------
loss: 0.013561 [ 2048/1237259]
loss: 0.012768 [206848/1237259]
loss: 0.014211 [411648/1237259]
loss: 0.013288 [616448/1237259]
loss: 0.014733 [821248/1237259]
loss: 0.013465 [1026048/1237259]
loss: 0.013864 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0571  
ndcg@20: 0.0462  
diversity: 0.1799  


Epoch 186
-------------------------------
loss: 0.013256 [ 2048/1237259]
loss: 0.014761 [206848/1237259]
loss: 0.014121 [411648/1237259]
loss: 0.014503 [616448/1237259]
loss: 0.013173 [821248/1237259]
loss: 0.015293 [1026048/1237259]
loss: 0.013649 [1230848/1237259]
Epoch 187
-------------------------------
loss: 0.015373 [ 2048/1237259]
loss: 0.013492 [206848/1237259]
loss: 0.012500 [411648/1237259]
loss: 0.014652 [616448/1237259]
loss: 0.013756 [821248/1237259]
loss: 0.013226 [1026048/1237259]
loss: 0.013917 [1230848/1237259]
Epoch 188
-------------------------------
loss: 0.014099 [ 2048/1237259]
loss: 0.014100 [206848/1237259]
loss: 0.014890 [411648/1237259]
loss: 0.013573 [616448/1237259]
loss: 0.014319 [821248/1237259]
loss: 0.013758 [1026048/1237259]
loss: 0.014307 [1230848/1237259]
Epoch 189
-------------------------------
loss: 0.012720 [ 2048/1237259]
loss: 0.012354 [206848/1237259]
loss: 0.014041 [411648/1237259]
loss: 0.012192 [616448/1237259]
loss: 0.011542 [821248/1237259]
loss: 0.013099 [1026048/1237259]
loss: 0.013017 [1230848/1237259]
Epoch 190
-------------------------------
loss: 0.016918 [ 2048/1237259]
loss: 0.015114 [206848/1237259]
loss: 0.015394 [411648/1237259]
loss: 0.013663 [616448/1237259]
loss: 0.014318 [821248/1237259]
loss: 0.015202 [1026048/1237259]
loss: 0.012842 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0573  
ndcg@20: 0.0464  
diversity: 0.1802  


Epoch 191
-------------------------------
loss: 0.013188 [ 2048/1237259]
loss: 0.015119 [206848/1237259]
loss: 0.012947 [411648/1237259]
loss: 0.013997 [616448/1237259]
loss: 0.013133 [821248/1237259]
loss: 0.014339 [1026048/1237259]
loss: 0.013132 [1230848/1237259]
Epoch 192
-------------------------------
loss: 0.013603 [ 2048/1237259]
loss: 0.013585 [206848/1237259]
loss: 0.013291 [411648/1237259]
loss: 0.015428 [616448/1237259]
loss: 0.013374 [821248/1237259]
loss: 0.013781 [1026048/1237259]
loss: 0.015550 [1230848/1237259]
Epoch 193
-------------------------------
loss: 0.014951 [ 2048/1237259]
loss: 0.014253 [206848/1237259]
loss: 0.013849 [411648/1237259]
loss: 0.014351 [616448/1237259]
loss: 0.013889 [821248/1237259]
loss: 0.012619 [1026048/1237259]
loss: 0.012754 [1230848/1237259]
Epoch 194
-------------------------------
loss: 0.014025 [ 2048/1237259]
loss: 0.013118 [206848/1237259]
loss: 0.013783 [411648/1237259]
loss: 0.014525 [616448/1237259]
loss: 0.013190 [821248/1237259]
loss: 0.014966 [1026048/1237259]
loss: 0.014507 [1230848/1237259]
Epoch 195
-------------------------------
loss: 0.014459 [ 2048/1237259]
loss: 0.013112 [206848/1237259]
loss: 0.013789 [411648/1237259]
loss: 0.014286 [616448/1237259]
loss: 0.012493 [821248/1237259]
loss: 0.012783 [1026048/1237259]
loss: 0.013292 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0576  
ndcg@20: 0.0466  
diversity: 0.1803  


Epoch 196
-------------------------------
loss: 0.012103 [ 2048/1237259]
loss: 0.015713 [206848/1237259]
loss: 0.015274 [411648/1237259]
loss: 0.014250 [616448/1237259]
loss: 0.015947 [821248/1237259]
loss: 0.014358 [1026048/1237259]
loss: 0.014030 [1230848/1237259]
Epoch 197
-------------------------------
loss: 0.014548 [ 2048/1237259]
loss: 0.014177 [206848/1237259]
loss: 0.013109 [411648/1237259]
loss: 0.013941 [616448/1237259]
loss: 0.012519 [821248/1237259]
loss: 0.014715 [1026048/1237259]
loss: 0.012442 [1230848/1237259]
Epoch 198
-------------------------------
loss: 0.016534 [ 2048/1237259]
loss: 0.016294 [206848/1237259]
loss: 0.014950 [411648/1237259]
loss: 0.013832 [616448/1237259]
loss: 0.012953 [821248/1237259]
loss: 0.016759 [1026048/1237259]
loss: 0.013343 [1230848/1237259]
Epoch 199
-------------------------------
loss: 0.014087 [ 2048/1237259]
loss: 0.017682 [206848/1237259]
loss: 0.014385 [411648/1237259]
loss: 0.015238 [616448/1237259]
loss: 0.013410 [821248/1237259]
loss: 0.013834 [1026048/1237259]
loss: 0.013510 [1230848/1237259]
Epoch 200
-------------------------------
loss: 0.014041 [ 2048/1237259]
loss: 0.014948 [206848/1237259]
loss: 0.014322 [411648/1237259]
loss: 0.013183 [616448/1237259]
loss: 0.012790 [821248/1237259]
loss: 0.012124 [1026048/1237259]
loss: 0.015100 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0579  
ndcg@20: 0.0468  
diversity: 0.1804  


Epoch 201
-------------------------------
loss: 0.013256 [ 2048/1237259]
loss: 0.012421 [206848/1237259]
loss: 0.013087 [411648/1237259]
loss: 0.011656 [616448/1237259]
loss: 0.014017 [821248/1237259]
loss: 0.015152 [1026048/1237259]
loss: 0.013809 [1230848/1237259]
Epoch 202
-------------------------------
loss: 0.011784 [ 2048/1237259]
loss: 0.015674 [206848/1237259]
loss: 0.013149 [411648/1237259]
loss: 0.015611 [616448/1237259]
loss: 0.012643 [821248/1237259]
loss: 0.012348 [1026048/1237259]
loss: 0.015266 [1230848/1237259]
Epoch 203
-------------------------------
loss: 0.013782 [ 2048/1237259]
loss: 0.014095 [206848/1237259]
loss: 0.014379 [411648/1237259]
loss: 0.013758 [616448/1237259]
loss: 0.014437 [821248/1237259]
loss: 0.014559 [1026048/1237259]
loss: 0.014456 [1230848/1237259]
Epoch 204
-------------------------------
loss: 0.013332 [ 2048/1237259]
loss: 0.014457 [206848/1237259]
loss: 0.014533 [411648/1237259]
loss: 0.014903 [616448/1237259]
loss: 0.014873 [821248/1237259]
loss: 0.013595 [1026048/1237259]
loss: 0.016024 [1230848/1237259]
Epoch 205
-------------------------------
loss: 0.012969 [ 2048/1237259]
loss: 0.012485 [206848/1237259]
loss: 0.014670 [411648/1237259]
loss: 0.014576 [616448/1237259]
loss: 0.015001 [821248/1237259]
loss: 0.014827 [1026048/1237259]
loss: 0.013162 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0581  
ndcg@20: 0.0469  
diversity: 0.1805  


Epoch 206
-------------------------------
loss: 0.012647 [ 2048/1237259]
loss: 0.015225 [206848/1237259]
loss: 0.015452 [411648/1237259]
loss: 0.014263 [616448/1237259]
loss: 0.014382 [821248/1237259]
loss: 0.012035 [1026048/1237259]
loss: 0.014783 [1230848/1237259]
Epoch 207
-------------------------------
loss: 0.016270 [ 2048/1237259]
loss: 0.012305 [206848/1237259]
loss: 0.014361 [411648/1237259]
loss: 0.014708 [616448/1237259]
loss: 0.012497 [821248/1237259]
loss: 0.013437 [1026048/1237259]
loss: 0.015237 [1230848/1237259]
Epoch 208
-------------------------------
loss: 0.014456 [ 2048/1237259]
loss: 0.014599 [206848/1237259]
loss: 0.014144 [411648/1237259]
loss: 0.014654 [616448/1237259]
loss: 0.012763 [821248/1237259]
loss: 0.014133 [1026048/1237259]
loss: 0.012600 [1230848/1237259]
Epoch 209
-------------------------------
loss: 0.013781 [ 2048/1237259]
loss: 0.013879 [206848/1237259]
loss: 0.014346 [411648/1237259]
loss: 0.012230 [616448/1237259]
loss: 0.013778 [821248/1237259]
loss: 0.013803 [1026048/1237259]
loss: 0.014055 [1230848/1237259]
Epoch 210
-------------------------------
loss: 0.014255 [ 2048/1237259]
loss: 0.013262 [206848/1237259]
loss: 0.012799 [411648/1237259]
loss: 0.014035 [616448/1237259]
loss: 0.012563 [821248/1237259]
loss: 0.012615 [1026048/1237259]
loss: 0.013986 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0583  
ndcg@20: 0.0471  
diversity: 0.1806  


Epoch 211
-------------------------------
loss: 0.012986 [ 2048/1237259]
loss: 0.016703 [206848/1237259]
loss: 0.015415 [411648/1237259]
loss: 0.014281 [616448/1237259]
loss: 0.013195 [821248/1237259]
loss: 0.015019 [1026048/1237259]
loss: 0.014728 [1230848/1237259]
Epoch 212
-------------------------------
loss: 0.013266 [ 2048/1237259]
loss: 0.013612 [206848/1237259]
loss: 0.013511 [411648/1237259]
loss: 0.015741 [616448/1237259]
loss: 0.013151 [821248/1237259]
loss: 0.013548 [1026048/1237259]
loss: 0.013233 [1230848/1237259]
Epoch 213
-------------------------------
loss: 0.014047 [ 2048/1237259]
loss: 0.013153 [206848/1237259]
loss: 0.012974 [411648/1237259]
loss: 0.012041 [616448/1237259]
loss: 0.014567 [821248/1237259]
loss: 0.012338 [1026048/1237259]
loss: 0.014907 [1230848/1237259]
Epoch 214
-------------------------------
loss: 0.014669 [ 2048/1237259]
loss: 0.013684 [206848/1237259]
loss: 0.014360 [411648/1237259]
loss: 0.013934 [616448/1237259]
loss: 0.014193 [821248/1237259]
loss: 0.013492 [1026048/1237259]
loss: 0.012583 [1230848/1237259]
Epoch 215
-------------------------------
loss: 0.015101 [ 2048/1237259]
loss: 0.014143 [206848/1237259]
loss: 0.014304 [411648/1237259]
loss: 0.013993 [616448/1237259]
loss: 0.012715 [821248/1237259]
loss: 0.013367 [1026048/1237259]
loss: 0.013783 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0583  
ndcg@20: 0.0470  
diversity: 0.1806  


Epoch 216
-------------------------------
loss: 0.013783 [ 2048/1237259]
loss: 0.013588 [206848/1237259]
loss: 0.012101 [411648/1237259]
loss: 0.012463 [616448/1237259]
loss: 0.013236 [821248/1237259]
loss: 0.013751 [1026048/1237259]
loss: 0.015085 [1230848/1237259]
Epoch 217
-------------------------------
loss: 0.012177 [ 2048/1237259]
loss: 0.012682 [206848/1237259]
loss: 0.014616 [411648/1237259]
loss: 0.013441 [616448/1237259]
loss: 0.014505 [821248/1237259]
loss: 0.013342 [1026048/1237259]
loss: 0.013608 [1230848/1237259]
Epoch 218
-------------------------------
loss: 0.013614 [ 2048/1237259]
loss: 0.014016 [206848/1237259]
loss: 0.013021 [411648/1237259]
loss: 0.013224 [616448/1237259]
loss: 0.014555 [821248/1237259]
loss: 0.013995 [1026048/1237259]
loss: 0.013456 [1230848/1237259]
Epoch 219
-------------------------------
loss: 0.014029 [ 2048/1237259]
loss: 0.014749 [206848/1237259]
loss: 0.013846 [411648/1237259]
loss: 0.012030 [616448/1237259]
loss: 0.014236 [821248/1237259]
loss: 0.015258 [1026048/1237259]
loss: 0.014970 [1230848/1237259]
Epoch 220
-------------------------------
loss: 0.014264 [ 2048/1237259]
loss: 0.011820 [206848/1237259]
loss: 0.013386 [411648/1237259]
loss: 0.013780 [616448/1237259]
loss: 0.014681 [821248/1237259]
loss: 0.013034 [1026048/1237259]
loss: 0.012719 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0582  
ndcg@20: 0.0470  
diversity: 0.1807  


Epoch 221
-------------------------------
loss: 0.013616 [ 2048/1237259]
loss: 0.014539 [206848/1237259]
loss: 0.013172 [411648/1237259]
loss: 0.016434 [616448/1237259]
loss: 0.012908 [821248/1237259]
loss: 0.015159 [1026048/1237259]
loss: 0.012318 [1230848/1237259]
Epoch 222
-------------------------------
loss: 0.013033 [ 2048/1237259]
loss: 0.014626 [206848/1237259]
loss: 0.016279 [411648/1237259]
loss: 0.012627 [616448/1237259]
loss: 0.015523 [821248/1237259]
loss: 0.015242 [1026048/1237259]
loss: 0.013112 [1230848/1237259]
Epoch 223
-------------------------------
loss: 0.014834 [ 2048/1237259]
loss: 0.014302 [206848/1237259]
loss: 0.012250 [411648/1237259]
loss: 0.013556 [616448/1237259]
loss: 0.015143 [821248/1237259]
loss: 0.013982 [1026048/1237259]
loss: 0.015265 [1230848/1237259]
Epoch 224
-------------------------------
loss: 0.015103 [ 2048/1237259]
loss: 0.014077 [206848/1237259]
loss: 0.014199 [411648/1237259]
loss: 0.013690 [616448/1237259]
loss: 0.014653 [821248/1237259]
loss: 0.013489 [1026048/1237259]
loss: 0.012286 [1230848/1237259]
Epoch 225
-------------------------------
loss: 0.013439 [ 2048/1237259]
loss: 0.013531 [206848/1237259]
loss: 0.013858 [411648/1237259]
loss: 0.014046 [616448/1237259]
loss: 0.014405 [821248/1237259]
loss: 0.013313 [1026048/1237259]
loss: 0.015138 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0586  
ndcg@20: 0.0472  
diversity: 0.1808  


Epoch 226
-------------------------------
loss: 0.011712 [ 2048/1237259]
loss: 0.013963 [206848/1237259]
loss: 0.013987 [411648/1237259]
loss: 0.014392 [616448/1237259]
loss: 0.013399 [821248/1237259]
loss: 0.013897 [1026048/1237259]
loss: 0.012354 [1230848/1237259]
Epoch 227
-------------------------------
loss: 0.013131 [ 2048/1237259]
loss: 0.014012 [206848/1237259]
loss: 0.012284 [411648/1237259]
loss: 0.015423 [616448/1237259]
loss: 0.012517 [821248/1237259]
loss: 0.013342 [1026048/1237259]
loss: 0.013488 [1230848/1237259]
Epoch 228
-------------------------------
loss: 0.015850 [ 2048/1237259]
loss: 0.014223 [206848/1237259]
loss: 0.013815 [411648/1237259]
loss: 0.012084 [616448/1237259]
loss: 0.013926 [821248/1237259]
loss: 0.012259 [1026048/1237259]
loss: 0.014034 [1230848/1237259]
Epoch 229
-------------------------------
loss: 0.013394 [ 2048/1237259]
loss: 0.014917 [206848/1237259]
loss: 0.012561 [411648/1237259]
loss: 0.012640 [616448/1237259]
loss: 0.014931 [821248/1237259]
loss: 0.014601 [1026048/1237259]
loss: 0.012966 [1230848/1237259]
Epoch 230
-------------------------------
loss: 0.013712 [ 2048/1237259]
loss: 0.013916 [206848/1237259]
loss: 0.013737 [411648/1237259]
loss: 0.014757 [616448/1237259]
loss: 0.016310 [821248/1237259]
loss: 0.014805 [1026048/1237259]
loss: 0.013837 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0588  
ndcg@20: 0.0474  
diversity: 0.1811  


Epoch 231
-------------------------------
loss: 0.013197 [ 2048/1237259]
loss: 0.014262 [206848/1237259]
loss: 0.013601 [411648/1237259]
loss: 0.014830 [616448/1237259]
loss: 0.013028 [821248/1237259]
loss: 0.013476 [1026048/1237259]
loss: 0.012414 [1230848/1237259]
Epoch 232
-------------------------------
loss: 0.012471 [ 2048/1237259]
loss: 0.013805 [206848/1237259]
loss: 0.013578 [411648/1237259]
loss: 0.013422 [616448/1237259]
loss: 0.010986 [821248/1237259]
loss: 0.013060 [1026048/1237259]
loss: 0.013813 [1230848/1237259]
Epoch 233
-------------------------------
loss: 0.013749 [ 2048/1237259]
loss: 0.013920 [206848/1237259]
loss: 0.014309 [411648/1237259]
loss: 0.015450 [616448/1237259]
loss: 0.013816 [821248/1237259]
loss: 0.012566 [1026048/1237259]
loss: 0.013411 [1230848/1237259]
Epoch 234
-------------------------------
loss: 0.014235 [ 2048/1237259]
loss: 0.013957 [206848/1237259]
loss: 0.013063 [411648/1237259]
loss: 0.013214 [616448/1237259]
loss: 0.014079 [821248/1237259]
loss: 0.011825 [1026048/1237259]
loss: 0.013199 [1230848/1237259]
Epoch 235
-------------------------------
loss: 0.014088 [ 2048/1237259]
loss: 0.013138 [206848/1237259]
loss: 0.012973 [411648/1237259]
loss: 0.012814 [616448/1237259]
loss: 0.013121 [821248/1237259]
loss: 0.014177 [1026048/1237259]
loss: 0.012326 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0586  
ndcg@20: 0.0473  
diversity: 0.1813  


Epoch 236
-------------------------------
loss: 0.012311 [ 2048/1237259]
loss: 0.015339 [206848/1237259]
loss: 0.012052 [411648/1237259]
loss: 0.013688 [616448/1237259]
loss: 0.014130 [821248/1237259]
loss: 0.014139 [1026048/1237259]
loss: 0.011303 [1230848/1237259]
Epoch 237
-------------------------------
loss: 0.012922 [ 2048/1237259]
loss: 0.015706 [206848/1237259]
loss: 0.016257 [411648/1237259]
loss: 0.013599 [616448/1237259]
loss: 0.014168 [821248/1237259]
loss: 0.014771 [1026048/1237259]
loss: 0.014518 [1230848/1237259]
Epoch 238
-------------------------------
loss: 0.012590 [ 2048/1237259]
loss: 0.013650 [206848/1237259]
loss: 0.012432 [411648/1237259]
loss: 0.014936 [616448/1237259]
loss: 0.012679 [821248/1237259]
loss: 0.013306 [1026048/1237259]
loss: 0.013567 [1230848/1237259]
Epoch 239
-------------------------------
loss: 0.014320 [ 2048/1237259]
loss: 0.013739 [206848/1237259]
loss: 0.012263 [411648/1237259]
loss: 0.012780 [616448/1237259]
loss: 0.012887 [821248/1237259]
loss: 0.012114 [1026048/1237259]
loss: 0.012241 [1230848/1237259]
Epoch 240
-------------------------------
loss: 0.014761 [ 2048/1237259]
loss: 0.012736 [206848/1237259]
loss: 0.012616 [411648/1237259]
loss: 0.013294 [616448/1237259]
loss: 0.013033 [821248/1237259]
loss: 0.012329 [1026048/1237259]
loss: 0.014121 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0587  
ndcg@20: 0.0474  
diversity: 0.1813  


Epoch 241
-------------------------------
loss: 0.013211 [ 2048/1237259]
loss: 0.014910 [206848/1237259]
loss: 0.014990 [411648/1237259]
loss: 0.014641 [616448/1237259]
loss: 0.013529 [821248/1237259]
loss: 0.014451 [1026048/1237259]
loss: 0.013692 [1230848/1237259]
Epoch 242
-------------------------------
loss: 0.016137 [ 2048/1237259]
loss: 0.012271 [206848/1237259]
loss: 0.016118 [411648/1237259]
loss: 0.014496 [616448/1237259]
loss: 0.013864 [821248/1237259]
loss: 0.013386 [1026048/1237259]
loss: 0.012798 [1230848/1237259]
Epoch 243
-------------------------------
loss: 0.013415 [ 2048/1237259]
loss: 0.012550 [206848/1237259]
loss: 0.014611 [411648/1237259]
loss: 0.014869 [616448/1237259]
loss: 0.012668 [821248/1237259]
loss: 0.011915 [1026048/1237259]
loss: 0.013356 [1230848/1237259]
Epoch 244
-------------------------------
loss: 0.015458 [ 2048/1237259]
loss: 0.014701 [206848/1237259]
loss: 0.013288 [411648/1237259]
loss: 0.013256 [616448/1237259]
loss: 0.014567 [821248/1237259]
loss: 0.012560 [1026048/1237259]
loss: 0.015254 [1230848/1237259]
Epoch 245
-------------------------------
loss: 0.013019 [ 2048/1237259]
loss: 0.012194 [206848/1237259]
loss: 0.013487 [411648/1237259]
loss: 0.013511 [616448/1237259]
loss: 0.011955 [821248/1237259]
loss: 0.012938 [1026048/1237259]
loss: 0.013879 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0589  
ndcg@20: 0.0475  
diversity: 0.1814  


Epoch 246
-------------------------------
loss: 0.014000 [ 2048/1237259]
loss: 0.012972 [206848/1237259]
loss: 0.013517 [411648/1237259]
loss: 0.014570 [616448/1237259]
loss: 0.013182 [821248/1237259]
loss: 0.013269 [1026048/1237259]
loss: 0.013513 [1230848/1237259]
Epoch 247
-------------------------------
loss: 0.012329 [ 2048/1237259]
loss: 0.013465 [206848/1237259]
loss: 0.012918 [411648/1237259]
loss: 0.016549 [616448/1237259]
loss: 0.014615 [821248/1237259]
loss: 0.013803 [1026048/1237259]
loss: 0.014287 [1230848/1237259]
Epoch 248
-------------------------------
loss: 0.012151 [ 2048/1237259]
loss: 0.012728 [206848/1237259]
loss: 0.013245 [411648/1237259]
loss: 0.013185 [616448/1237259]
loss: 0.012206 [821248/1237259]
loss: 0.013640 [1026048/1237259]
loss: 0.012723 [1230848/1237259]
Epoch 249
-------------------------------
loss: 0.014976 [ 2048/1237259]
loss: 0.013103 [206848/1237259]
loss: 0.013867 [411648/1237259]
loss: 0.013988 [616448/1237259]
loss: 0.013983 [821248/1237259]
loss: 0.012010 [1026048/1237259]
loss: 0.013258 [1230848/1237259]
Epoch 250
-------------------------------
loss: 0.013771 [ 2048/1237259]
loss: 0.012151 [206848/1237259]
loss: 0.013535 [411648/1237259]
loss: 0.013477 [616448/1237259]
loss: 0.014416 [821248/1237259]
loss: 0.012282 [1026048/1237259]
loss: 0.012007 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0590  
ndcg@20: 0.0475  
diversity: 0.1816  


Epoch 251
-------------------------------
loss: 0.013131 [ 2048/1237259]
loss: 0.014396 [206848/1237259]
loss: 0.013340 [411648/1237259]
loss: 0.014608 [616448/1237259]
loss: 0.013180 [821248/1237259]
loss: 0.013893 [1026048/1237259]
loss: 0.013253 [1230848/1237259]
Epoch 252
-------------------------------
loss: 0.011859 [ 2048/1237259]
loss: 0.015700 [206848/1237259]
loss: 0.012568 [411648/1237259]
loss: 0.013586 [616448/1237259]
loss: 0.014307 [821248/1237259]
loss: 0.014372 [1026048/1237259]
loss: 0.014479 [1230848/1237259]
Epoch 253
-------------------------------
loss: 0.016381 [ 2048/1237259]
loss: 0.014044 [206848/1237259]
loss: 0.011350 [411648/1237259]
loss: 0.015120 [616448/1237259]
loss: 0.014909 [821248/1237259]
loss: 0.011830 [1026048/1237259]
loss: 0.012938 [1230848/1237259]
Epoch 254
-------------------------------
loss: 0.013153 [ 2048/1237259]
loss: 0.014390 [206848/1237259]
loss: 0.014193 [411648/1237259]
loss: 0.016458 [616448/1237259]
loss: 0.012670 [821248/1237259]
loss: 0.015160 [1026048/1237259]
loss: 0.013292 [1230848/1237259]
Epoch 255
-------------------------------
loss: 0.012514 [ 2048/1237259]
loss: 0.013249 [206848/1237259]
loss: 0.013503 [411648/1237259]
loss: 0.011960 [616448/1237259]
loss: 0.014120 [821248/1237259]
loss: 0.012466 [1026048/1237259]
loss: 0.013440 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0590  
ndcg@20: 0.0476  
diversity: 0.1815  


Epoch 256
-------------------------------
loss: 0.014034 [ 2048/1237259]
loss: 0.014530 [206848/1237259]
loss: 0.015035 [411648/1237259]
loss: 0.014622 [616448/1237259]
loss: 0.015353 [821248/1237259]
loss: 0.013361 [1026048/1237259]
loss: 0.012926 [1230848/1237259]
Epoch 257
-------------------------------
loss: 0.013388 [ 2048/1237259]
loss: 0.012790 [206848/1237259]
loss: 0.015362 [411648/1237259]
loss: 0.013442 [616448/1237259]
loss: 0.012302 [821248/1237259]
loss: 0.013290 [1026048/1237259]
loss: 0.014304 [1230848/1237259]
Epoch 258
-------------------------------
loss: 0.014942 [ 2048/1237259]
loss: 0.012733 [206848/1237259]
loss: 0.013931 [411648/1237259]
loss: 0.013039 [616448/1237259]
loss: 0.015603 [821248/1237259]
loss: 0.012646 [1026048/1237259]
loss: 0.013285 [1230848/1237259]
Epoch 259
-------------------------------
loss: 0.014648 [ 2048/1237259]
loss: 0.012378 [206848/1237259]
loss: 0.014388 [411648/1237259]
loss: 0.013459 [616448/1237259]
loss: 0.014968 [821248/1237259]
loss: 0.013751 [1026048/1237259]
loss: 0.012990 [1230848/1237259]
Epoch 260
-------------------------------
loss: 0.012759 [ 2048/1237259]
loss: 0.013876 [206848/1237259]
loss: 0.012760 [411648/1237259]
loss: 0.012690 [616448/1237259]
loss: 0.013745 [821248/1237259]
loss: 0.014261 [1026048/1237259]
loss: 0.012354 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0593  
ndcg@20: 0.0478  
diversity: 0.1817  


Epoch 261
-------------------------------
loss: 0.012469 [ 2048/1237259]
loss: 0.015027 [206848/1237259]
loss: 0.012655 [411648/1237259]
loss: 0.011659 [616448/1237259]
loss: 0.014005 [821248/1237259]
loss: 0.012875 [1026048/1237259]
loss: 0.013597 [1230848/1237259]
Epoch 262
-------------------------------
loss: 0.013309 [ 2048/1237259]
loss: 0.012774 [206848/1237259]
loss: 0.011601 [411648/1237259]
loss: 0.013076 [616448/1237259]
loss: 0.013924 [821248/1237259]
loss: 0.012882 [1026048/1237259]
loss: 0.014842 [1230848/1237259]
Epoch 263
-------------------------------
loss: 0.014853 [ 2048/1237259]
loss: 0.012502 [206848/1237259]
loss: 0.014151 [411648/1237259]
loss: 0.014502 [616448/1237259]
loss: 0.013330 [821248/1237259]
loss: 0.013469 [1026048/1237259]
loss: 0.011733 [1230848/1237259]
Epoch 264
-------------------------------
loss: 0.012816 [ 2048/1237259]
loss: 0.015653 [206848/1237259]
loss: 0.012832 [411648/1237259]
loss: 0.012394 [616448/1237259]
loss: 0.012872 [821248/1237259]
loss: 0.012297 [1026048/1237259]
loss: 0.012832 [1230848/1237259]
Epoch 265
-------------------------------
loss: 0.013314 [ 2048/1237259]
loss: 0.013480 [206848/1237259]
loss: 0.013507 [411648/1237259]
loss: 0.012718 [616448/1237259]
loss: 0.013224 [821248/1237259]
loss: 0.014120 [1026048/1237259]
loss: 0.012021 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0593  
ndcg@20: 0.0479  
diversity: 0.1816  


Epoch 266
-------------------------------
loss: 0.012759 [ 2048/1237259]
loss: 0.014653 [206848/1237259]
loss: 0.012905 [411648/1237259]
loss: 0.013833 [616448/1237259]
loss: 0.014253 [821248/1237259]
loss: 0.012426 [1026048/1237259]
loss: 0.013612 [1230848/1237259]
Epoch 267
-------------------------------
loss: 0.012110 [ 2048/1237259]
loss: 0.013616 [206848/1237259]
loss: 0.013695 [411648/1237259]
loss: 0.012516 [616448/1237259]
loss: 0.012887 [821248/1237259]
loss: 0.014737 [1026048/1237259]
loss: 0.013021 [1230848/1237259]
Epoch 268
-------------------------------
loss: 0.012430 [ 2048/1237259]
loss: 0.014115 [206848/1237259]
loss: 0.014301 [411648/1237259]
loss: 0.015860 [616448/1237259]
loss: 0.013869 [821248/1237259]
loss: 0.012984 [1026048/1237259]
loss: 0.014342 [1230848/1237259]
Epoch 269
-------------------------------
loss: 0.012345 [ 2048/1237259]
loss: 0.011741 [206848/1237259]
loss: 0.015949 [411648/1237259]
loss: 0.013012 [616448/1237259]
loss: 0.012174 [821248/1237259]
loss: 0.013137 [1026048/1237259]
loss: 0.012907 [1230848/1237259]
Epoch 270
-------------------------------
loss: 0.012586 [ 2048/1237259]
loss: 0.012187 [206848/1237259]
loss: 0.013277 [411648/1237259]
loss: 0.012120 [616448/1237259]
loss: 0.011838 [821248/1237259]
loss: 0.013330 [1026048/1237259]
loss: 0.014531 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0594  
ndcg@20: 0.0480  
diversity: 0.1818  


Epoch 271
-------------------------------
loss: 0.013298 [ 2048/1237259]
loss: 0.012861 [206848/1237259]
loss: 0.012654 [411648/1237259]
loss: 0.012450 [616448/1237259]
loss: 0.013749 [821248/1237259]
loss: 0.013467 [1026048/1237259]
loss: 0.012470 [1230848/1237259]
Epoch 272
-------------------------------
loss: 0.013216 [ 2048/1237259]
loss: 0.012104 [206848/1237259]
loss: 0.014094 [411648/1237259]
loss: 0.012420 [616448/1237259]
loss: 0.016170 [821248/1237259]
loss: 0.013912 [1026048/1237259]
loss: 0.013158 [1230848/1237259]
Epoch 273
-------------------------------
loss: 0.013841 [ 2048/1237259]
loss: 0.014551 [206848/1237259]
loss: 0.014094 [411648/1237259]
loss: 0.012513 [616448/1237259]
loss: 0.012554 [821248/1237259]
loss: 0.014025 [1026048/1237259]
loss: 0.013264 [1230848/1237259]
Epoch 274
-------------------------------
loss: 0.014454 [ 2048/1237259]
loss: 0.012904 [206848/1237259]
loss: 0.014356 [411648/1237259]
loss: 0.014065 [616448/1237259]
loss: 0.013656 [821248/1237259]
loss: 0.012623 [1026048/1237259]
loss: 0.013491 [1230848/1237259]
Epoch 275
-------------------------------
loss: 0.013319 [ 2048/1237259]
loss: 0.014520 [206848/1237259]
loss: 0.012995 [411648/1237259]
loss: 0.011672 [616448/1237259]
loss: 0.012604 [821248/1237259]
loss: 0.012197 [1026048/1237259]
loss: 0.013511 [1230848/1237259]
Eval results: 
recall@20: 0.0592  
ndcg@20: 0.0479  
diversity: 0.1817  


Epoch 276
-------------------------------
loss: 0.014054 [ 2048/1237259]
loss: 0.013655 [206848/1237259]
loss: 0.011884 [411648/1237259]
loss: 0.013060 [616448/1237259]
loss: 0.013845 [821248/1237259]
loss: 0.012548 [1026048/1237259]
loss: 0.012185 [1230848/1237259]
Epoch 277
-------------------------------
loss: 0.014213 [ 2048/1237259]
loss: 0.012314 [206848/1237259]
loss: 0.011721 [411648/1237259]
loss: 0.013587 [616448/1237259]
loss: 0.014071 [821248/1237259]
loss: 0.014541 [1026048/1237259]
loss: 0.011617 [1230848/1237259]
Epoch 278
-------------------------------
loss: 0.013182 [ 2048/1237259]
loss: 0.014224 [206848/1237259]
loss: 0.013305 [411648/1237259]
loss: 0.012377 [616448/1237259]
loss: 0.013596 [821248/1237259]
loss: 0.014679 [1026048/1237259]
loss: 0.014067 [1230848/1237259]
Epoch 279
-------------------------------
loss: 0.011162 [ 2048/1237259]
loss: 0.013023 [206848/1237259]
loss: 0.012511 [411648/1237259]
loss: 0.014317 [616448/1237259]
loss: 0.011910 [821248/1237259]
loss: 0.013563 [1026048/1237259]
loss: 0.013912 [1230848/1237259]
Epoch 280
-------------------------------
loss: 0.012828 [ 2048/1237259]
loss: 0.012109 [206848/1237259]
loss: 0.015258 [411648/1237259]
loss: 0.015022 [616448/1237259]
loss: 0.013463 [821248/1237259]
loss: 0.013396 [1026048/1237259]
loss: 0.013825 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0593  
ndcg@20: 0.0480  
diversity: 0.1819  


Epoch 281
-------------------------------
loss: 0.012236 [ 2048/1237259]
loss: 0.012249 [206848/1237259]
loss: 0.012514 [411648/1237259]
loss: 0.013467 [616448/1237259]
loss: 0.013567 [821248/1237259]
loss: 0.013548 [1026048/1237259]
loss: 0.012577 [1230848/1237259]
Epoch 282
-------------------------------
loss: 0.014642 [ 2048/1237259]
loss: 0.012793 [206848/1237259]
loss: 0.013491 [411648/1237259]
loss: 0.013668 [616448/1237259]
loss: 0.013248 [821248/1237259]
loss: 0.012760 [1026048/1237259]
loss: 0.013221 [1230848/1237259]
Epoch 283
-------------------------------
loss: 0.013019 [ 2048/1237259]
loss: 0.013226 [206848/1237259]
loss: 0.014271 [411648/1237259]
loss: 0.013349 [616448/1237259]
loss: 0.011628 [821248/1237259]
loss: 0.012998 [1026048/1237259]
loss: 0.013283 [1230848/1237259]
Epoch 284
-------------------------------
loss: 0.014361 [ 2048/1237259]
loss: 0.012419 [206848/1237259]
loss: 0.015118 [411648/1237259]
loss: 0.012056 [616448/1237259]
loss: 0.012458 [821248/1237259]
loss: 0.012607 [1026048/1237259]
loss: 0.013584 [1230848/1237259]
Epoch 285
-------------------------------
loss: 0.012187 [ 2048/1237259]
loss: 0.012526 [206848/1237259]
loss: 0.012350 [411648/1237259]
loss: 0.012149 [616448/1237259]
loss: 0.014042 [821248/1237259]
loss: 0.013161 [1026048/1237259]
loss: 0.012206 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0597  
ndcg@20: 0.0482  
diversity: 0.1819  


Epoch 286
-------------------------------
loss: 0.013982 [ 2048/1237259]
loss: 0.013631 [206848/1237259]
loss: 0.013062 [411648/1237259]
loss: 0.013725 [616448/1237259]
loss: 0.014499 [821248/1237259]
loss: 0.013882 [1026048/1237259]
loss: 0.012460 [1230848/1237259]
Epoch 287
-------------------------------
loss: 0.013892 [ 2048/1237259]
loss: 0.013112 [206848/1237259]
loss: 0.012128 [411648/1237259]
loss: 0.014384 [616448/1237259]
loss: 0.011887 [821248/1237259]
loss: 0.014253 [1026048/1237259]
loss: 0.015076 [1230848/1237259]
Epoch 288
-------------------------------
loss: 0.012229 [ 2048/1237259]
loss: 0.012553 [206848/1237259]
loss: 0.012799 [411648/1237259]
loss: 0.012587 [616448/1237259]
loss: 0.013647 [821248/1237259]
loss: 0.013201 [1026048/1237259]
loss: 0.013319 [1230848/1237259]
Epoch 289
-------------------------------
loss: 0.013151 [ 2048/1237259]
loss: 0.013414 [206848/1237259]
loss: 0.013931 [411648/1237259]
loss: 0.011930 [616448/1237259]
loss: 0.012493 [821248/1237259]
loss: 0.013383 [1026048/1237259]
loss: 0.013702 [1230848/1237259]
Epoch 290
-------------------------------
loss: 0.011885 [ 2048/1237259]
loss: 0.012846 [206848/1237259]
loss: 0.012775 [411648/1237259]
loss: 0.014446 [616448/1237259]
loss: 0.013161 [821248/1237259]
loss: 0.013510 [1026048/1237259]
loss: 0.012808 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0595  
ndcg@20: 0.0481  
diversity: 0.1823  


Epoch 291
-------------------------------
loss: 0.012772 [ 2048/1237259]
loss: 0.012305 [206848/1237259]
loss: 0.013355 [411648/1237259]
loss: 0.013125 [616448/1237259]
loss: 0.012519 [821248/1237259]
loss: 0.013823 [1026048/1237259]
loss: 0.014555 [1230848/1237259]
Epoch 292
-------------------------------
loss: 0.011273 [ 2048/1237259]
loss: 0.013251 [206848/1237259]
loss: 0.012250 [411648/1237259]
loss: 0.012858 [616448/1237259]
loss: 0.013501 [821248/1237259]
loss: 0.012131 [1026048/1237259]
loss: 0.014038 [1230848/1237259]
Epoch 293
-------------------------------
loss: 0.013988 [ 2048/1237259]
loss: 0.013686 [206848/1237259]
loss: 0.011867 [411648/1237259]
loss: 0.012762 [616448/1237259]
loss: 0.012020 [821248/1237259]
loss: 0.012118 [1026048/1237259]
loss: 0.013983 [1230848/1237259]
Epoch 294
-------------------------------
loss: 0.013651 [ 2048/1237259]
loss: 0.011732 [206848/1237259]
loss: 0.013670 [411648/1237259]
loss: 0.012840 [616448/1237259]
loss: 0.014239 [821248/1237259]
loss: 0.013470 [1026048/1237259]
loss: 0.015772 [1230848/1237259]
Epoch 295
-------------------------------
loss: 0.011523 [ 2048/1237259]
loss: 0.013704 [206848/1237259]
loss: 0.013866 [411648/1237259]
loss: 0.013056 [616448/1237259]
loss: 0.013662 [821248/1237259]
loss: 0.011351 [1026048/1237259]
loss: 0.011958 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0601  
ndcg@20: 0.0485  
diversity: 0.1821  


Epoch 296
-------------------------------
loss: 0.015023 [ 2048/1237259]
loss: 0.013471 [206848/1237259]
loss: 0.013784 [411648/1237259]
loss: 0.012379 [616448/1237259]
loss: 0.014275 [821248/1237259]
loss: 0.012140 [1026048/1237259]
loss: 0.013006 [1230848/1237259]
Epoch 297
-------------------------------
loss: 0.012917 [ 2048/1237259]
loss: 0.014428 [206848/1237259]
loss: 0.014601 [411648/1237259]
loss: 0.012783 [616448/1237259]
loss: 0.012766 [821248/1237259]
loss: 0.014020 [1026048/1237259]
loss: 0.013281 [1230848/1237259]
Epoch 298
-------------------------------
loss: 0.012680 [ 2048/1237259]
loss: 0.014526 [206848/1237259]
loss: 0.011636 [411648/1237259]
loss: 0.012724 [616448/1237259]
loss: 0.014258 [821248/1237259]
loss: 0.013228 [1026048/1237259]
loss: 0.012463 [1230848/1237259]
Epoch 299
-------------------------------
loss: 0.012765 [ 2048/1237259]
loss: 0.011785 [206848/1237259]
loss: 0.012097 [411648/1237259]
loss: 0.013100 [616448/1237259]
loss: 0.011669 [821248/1237259]
loss: 0.012246 [1026048/1237259]
loss: 0.014364 [1230848/1237259]
Epoch 300
-------------------------------
loss: 0.012453 [ 2048/1237259]
loss: 0.015713 [206848/1237259]
loss: 0.013684 [411648/1237259]
loss: 0.011947 [616448/1237259]
loss: 0.013452 [821248/1237259]
loss: 0.013080 [1026048/1237259]
loss: 0.013836 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0601  
ndcg@20: 0.0485  
diversity: 0.1823  


Epoch 301
-------------------------------
loss: 0.014876 [ 2048/1237259]
loss: 0.012125 [206848/1237259]
loss: 0.013297 [411648/1237259]
loss: 0.014668 [616448/1237259]
loss: 0.012363 [821248/1237259]
loss: 0.012023 [1026048/1237259]
loss: 0.011990 [1230848/1237259]
Epoch 302
-------------------------------
loss: 0.014119 [ 2048/1237259]
loss: 0.013330 [206848/1237259]
loss: 0.012045 [411648/1237259]
loss: 0.013054 [616448/1237259]
loss: 0.014026 [821248/1237259]
loss: 0.014055 [1026048/1237259]
loss: 0.013257 [1230848/1237259]
Epoch 303
-------------------------------
loss: 0.016324 [ 2048/1237259]
loss: 0.012690 [206848/1237259]
loss: 0.013161 [411648/1237259]
loss: 0.014531 [616448/1237259]
loss: 0.014095 [821248/1237259]
loss: 0.013087 [1026048/1237259]
loss: 0.013325 [1230848/1237259]
Epoch 304
-------------------------------
loss: 0.011706 [ 2048/1237259]
loss: 0.013188 [206848/1237259]
loss: 0.013326 [411648/1237259]
loss: 0.012428 [616448/1237259]
loss: 0.013900 [821248/1237259]
loss: 0.012612 [1026048/1237259]
loss: 0.013745 [1230848/1237259]
Epoch 305
-------------------------------
loss: 0.014595 [ 2048/1237259]
loss: 0.012980 [206848/1237259]
loss: 0.011862 [411648/1237259]
loss: 0.011609 [616448/1237259]
loss: 0.014130 [821248/1237259]
loss: 0.014577 [1026048/1237259]
loss: 0.012947 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0601  
ndcg@20: 0.0484  
diversity: 0.1824  


Epoch 306
-------------------------------
loss: 0.013259 [ 2048/1237259]
loss: 0.014208 [206848/1237259]
loss: 0.014311 [411648/1237259]
loss: 0.012035 [616448/1237259]
loss: 0.013335 [821248/1237259]
loss: 0.012427 [1026048/1237259]
loss: 0.012652 [1230848/1237259]
Epoch 307
-------------------------------
loss: 0.014902 [ 2048/1237259]
loss: 0.013161 [206848/1237259]
loss: 0.015364 [411648/1237259]
loss: 0.014071 [616448/1237259]
loss: 0.013994 [821248/1237259]
loss: 0.015607 [1026048/1237259]
loss: 0.013916 [1230848/1237259]
Epoch 308
-------------------------------
loss: 0.013374 [ 2048/1237259]
loss: 0.013651 [206848/1237259]
loss: 0.013805 [411648/1237259]
loss: 0.013750 [616448/1237259]
loss: 0.012106 [821248/1237259]
loss: 0.013876 [1026048/1237259]
loss: 0.011835 [1230848/1237259]
Epoch 309
-------------------------------
loss: 0.013160 [ 2048/1237259]
loss: 0.012846 [206848/1237259]
loss: 0.016019 [411648/1237259]
loss: 0.013962 [616448/1237259]
loss: 0.016030 [821248/1237259]
loss: 0.013539 [1026048/1237259]
loss: 0.013660 [1230848/1237259]
Epoch 310
-------------------------------
loss: 0.012591 [ 2048/1237259]
loss: 0.015017 [206848/1237259]
loss: 0.013521 [411648/1237259]
loss: 0.011718 [616448/1237259]
loss: 0.013594 [821248/1237259]
loss: 0.012740 [1026048/1237259]
loss: 0.013399 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0599  
ndcg@20: 0.0483  
diversity: 0.1824  


Epoch 311
-------------------------------
loss: 0.012159 [ 2048/1237259]
loss: 0.012753 [206848/1237259]
loss: 0.012428 [411648/1237259]
loss: 0.012365 [616448/1237259]
loss: 0.012701 [821248/1237259]
loss: 0.013221 [1026048/1237259]
loss: 0.012882 [1230848/1237259]
Epoch 312
-------------------------------
loss: 0.012831 [ 2048/1237259]
loss: 0.013760 [206848/1237259]
loss: 0.011996 [411648/1237259]
loss: 0.012912 [616448/1237259]
loss: 0.013183 [821248/1237259]
loss: 0.012855 [1026048/1237259]
loss: 0.013191 [1230848/1237259]
Epoch 313
-------------------------------
loss: 0.012721 [ 2048/1237259]
loss: 0.012853 [206848/1237259]
loss: 0.012454 [411648/1237259]
loss: 0.013336 [616448/1237259]
loss: 0.012018 [821248/1237259]
loss: 0.013801 [1026048/1237259]
loss: 0.014069 [1230848/1237259]
Epoch 314
-------------------------------
loss: 0.013022 [ 2048/1237259]
loss: 0.013636 [206848/1237259]
loss: 0.013828 [411648/1237259]
loss: 0.013064 [616448/1237259]
loss: 0.012433 [821248/1237259]
loss: 0.012979 [1026048/1237259]
loss: 0.013417 [1230848/1237259]
Epoch 315
-------------------------------
loss: 0.013533 [ 2048/1237259]
loss: 0.013202 [206848/1237259]
loss: 0.014212 [411648/1237259]
loss: 0.012683 [616448/1237259]
loss: 0.013488 [821248/1237259]
loss: 0.012286 [1026048/1237259]
loss: 0.012908 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0601  
ndcg@20: 0.0484  
diversity: 0.1825  


Epoch 316
-------------------------------
loss: 0.012754 [ 2048/1237259]
loss: 0.013080 [206848/1237259]
loss: 0.013380 [411648/1237259]
loss: 0.011900 [616448/1237259]
loss: 0.013428 [821248/1237259]
loss: 0.014924 [1026048/1237259]
loss: 0.014647 [1230848/1237259]
Epoch 317
-------------------------------
loss: 0.012902 [ 2048/1237259]
loss: 0.013409 [206848/1237259]
loss: 0.013802 [411648/1237259]
loss: 0.012888 [616448/1237259]
loss: 0.015018 [821248/1237259]
loss: 0.012100 [1026048/1237259]
loss: 0.014168 [1230848/1237259]
Epoch 318
-------------------------------
loss: 0.011767 [ 2048/1237259]
loss: 0.016496 [206848/1237259]
loss: 0.015532 [411648/1237259]
loss: 0.011684 [616448/1237259]
loss: 0.012541 [821248/1237259]
loss: 0.011589 [1026048/1237259]
loss: 0.013173 [1230848/1237259]
Epoch 319
-------------------------------
loss: 0.012133 [ 2048/1237259]
loss: 0.012296 [206848/1237259]
loss: 0.013445 [411648/1237259]
loss: 0.013122 [616448/1237259]
loss: 0.013223 [821248/1237259]
loss: 0.013281 [1026048/1237259]
loss: 0.012205 [1230848/1237259]
Epoch 320
-------------------------------
loss: 0.012534 [ 2048/1237259]
loss: 0.014191 [206848/1237259]
loss: 0.012709 [411648/1237259]
loss: 0.015110 [616448/1237259]
loss: 0.011475 [821248/1237259]
loss: 0.012212 [1026048/1237259]
loss: 0.013251 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0603  
ndcg@20: 0.0487  
diversity: 0.1825  


Epoch 321
-------------------------------
loss: 0.013211 [ 2048/1237259]
loss: 0.012620 [206848/1237259]
loss: 0.013234 [411648/1237259]
loss: 0.012851 [616448/1237259]
loss: 0.014194 [821248/1237259]
loss: 0.012780 [1026048/1237259]
loss: 0.013844 [1230848/1237259]
Epoch 322
-------------------------------
loss: 0.013508 [ 2048/1237259]
loss: 0.012040 [206848/1237259]
loss: 0.013884 [411648/1237259]
loss: 0.012326 [616448/1237259]
loss: 0.014945 [821248/1237259]
loss: 0.013758 [1026048/1237259]
loss: 0.013770 [1230848/1237259]
Epoch 323
-------------------------------
loss: 0.014868 [ 2048/1237259]
loss: 0.012505 [206848/1237259]
loss: 0.012848 [411648/1237259]
loss: 0.013989 [616448/1237259]
loss: 0.012216 [821248/1237259]
loss: 0.012998 [1026048/1237259]
loss: 0.012429 [1230848/1237259]
Epoch 324
-------------------------------
loss: 0.012301 [ 2048/1237259]
loss: 0.013151 [206848/1237259]
loss: 0.013401 [411648/1237259]
loss: 0.013403 [616448/1237259]
loss: 0.012175 [821248/1237259]
loss: 0.014406 [1026048/1237259]
loss: 0.012916 [1230848/1237259]
Epoch 325
-------------------------------
loss: 0.012550 [ 2048/1237259]
loss: 0.012397 [206848/1237259]
loss: 0.013511 [411648/1237259]
loss: 0.013455 [616448/1237259]
loss: 0.012915 [821248/1237259]
loss: 0.013406 [1026048/1237259]
loss: 0.011587 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0604  
ndcg@20: 0.0486  
diversity: 0.1826  


Epoch 326
-------------------------------
loss: 0.012388 [ 2048/1237259]
loss: 0.011261 [206848/1237259]
loss: 0.014675 [411648/1237259]
loss: 0.012463 [616448/1237259]
loss: 0.013278 [821248/1237259]
loss: 0.014541 [1026048/1237259]
loss: 0.012833 [1230848/1237259]
Epoch 327
-------------------------------
loss: 0.014331 [ 2048/1237259]
loss: 0.012102 [206848/1237259]
loss: 0.012393 [411648/1237259]
loss: 0.013374 [616448/1237259]
loss: 0.013649 [821248/1237259]
loss: 0.013155 [1026048/1237259]
loss: 0.012520 [1230848/1237259]
Epoch 328
-------------------------------
loss: 0.013653 [ 2048/1237259]
loss: 0.013857 [206848/1237259]
loss: 0.012899 [411648/1237259]
loss: 0.011896 [616448/1237259]
loss: 0.012279 [821248/1237259]
loss: 0.013217 [1026048/1237259]
loss: 0.013212 [1230848/1237259]
Epoch 329
-------------------------------
loss: 0.013740 [ 2048/1237259]
loss: 0.011804 [206848/1237259]
loss: 0.013389 [411648/1237259]
loss: 0.013505 [616448/1237259]
loss: 0.012270 [821248/1237259]
loss: 0.013251 [1026048/1237259]
loss: 0.011997 [1230848/1237259]
Epoch 330
-------------------------------
loss: 0.014050 [ 2048/1237259]
loss: 0.013841 [206848/1237259]
loss: 0.011750 [411648/1237259]
loss: 0.013572 [616448/1237259]
loss: 0.014886 [821248/1237259]
loss: 0.011943 [1026048/1237259]
loss: 0.011293 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0604  
ndcg@20: 0.0488  
diversity: 0.1828  


Epoch 331
-------------------------------
loss: 0.014992 [ 2048/1237259]
loss: 0.012509 [206848/1237259]
loss: 0.013191 [411648/1237259]
loss: 0.012180 [616448/1237259]
loss: 0.013302 [821248/1237259]
loss: 0.015816 [1026048/1237259]
loss: 0.013082 [1230848/1237259]
Epoch 332
-------------------------------
loss: 0.011786 [ 2048/1237259]
loss: 0.012504 [206848/1237259]
loss: 0.013284 [411648/1237259]
loss: 0.012787 [616448/1237259]
loss: 0.012950 [821248/1237259]
loss: 0.013605 [1026048/1237259]
loss: 0.014645 [1230848/1237259]
Epoch 333
-------------------------------
loss: 0.013672 [ 2048/1237259]
loss: 0.014051 [206848/1237259]
loss: 0.016067 [411648/1237259]
loss: 0.012052 [616448/1237259]
loss: 0.013155 [821248/1237259]
loss: 0.013273 [1026048/1237259]
loss: 0.013311 [1230848/1237259]
Epoch 334
-------------------------------
loss: 0.013060 [ 2048/1237259]
loss: 0.013808 [206848/1237259]
loss: 0.012957 [411648/1237259]
loss: 0.013171 [616448/1237259]
loss: 0.013554 [821248/1237259]
loss: 0.013621 [1026048/1237259]
loss: 0.011974 [1230848/1237259]
Epoch 335
-------------------------------
loss: 0.013027 [ 2048/1237259]
loss: 0.012180 [206848/1237259]
loss: 0.012897 [411648/1237259]
loss: 0.012304 [616448/1237259]
loss: 0.012022 [821248/1237259]
loss: 0.012434 [1026048/1237259]
loss: 0.012964 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0606  
ndcg@20: 0.0490  
diversity: 0.1827  


Epoch 336
-------------------------------
loss: 0.012938 [ 2048/1237259]
loss: 0.013189 [206848/1237259]
loss: 0.014226 [411648/1237259]
loss: 0.012426 [616448/1237259]
loss: 0.014100 [821248/1237259]
loss: 0.014631 [1026048/1237259]
loss: 0.013471 [1230848/1237259]
Epoch 337
-------------------------------
loss: 0.013035 [ 2048/1237259]
loss: 0.014119 [206848/1237259]
loss: 0.012550 [411648/1237259]
loss: 0.011817 [616448/1237259]
loss: 0.012645 [821248/1237259]
loss: 0.012668 [1026048/1237259]
loss: 0.014722 [1230848/1237259]
Epoch 338
-------------------------------
loss: 0.013991 [ 2048/1237259]
loss: 0.015232 [206848/1237259]
loss: 0.013277 [411648/1237259]
loss: 0.014472 [616448/1237259]
loss: 0.014320 [821248/1237259]
loss: 0.016124 [1026048/1237259]
loss: 0.013028 [1230848/1237259]
Epoch 339
-------------------------------
loss: 0.011814 [ 2048/1237259]
loss: 0.013503 [206848/1237259]
loss: 0.011625 [411648/1237259]
loss: 0.013454 [616448/1237259]
loss: 0.014125 [821248/1237259]
loss: 0.011579 [1026048/1237259]
loss: 0.012067 [1230848/1237259]
Epoch 340
-------------------------------
loss: 0.010593 [ 2048/1237259]
loss: 0.013650 [206848/1237259]
loss: 0.013455 [411648/1237259]
loss: 0.013511 [616448/1237259]
loss: 0.011695 [821248/1237259]
loss: 0.011555 [1026048/1237259]
loss: 0.012510 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0609  
ndcg@20: 0.0491  
diversity: 0.1828  


Epoch 341
-------------------------------
loss: 0.013498 [ 2048/1237259]
loss: 0.013471 [206848/1237259]
loss: 0.013332 [411648/1237259]
loss: 0.013529 [616448/1237259]
loss: 0.013284 [821248/1237259]
loss: 0.014305 [1026048/1237259]
loss: 0.012787 [1230848/1237259]
Epoch 342
-------------------------------
loss: 0.012149 [ 2048/1237259]
loss: 0.012248 [206848/1237259]
loss: 0.012855 [411648/1237259]
loss: 0.012714 [616448/1237259]
loss: 0.013063 [821248/1237259]
loss: 0.012470 [1026048/1237259]
loss: 0.012940 [1230848/1237259]
Epoch 343
-------------------------------
loss: 0.013448 [ 2048/1237259]
loss: 0.013072 [206848/1237259]
loss: 0.011878 [411648/1237259]
loss: 0.013515 [616448/1237259]
loss: 0.014371 [821248/1237259]
loss: 0.012440 [1026048/1237259]
loss: 0.014060 [1230848/1237259]
Epoch 344
-------------------------------
loss: 0.014659 [ 2048/1237259]
loss: 0.013444 [206848/1237259]
loss: 0.014014 [411648/1237259]
loss: 0.014701 [616448/1237259]
loss: 0.011638 [821248/1237259]
loss: 0.012096 [1026048/1237259]
loss: 0.013091 [1230848/1237259]
Epoch 345
-------------------------------
loss: 0.012314 [ 2048/1237259]
loss: 0.013358 [206848/1237259]
loss: 0.013366 [411648/1237259]
loss: 0.013672 [616448/1237259]
loss: 0.012380 [821248/1237259]
loss: 0.013700 [1026048/1237259]
loss: 0.013773 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0492  
diversity: 0.1827  


Epoch 346
-------------------------------
loss: 0.012245 [ 2048/1237259]
loss: 0.014427 [206848/1237259]
loss: 0.012720 [411648/1237259]
loss: 0.014474 [616448/1237259]
loss: 0.015103 [821248/1237259]
loss: 0.012677 [1026048/1237259]
loss: 0.014316 [1230848/1237259]
Epoch 347
-------------------------------
loss: 0.012369 [ 2048/1237259]
loss: 0.013184 [206848/1237259]
loss: 0.012983 [411648/1237259]
loss: 0.013197 [616448/1237259]
loss: 0.012436 [821248/1237259]
loss: 0.012878 [1026048/1237259]
loss: 0.012665 [1230848/1237259]
Epoch 348
-------------------------------
loss: 0.012857 [ 2048/1237259]
loss: 0.012366 [206848/1237259]
loss: 0.012276 [411648/1237259]
loss: 0.014106 [616448/1237259]
loss: 0.012918 [821248/1237259]
loss: 0.013492 [1026048/1237259]
loss: 0.012402 [1230848/1237259]
Epoch 349
-------------------------------
loss: 0.013897 [ 2048/1237259]
loss: 0.013716 [206848/1237259]
loss: 0.016299 [411648/1237259]
loss: 0.014204 [616448/1237259]
loss: 0.014305 [821248/1237259]
loss: 0.013020 [1026048/1237259]
loss: 0.013396 [1230848/1237259]
Epoch 350
-------------------------------
loss: 0.012513 [ 2048/1237259]
loss: 0.014191 [206848/1237259]
loss: 0.014987 [411648/1237259]
loss: 0.013428 [616448/1237259]
loss: 0.012096 [821248/1237259]
loss: 0.012503 [1026048/1237259]
loss: 0.013667 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0609  
ndcg@20: 0.0492  
diversity: 0.1830  


Epoch 351
-------------------------------
loss: 0.014863 [ 2048/1237259]
loss: 0.012250 [206848/1237259]
loss: 0.012471 [411648/1237259]
loss: 0.013087 [616448/1237259]
loss: 0.012157 [821248/1237259]
loss: 0.013365 [1026048/1237259]
loss: 0.012931 [1230848/1237259]
Epoch 352
-------------------------------
loss: 0.012939 [ 2048/1237259]
loss: 0.012740 [206848/1237259]
loss: 0.014004 [411648/1237259]
loss: 0.013471 [616448/1237259]
loss: 0.014454 [821248/1237259]
loss: 0.015328 [1026048/1237259]
loss: 0.011877 [1230848/1237259]
Epoch 353
-------------------------------
loss: 0.012385 [ 2048/1237259]
loss: 0.013844 [206848/1237259]
loss: 0.012862 [411648/1237259]
loss: 0.012301 [616448/1237259]
loss: 0.013496 [821248/1237259]
loss: 0.013443 [1026048/1237259]
loss: 0.013213 [1230848/1237259]
Epoch 354
-------------------------------
loss: 0.013181 [ 2048/1237259]
loss: 0.013082 [206848/1237259]
loss: 0.012500 [411648/1237259]
loss: 0.012640 [616448/1237259]
loss: 0.011339 [821248/1237259]
loss: 0.012693 [1026048/1237259]
loss: 0.013322 [1230848/1237259]
Epoch 355
-------------------------------
loss: 0.012304 [ 2048/1237259]
loss: 0.014281 [206848/1237259]
loss: 0.014438 [411648/1237259]
loss: 0.011336 [616448/1237259]
loss: 0.012744 [821248/1237259]
loss: 0.012520 [1026048/1237259]
loss: 0.012589 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0609  
ndcg@20: 0.0492  
diversity: 0.1829  


Epoch 356
-------------------------------
loss: 0.013174 [ 2048/1237259]
loss: 0.012884 [206848/1237259]
loss: 0.011430 [411648/1237259]
loss: 0.015065 [616448/1237259]
loss: 0.011427 [821248/1237259]
loss: 0.013444 [1026048/1237259]
loss: 0.013066 [1230848/1237259]
Epoch 357
-------------------------------
loss: 0.013995 [ 2048/1237259]
loss: 0.012604 [206848/1237259]
loss: 0.011343 [411648/1237259]
loss: 0.014161 [616448/1237259]
loss: 0.013711 [821248/1237259]
loss: 0.014805 [1026048/1237259]
loss: 0.013791 [1230848/1237259]
Epoch 358
-------------------------------
loss: 0.014632 [ 2048/1237259]
loss: 0.012913 [206848/1237259]
loss: 0.014961 [411648/1237259]
loss: 0.014447 [616448/1237259]
loss: 0.012343 [821248/1237259]
loss: 0.014526 [1026048/1237259]
loss: 0.012726 [1230848/1237259]
Epoch 359
-------------------------------
loss: 0.014380 [ 2048/1237259]
loss: 0.011899 [206848/1237259]
loss: 0.012427 [411648/1237259]
loss: 0.012376 [616448/1237259]
loss: 0.013083 [821248/1237259]
loss: 0.013087 [1026048/1237259]
loss: 0.013249 [1230848/1237259]
Epoch 360
-------------------------------
loss: 0.012282 [ 2048/1237259]
loss: 0.014739 [206848/1237259]
loss: 0.012783 [411648/1237259]
loss: 0.011073 [616448/1237259]
loss: 0.012879 [821248/1237259]
loss: 0.013338 [1026048/1237259]
loss: 0.013156 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0494  
diversity: 0.1830  


Epoch 361
-------------------------------
loss: 0.015060 [ 2048/1237259]
loss: 0.011923 [206848/1237259]
loss: 0.014258 [411648/1237259]
loss: 0.013846 [616448/1237259]
loss: 0.013621 [821248/1237259]
loss: 0.010868 [1026048/1237259]
loss: 0.015595 [1230848/1237259]
Epoch 362
-------------------------------
loss: 0.013247 [ 2048/1237259]
loss: 0.013232 [206848/1237259]
loss: 0.012063 [411648/1237259]
loss: 0.010755 [616448/1237259]
loss: 0.014268 [821248/1237259]
loss: 0.013425 [1026048/1237259]
loss: 0.011977 [1230848/1237259]
Epoch 363
-------------------------------
loss: 0.012902 [ 2048/1237259]
loss: 0.012584 [206848/1237259]
loss: 0.013519 [411648/1237259]
loss: 0.011862 [616448/1237259]
loss: 0.012341 [821248/1237259]
loss: 0.012102 [1026048/1237259]
loss: 0.013319 [1230848/1237259]
Epoch 364
-------------------------------
loss: 0.013432 [ 2048/1237259]
loss: 0.013602 [206848/1237259]
loss: 0.012196 [411648/1237259]
loss: 0.012282 [616448/1237259]
loss: 0.013612 [821248/1237259]
loss: 0.012885 [1026048/1237259]
loss: 0.013646 [1230848/1237259]
Epoch 365
-------------------------------
loss: 0.015777 [ 2048/1237259]
loss: 0.012575 [206848/1237259]
loss: 0.014996 [411648/1237259]
loss: 0.013204 [616448/1237259]
loss: 0.012097 [821248/1237259]
loss: 0.012817 [1026048/1237259]
loss: 0.012477 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0495  
diversity: 0.1831  


Epoch 366
-------------------------------
loss: 0.012544 [ 2048/1237259]
loss: 0.013785 [206848/1237259]
loss: 0.013774 [411648/1237259]
loss: 0.013838 [616448/1237259]
loss: 0.012937 [821248/1237259]
loss: 0.010795 [1026048/1237259]
loss: 0.013950 [1230848/1237259]
Epoch 367
-------------------------------
loss: 0.011810 [ 2048/1237259]
loss: 0.012826 [206848/1237259]
loss: 0.012191 [411648/1237259]
loss: 0.012848 [616448/1237259]
loss: 0.012982 [821248/1237259]
loss: 0.013553 [1026048/1237259]
loss: 0.013080 [1230848/1237259]
Epoch 368
-------------------------------
loss: 0.015998 [ 2048/1237259]
loss: 0.013051 [206848/1237259]
loss: 0.014049 [411648/1237259]
loss: 0.013137 [616448/1237259]
loss: 0.013953 [821248/1237259]
loss: 0.013642 [1026048/1237259]
loss: 0.011142 [1230848/1237259]
Epoch 369
-------------------------------
loss: 0.012670 [ 2048/1237259]
loss: 0.012704 [206848/1237259]
loss: 0.012403 [411648/1237259]
loss: 0.013927 [616448/1237259]
loss: 0.014379 [821248/1237259]
loss: 0.011914 [1026048/1237259]
loss: 0.013309 [1230848/1237259]
Epoch 370
-------------------------------
loss: 0.010999 [ 2048/1237259]
loss: 0.012078 [206848/1237259]
loss: 0.012028 [411648/1237259]
loss: 0.013668 [616448/1237259]
loss: 0.014564 [821248/1237259]
loss: 0.014357 [1026048/1237259]
loss: 0.013052 [1230848/1237259]
Eval results: 
recall@20: 0.0611  
ndcg@20: 0.0494  
diversity: 0.1830  


Epoch 371
-------------------------------
loss: 0.012145 [ 2048/1237259]
loss: 0.013615 [206848/1237259]
loss: 0.012700 [411648/1237259]
loss: 0.011505 [616448/1237259]
loss: 0.013042 [821248/1237259]
loss: 0.014968 [1026048/1237259]
loss: 0.013420 [1230848/1237259]
Epoch 372
-------------------------------
loss: 0.011809 [ 2048/1237259]
loss: 0.012770 [206848/1237259]
loss: 0.013503 [411648/1237259]
loss: 0.012946 [616448/1237259]
loss: 0.013119 [821248/1237259]
loss: 0.013520 [1026048/1237259]
loss: 0.014611 [1230848/1237259]
Epoch 373
-------------------------------
loss: 0.013505 [ 2048/1237259]
loss: 0.014254 [206848/1237259]
loss: 0.012600 [411648/1237259]
loss: 0.013751 [616448/1237259]
loss: 0.010943 [821248/1237259]
loss: 0.013186 [1026048/1237259]
loss: 0.014222 [1230848/1237259]
Epoch 374
-------------------------------
loss: 0.013270 [ 2048/1237259]
loss: 0.012668 [206848/1237259]
loss: 0.012913 [411648/1237259]
loss: 0.015223 [616448/1237259]
loss: 0.013013 [821248/1237259]
loss: 0.013116 [1026048/1237259]
loss: 0.014310 [1230848/1237259]
Epoch 375
-------------------------------
loss: 0.012004 [ 2048/1237259]
loss: 0.011635 [206848/1237259]
loss: 0.014190 [411648/1237259]
loss: 0.012160 [616448/1237259]
loss: 0.011158 [821248/1237259]
loss: 0.013012 [1026048/1237259]
loss: 0.013191 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0496  
diversity: 0.1830  


Epoch 376
-------------------------------
loss: 0.012912 [ 2048/1237259]
loss: 0.012596 [206848/1237259]
loss: 0.013163 [411648/1237259]
loss: 0.011970 [616448/1237259]
loss: 0.012491 [821248/1237259]
loss: 0.012838 [1026048/1237259]
loss: 0.013370 [1230848/1237259]
Epoch 377
-------------------------------
loss: 0.013475 [ 2048/1237259]
loss: 0.012347 [206848/1237259]
loss: 0.012940 [411648/1237259]
loss: 0.012458 [616448/1237259]
loss: 0.013913 [821248/1237259]
loss: 0.013859 [1026048/1237259]
loss: 0.013742 [1230848/1237259]
Epoch 378
-------------------------------
loss: 0.012002 [ 2048/1237259]
loss: 0.012373 [206848/1237259]
loss: 0.013256 [411648/1237259]
loss: 0.013047 [616448/1237259]
loss: 0.012883 [821248/1237259]
loss: 0.012672 [1026048/1237259]
loss: 0.012186 [1230848/1237259]
Epoch 379
-------------------------------
loss: 0.013087 [ 2048/1237259]
loss: 0.012810 [206848/1237259]
loss: 0.012193 [411648/1237259]
loss: 0.013416 [616448/1237259]
loss: 0.014066 [821248/1237259]
loss: 0.012838 [1026048/1237259]
loss: 0.011486 [1230848/1237259]
Epoch 380
-------------------------------
loss: 0.012183 [ 2048/1237259]
loss: 0.013155 [206848/1237259]
loss: 0.014381 [411648/1237259]
loss: 0.011437 [616448/1237259]
loss: 0.013423 [821248/1237259]
loss: 0.013241 [1026048/1237259]
loss: 0.014172 [1230848/1237259]
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0494  
diversity: 0.1830  


Epoch 381
-------------------------------
loss: 0.012157 [ 2048/1237259]
loss: 0.012117 [206848/1237259]
loss: 0.012270 [411648/1237259]
loss: 0.011928 [616448/1237259]
loss: 0.011911 [821248/1237259]
loss: 0.012081 [1026048/1237259]
loss: 0.013113 [1230848/1237259]
Epoch 382
-------------------------------
loss: 0.012574 [ 2048/1237259]
loss: 0.011897 [206848/1237259]
loss: 0.012943 [411648/1237259]
loss: 0.013124 [616448/1237259]
loss: 0.013270 [821248/1237259]
loss: 0.011435 [1026048/1237259]
loss: 0.014688 [1230848/1237259]
Epoch 383
-------------------------------
loss: 0.013340 [ 2048/1237259]
loss: 0.012971 [206848/1237259]
loss: 0.013044 [411648/1237259]
loss: 0.011652 [616448/1237259]
loss: 0.013559 [821248/1237259]
loss: 0.013863 [1026048/1237259]
loss: 0.012044 [1230848/1237259]
Epoch 384
-------------------------------
loss: 0.014125 [ 2048/1237259]
loss: 0.013537 [206848/1237259]
loss: 0.011332 [411648/1237259]
loss: 0.012921 [616448/1237259]
loss: 0.013749 [821248/1237259]
loss: 0.014468 [1026048/1237259]
loss: 0.012291 [1230848/1237259]
Epoch 385
-------------------------------
loss: 0.013079 [ 2048/1237259]
loss: 0.013770 [206848/1237259]
loss: 0.013516 [411648/1237259]
loss: 0.011807 [616448/1237259]
loss: 0.015074 [821248/1237259]
loss: 0.013377 [1026048/1237259]
loss: 0.012940 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0609  
ndcg@20: 0.0494  
diversity: 0.1831  


Epoch 386
-------------------------------
loss: 0.013478 [ 2048/1237259]
loss: 0.011934 [206848/1237259]
loss: 0.013071 [411648/1237259]
loss: 0.013647 [616448/1237259]
loss: 0.013762 [821248/1237259]
loss: 0.012775 [1026048/1237259]
loss: 0.012181 [1230848/1237259]
Epoch 387
-------------------------------
loss: 0.012643 [ 2048/1237259]
loss: 0.014763 [206848/1237259]
loss: 0.013492 [411648/1237259]
loss: 0.013155 [616448/1237259]
loss: 0.012589 [821248/1237259]
loss: 0.013524 [1026048/1237259]
loss: 0.013067 [1230848/1237259]
Epoch 388
-------------------------------
loss: 0.010939 [ 2048/1237259]
loss: 0.010865 [206848/1237259]
loss: 0.013408 [411648/1237259]
loss: 0.011509 [616448/1237259]
loss: 0.014330 [821248/1237259]
loss: 0.012540 [1026048/1237259]
loss: 0.013862 [1230848/1237259]
Epoch 389
-------------------------------
loss: 0.011272 [ 2048/1237259]
loss: 0.013033 [206848/1237259]
loss: 0.013465 [411648/1237259]
loss: 0.014530 [616448/1237259]
loss: 0.012451 [821248/1237259]
loss: 0.012969 [1026048/1237259]
loss: 0.012264 [1230848/1237259]
Epoch 390
-------------------------------
loss: 0.011524 [ 2048/1237259]
loss: 0.012546 [206848/1237259]
loss: 0.014493 [411648/1237259]
loss: 0.013517 [616448/1237259]
loss: 0.013910 [821248/1237259]
loss: 0.011707 [1026048/1237259]
loss: 0.012035 [1230848/1237259]
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0495  
diversity: 0.1830  


Epoch 391
-------------------------------
loss: 0.012632 [ 2048/1237259]
loss: 0.013445 [206848/1237259]
loss: 0.011625 [411648/1237259]
loss: 0.014165 [616448/1237259]
loss: 0.013790 [821248/1237259]
loss: 0.013238 [1026048/1237259]
loss: 0.013106 [1230848/1237259]
Epoch 392
-------------------------------
loss: 0.013495 [ 2048/1237259]
loss: 0.012483 [206848/1237259]
loss: 0.012349 [411648/1237259]
loss: 0.013129 [616448/1237259]
loss: 0.013781 [821248/1237259]
loss: 0.011871 [1026048/1237259]
loss: 0.014888 [1230848/1237259]
Epoch 393
-------------------------------
loss: 0.013975 [ 2048/1237259]
loss: 0.013244 [206848/1237259]
loss: 0.012150 [411648/1237259]
loss: 0.012967 [616448/1237259]
loss: 0.012450 [821248/1237259]
loss: 0.013376 [1026048/1237259]
loss: 0.013263 [1230848/1237259]
Epoch 394
-------------------------------
loss: 0.013122 [ 2048/1237259]
loss: 0.012956 [206848/1237259]
loss: 0.013308 [411648/1237259]
loss: 0.015735 [616448/1237259]
loss: 0.012419 [821248/1237259]
loss: 0.012899 [1026048/1237259]
loss: 0.013467 [1230848/1237259]
Epoch 395
-------------------------------
loss: 0.012677 [ 2048/1237259]
loss: 0.013089 [206848/1237259]
loss: 0.012941 [411648/1237259]
loss: 0.013238 [616448/1237259]
loss: 0.013029 [821248/1237259]
loss: 0.011747 [1026048/1237259]
loss: 0.013292 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0496  
diversity: 0.1832  


Epoch 396
-------------------------------
loss: 0.012237 [ 2048/1237259]
loss: 0.013025 [206848/1237259]
loss: 0.012978 [411648/1237259]
loss: 0.013907 [616448/1237259]
loss: 0.013993 [821248/1237259]
loss: 0.012757 [1026048/1237259]
loss: 0.013461 [1230848/1237259]
Epoch 397
-------------------------------
loss: 0.011328 [ 2048/1237259]
loss: 0.013905 [206848/1237259]
loss: 0.011770 [411648/1237259]
loss: 0.014118 [616448/1237259]
loss: 0.013897 [821248/1237259]
loss: 0.013547 [1026048/1237259]
loss: 0.012039 [1230848/1237259]
Epoch 398
-------------------------------
loss: 0.012875 [ 2048/1237259]
loss: 0.013959 [206848/1237259]
loss: 0.012840 [411648/1237259]
loss: 0.011186 [616448/1237259]
loss: 0.012829 [821248/1237259]
loss: 0.013088 [1026048/1237259]
loss: 0.011883 [1230848/1237259]
Epoch 399
-------------------------------
loss: 0.011558 [ 2048/1237259]
loss: 0.013984 [206848/1237259]
loss: 0.012064 [411648/1237259]
loss: 0.013160 [616448/1237259]
loss: 0.011952 [821248/1237259]
loss: 0.012881 [1026048/1237259]
loss: 0.013474 [1230848/1237259]
Epoch 400
-------------------------------
loss: 0.012519 [ 2048/1237259]
loss: 0.013253 [206848/1237259]
loss: 0.012751 [411648/1237259]
loss: 0.013176 [616448/1237259]
loss: 0.010860 [821248/1237259]
loss: 0.013259 [1026048/1237259]
loss: 0.012572 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0494  
diversity: 0.1833  


Epoch 401
-------------------------------
loss: 0.011433 [ 2048/1237259]
loss: 0.012459 [206848/1237259]
loss: 0.011718 [411648/1237259]
loss: 0.013418 [616448/1237259]
loss: 0.012686 [821248/1237259]
loss: 0.011723 [1026048/1237259]
loss: 0.013282 [1230848/1237259]
Epoch 402
-------------------------------
loss: 0.011560 [ 2048/1237259]
loss: 0.012388 [206848/1237259]
loss: 0.012405 [411648/1237259]
loss: 0.013261 [616448/1237259]
loss: 0.012350 [821248/1237259]
loss: 0.013052 [1026048/1237259]
loss: 0.011873 [1230848/1237259]
Epoch 403
-------------------------------
loss: 0.013054 [ 2048/1237259]
loss: 0.011823 [206848/1237259]
loss: 0.012186 [411648/1237259]
loss: 0.013098 [616448/1237259]
loss: 0.012988 [821248/1237259]
loss: 0.012427 [1026048/1237259]
loss: 0.012281 [1230848/1237259]
Epoch 404
-------------------------------
loss: 0.014585 [ 2048/1237259]
loss: 0.013686 [206848/1237259]
loss: 0.012984 [411648/1237259]
loss: 0.012105 [616448/1237259]
loss: 0.013942 [821248/1237259]
loss: 0.012873 [1026048/1237259]
loss: 0.010840 [1230848/1237259]
Epoch 405
-------------------------------
loss: 0.013249 [ 2048/1237259]
loss: 0.012435 [206848/1237259]
loss: 0.012828 [411648/1237259]
loss: 0.011097 [616448/1237259]
loss: 0.012700 [821248/1237259]
loss: 0.012450 [1026048/1237259]
loss: 0.013241 [1230848/1237259]
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0496  
diversity: 0.1833  


Epoch 406
-------------------------------
loss: 0.014226 [ 2048/1237259]
loss: 0.012236 [206848/1237259]
loss: 0.013948 [411648/1237259]
loss: 0.014153 [616448/1237259]
loss: 0.012442 [821248/1237259]
loss: 0.012296 [1026048/1237259]
loss: 0.013159 [1230848/1237259]
Epoch 407
-------------------------------
loss: 0.013854 [ 2048/1237259]
loss: 0.012884 [206848/1237259]
loss: 0.012682 [411648/1237259]
loss: 0.014090 [616448/1237259]
loss: 0.014251 [821248/1237259]
loss: 0.012922 [1026048/1237259]
loss: 0.011640 [1230848/1237259]
Epoch 408
-------------------------------
loss: 0.011675 [ 2048/1237259]
loss: 0.012658 [206848/1237259]
loss: 0.013683 [411648/1237259]
loss: 0.013748 [616448/1237259]
loss: 0.012629 [821248/1237259]
loss: 0.014216 [1026048/1237259]
loss: 0.011450 [1230848/1237259]
Epoch 409
-------------------------------
loss: 0.013228 [ 2048/1237259]
loss: 0.014068 [206848/1237259]
loss: 0.011284 [411648/1237259]
loss: 0.012583 [616448/1237259]
loss: 0.011557 [821248/1237259]
loss: 0.016588 [1026048/1237259]
loss: 0.012498 [1230848/1237259]
Epoch 410
-------------------------------
loss: 0.013446 [ 2048/1237259]
loss: 0.012785 [206848/1237259]
loss: 0.013365 [411648/1237259]
loss: 0.014174 [616448/1237259]
loss: 0.013299 [821248/1237259]
loss: 0.014371 [1026048/1237259]
loss: 0.012297 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0496  
diversity: 0.1834  


Epoch 411
-------------------------------
loss: 0.012024 [ 2048/1237259]
loss: 0.012081 [206848/1237259]
loss: 0.013738 [411648/1237259]
loss: 0.011708 [616448/1237259]
loss: 0.015098 [821248/1237259]
loss: 0.013383 [1026048/1237259]
loss: 0.011025 [1230848/1237259]
Epoch 412
-------------------------------
loss: 0.012479 [ 2048/1237259]
loss: 0.013660 [206848/1237259]
loss: 0.011880 [411648/1237259]
loss: 0.013243 [616448/1237259]
loss: 0.013502 [821248/1237259]
loss: 0.012272 [1026048/1237259]
loss: 0.011764 [1230848/1237259]
Epoch 413
-------------------------------
loss: 0.012176 [ 2048/1237259]
loss: 0.014160 [206848/1237259]
loss: 0.013050 [411648/1237259]
loss: 0.013176 [616448/1237259]
loss: 0.012347 [821248/1237259]
loss: 0.013565 [1026048/1237259]
loss: 0.010616 [1230848/1237259]
Epoch 414
-------------------------------
loss: 0.013471 [ 2048/1237259]
loss: 0.012645 [206848/1237259]
loss: 0.012983 [411648/1237259]
loss: 0.012668 [616448/1237259]
loss: 0.011723 [821248/1237259]
loss: 0.014853 [1026048/1237259]
loss: 0.013381 [1230848/1237259]
Epoch 415
-------------------------------
loss: 0.011690 [ 2048/1237259]
loss: 0.012617 [206848/1237259]
loss: 0.011516 [411648/1237259]
loss: 0.013319 [616448/1237259]
loss: 0.013439 [821248/1237259]
loss: 0.012107 [1026048/1237259]
loss: 0.013354 [1230848/1237259]
Eval results: 
recall@20: 0.0612  
ndcg@20: 0.0496  
diversity: 0.1833  


Epoch 416
-------------------------------
loss: 0.012201 [ 2048/1237259]
loss: 0.013235 [206848/1237259]
loss: 0.011074 [411648/1237259]
loss: 0.011408 [616448/1237259]
loss: 0.012624 [821248/1237259]
loss: 0.014231 [1026048/1237259]
loss: 0.012081 [1230848/1237259]
Epoch 417
-------------------------------
loss: 0.013546 [ 2048/1237259]
loss: 0.012868 [206848/1237259]
loss: 0.013454 [411648/1237259]
loss: 0.013449 [616448/1237259]
loss: 0.013625 [821248/1237259]
loss: 0.013926 [1026048/1237259]
loss: 0.013189 [1230848/1237259]
Epoch 418
-------------------------------
loss: 0.012720 [ 2048/1237259]
loss: 0.012242 [206848/1237259]
loss: 0.013057 [411648/1237259]
loss: 0.012317 [616448/1237259]
loss: 0.013564 [821248/1237259]
loss: 0.012436 [1026048/1237259]
loss: 0.012181 [1230848/1237259]
Epoch 419
-------------------------------
loss: 0.013229 [ 2048/1237259]
loss: 0.012386 [206848/1237259]
loss: 0.013898 [411648/1237259]
loss: 0.011690 [616448/1237259]
loss: 0.011650 [821248/1237259]
loss: 0.013402 [1026048/1237259]
loss: 0.011648 [1230848/1237259]
Epoch 420
-------------------------------
loss: 0.014484 [ 2048/1237259]
loss: 0.013256 [206848/1237259]
loss: 0.011698 [411648/1237259]
loss: 0.013555 [616448/1237259]
loss: 0.012588 [821248/1237259]
loss: 0.012557 [1026048/1237259]
loss: 0.012201 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0616  
ndcg@20: 0.0498  
diversity: 0.1832  


Epoch 421
-------------------------------
loss: 0.013570 [ 2048/1237259]
loss: 0.013756 [206848/1237259]
loss: 0.012574 [411648/1237259]
loss: 0.012552 [616448/1237259]
loss: 0.011862 [821248/1237259]
loss: 0.012678 [1026048/1237259]
loss: 0.014306 [1230848/1237259]
Epoch 422
-------------------------------
loss: 0.013017 [ 2048/1237259]
loss: 0.013901 [206848/1237259]
loss: 0.011699 [411648/1237259]
loss: 0.011224 [616448/1237259]
loss: 0.012214 [821248/1237259]
loss: 0.011977 [1026048/1237259]
loss: 0.013207 [1230848/1237259]
Epoch 423
-------------------------------
loss: 0.011729 [ 2048/1237259]
loss: 0.011643 [206848/1237259]
loss: 0.012083 [411648/1237259]
loss: 0.012218 [616448/1237259]
loss: 0.013728 [821248/1237259]
loss: 0.011980 [1026048/1237259]
loss: 0.011842 [1230848/1237259]
Epoch 424
-------------------------------
loss: 0.012403 [ 2048/1237259]
loss: 0.012506 [206848/1237259]
loss: 0.014678 [411648/1237259]
loss: 0.013111 [616448/1237259]
loss: 0.011755 [821248/1237259]
loss: 0.013510 [1026048/1237259]
loss: 0.012956 [1230848/1237259]
Epoch 425
-------------------------------
loss: 0.012395 [ 2048/1237259]
loss: 0.014088 [206848/1237259]
loss: 0.011757 [411648/1237259]
loss: 0.012945 [616448/1237259]
loss: 0.013404 [821248/1237259]
loss: 0.014382 [1026048/1237259]
loss: 0.014297 [1230848/1237259]
Eval results: 
recall@20: 0.0615  
ndcg@20: 0.0497  
diversity: 0.1833  


Epoch 426
-------------------------------
loss: 0.013465 [ 2048/1237259]
loss: 0.011536 [206848/1237259]
loss: 0.013474 [411648/1237259]
loss: 0.011417 [616448/1237259]
loss: 0.013840 [821248/1237259]
loss: 0.012385 [1026048/1237259]
loss: 0.013824 [1230848/1237259]
Epoch 427
-------------------------------
loss: 0.012886 [ 2048/1237259]
loss: 0.012128 [206848/1237259]
loss: 0.012977 [411648/1237259]
loss: 0.012658 [616448/1237259]
loss: 0.012344 [821248/1237259]
loss: 0.012605 [1026048/1237259]
loss: 0.012824 [1230848/1237259]
Epoch 428
-------------------------------
loss: 0.013216 [ 2048/1237259]
loss: 0.013087 [206848/1237259]
loss: 0.013239 [411648/1237259]
loss: 0.011013 [616448/1237259]
loss: 0.011717 [821248/1237259]
loss: 0.013152 [1026048/1237259]
loss: 0.013171 [1230848/1237259]
Epoch 429
-------------------------------
loss: 0.012885 [ 2048/1237259]
loss: 0.012280 [206848/1237259]
loss: 0.012980 [411648/1237259]
loss: 0.012841 [616448/1237259]
loss: 0.014875 [821248/1237259]
loss: 0.011977 [1026048/1237259]
loss: 0.011845 [1230848/1237259]
Epoch 430
-------------------------------
loss: 0.012383 [ 2048/1237259]
loss: 0.012871 [206848/1237259]
loss: 0.013141 [411648/1237259]
loss: 0.013669 [616448/1237259]
loss: 0.012183 [821248/1237259]
loss: 0.012805 [1026048/1237259]
loss: 0.012938 [1230848/1237259]
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0495  
diversity: 0.1834  


Epoch 431
-------------------------------
loss: 0.014055 [ 2048/1237259]
loss: 0.014196 [206848/1237259]
loss: 0.012147 [411648/1237259]
loss: 0.013874 [616448/1237259]
loss: 0.012330 [821248/1237259]
loss: 0.012905 [1026048/1237259]
loss: 0.014574 [1230848/1237259]
Epoch 432
-------------------------------
loss: 0.012155 [ 2048/1237259]
loss: 0.013502 [206848/1237259]
loss: 0.014002 [411648/1237259]
loss: 0.013027 [616448/1237259]
loss: 0.013620 [821248/1237259]
loss: 0.012430 [1026048/1237259]
loss: 0.013147 [1230848/1237259]
Epoch 433
-------------------------------
loss: 0.013773 [ 2048/1237259]
loss: 0.011176 [206848/1237259]
loss: 0.012441 [411648/1237259]
loss: 0.014285 [616448/1237259]
loss: 0.013564 [821248/1237259]
loss: 0.011522 [1026048/1237259]
loss: 0.013561 [1230848/1237259]
Epoch 434
-------------------------------
loss: 0.012773 [ 2048/1237259]
loss: 0.013042 [206848/1237259]
loss: 0.012737 [411648/1237259]
loss: 0.012211 [616448/1237259]
loss: 0.013754 [821248/1237259]
loss: 0.014296 [1026048/1237259]
loss: 0.012041 [1230848/1237259]
Epoch 435
-------------------------------
loss: 0.013402 [ 2048/1237259]
loss: 0.011668 [206848/1237259]
loss: 0.013242 [411648/1237259]
loss: 0.012162 [616448/1237259]
loss: 0.013664 [821248/1237259]
loss: 0.012396 [1026048/1237259]
loss: 0.012948 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0618  
ndcg@20: 0.0499  
diversity: 0.1834  


Epoch 436
-------------------------------
loss: 0.012146 [ 2048/1237259]
loss: 0.012258 [206848/1237259]
loss: 0.012027 [411648/1237259]
loss: 0.012609 [616448/1237259]
loss: 0.011829 [821248/1237259]
loss: 0.012859 [1026048/1237259]
loss: 0.012009 [1230848/1237259]
Epoch 437
-------------------------------
loss: 0.012829 [ 2048/1237259]
loss: 0.013292 [206848/1237259]
loss: 0.011909 [411648/1237259]
loss: 0.011588 [616448/1237259]
loss: 0.011912 [821248/1237259]
loss: 0.013971 [1026048/1237259]
loss: 0.011318 [1230848/1237259]
Epoch 438
-------------------------------
loss: 0.013499 [ 2048/1237259]
loss: 0.012261 [206848/1237259]
loss: 0.014872 [411648/1237259]
loss: 0.012609 [616448/1237259]
loss: 0.013624 [821248/1237259]
loss: 0.012278 [1026048/1237259]
loss: 0.014981 [1230848/1237259]
Epoch 439
-------------------------------
loss: 0.013432 [ 2048/1237259]
loss: 0.012646 [206848/1237259]
loss: 0.012432 [411648/1237259]
loss: 0.011342 [616448/1237259]
loss: 0.012260 [821248/1237259]
loss: 0.012986 [1026048/1237259]
loss: 0.011513 [1230848/1237259]
Epoch 440
-------------------------------
loss: 0.011871 [ 2048/1237259]
loss: 0.012610 [206848/1237259]
loss: 0.012558 [411648/1237259]
loss: 0.013724 [616448/1237259]
loss: 0.013677 [821248/1237259]
loss: 0.013739 [1026048/1237259]
loss: 0.012104 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0615  
ndcg@20: 0.0497  
diversity: 0.1835  


Epoch 441
-------------------------------
loss: 0.011833 [ 2048/1237259]
loss: 0.011069 [206848/1237259]
loss: 0.012865 [411648/1237259]
loss: 0.013251 [616448/1237259]
loss: 0.012313 [821248/1237259]
loss: 0.014292 [1026048/1237259]
loss: 0.013134 [1230848/1237259]
Epoch 442
-------------------------------
loss: 0.012650 [ 2048/1237259]
loss: 0.013835 [206848/1237259]
loss: 0.013995 [411648/1237259]
loss: 0.014040 [616448/1237259]
loss: 0.013852 [821248/1237259]
loss: 0.012425 [1026048/1237259]
loss: 0.013673 [1230848/1237259]
Epoch 443
-------------------------------
loss: 0.012123 [ 2048/1237259]
loss: 0.012355 [206848/1237259]
loss: 0.012787 [411648/1237259]
loss: 0.014428 [616448/1237259]
loss: 0.011987 [821248/1237259]
loss: 0.012472 [1026048/1237259]
loss: 0.012526 [1230848/1237259]
Epoch 444
-------------------------------
loss: 0.011706 [ 2048/1237259]
loss: 0.012739 [206848/1237259]
loss: 0.013352 [411648/1237259]
loss: 0.013698 [616448/1237259]
loss: 0.013178 [821248/1237259]
loss: 0.012008 [1026048/1237259]
loss: 0.011702 [1230848/1237259]
Epoch 445
-------------------------------
loss: 0.013598 [ 2048/1237259]
loss: 0.012496 [206848/1237259]
loss: 0.012506 [411648/1237259]
loss: 0.012599 [616448/1237259]
loss: 0.012216 [821248/1237259]
loss: 0.012631 [1026048/1237259]
loss: 0.011981 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0616  
ndcg@20: 0.0497  
diversity: 0.1836  


Epoch 446
-------------------------------
loss: 0.011798 [ 2048/1237259]
loss: 0.013561 [206848/1237259]
loss: 0.012099 [411648/1237259]
loss: 0.013878 [616448/1237259]
loss: 0.015375 [821248/1237259]
loss: 0.012140 [1026048/1237259]
loss: 0.012745 [1230848/1237259]
Epoch 447
-------------------------------
loss: 0.014346 [ 2048/1237259]
loss: 0.016043 [206848/1237259]
loss: 0.012084 [411648/1237259]
loss: 0.013036 [616448/1237259]
loss: 0.013896 [821248/1237259]
loss: 0.012764 [1026048/1237259]
loss: 0.011179 [1230848/1237259]
Epoch 448
-------------------------------
loss: 0.012052 [ 2048/1237259]
loss: 0.012206 [206848/1237259]
loss: 0.013477 [411648/1237259]
loss: 0.014721 [616448/1237259]
loss: 0.011796 [821248/1237259]
loss: 0.012389 [1026048/1237259]
loss: 0.011496 [1230848/1237259]
Epoch 449
-------------------------------
loss: 0.012037 [ 2048/1237259]
loss: 0.012261 [206848/1237259]
loss: 0.013750 [411648/1237259]
loss: 0.013096 [616448/1237259]
loss: 0.013010 [821248/1237259]
loss: 0.012799 [1026048/1237259]
loss: 0.012614 [1230848/1237259]
Epoch 450
-------------------------------
loss: 0.013636 [ 2048/1237259]
loss: 0.012189 [206848/1237259]
loss: 0.012043 [411648/1237259]
loss: 0.012521 [616448/1237259]
loss: 0.014460 [821248/1237259]
loss: 0.013245 [1026048/1237259]
loss: 0.011575 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0617  
ndcg@20: 0.0499  
diversity: 0.1837  


Epoch 451
-------------------------------
loss: 0.011898 [ 2048/1237259]
loss: 0.013120 [206848/1237259]
loss: 0.012676 [411648/1237259]
loss: 0.014219 [616448/1237259]
loss: 0.013316 [821248/1237259]
loss: 0.013938 [1026048/1237259]
loss: 0.013032 [1230848/1237259]
Epoch 452
-------------------------------
loss: 0.013318 [ 2048/1237259]
loss: 0.012817 [206848/1237259]
loss: 0.013439 [411648/1237259]
loss: 0.013775 [616448/1237259]
loss: 0.012877 [821248/1237259]
loss: 0.012155 [1026048/1237259]
loss: 0.012027 [1230848/1237259]
Epoch 453
-------------------------------
loss: 0.011841 [ 2048/1237259]
loss: 0.013336 [206848/1237259]
loss: 0.012095 [411648/1237259]
loss: 0.011318 [616448/1237259]
loss: 0.012119 [821248/1237259]
loss: 0.012705 [1026048/1237259]
loss: 0.012848 [1230848/1237259]
Epoch 454
-------------------------------
loss: 0.014093 [ 2048/1237259]
loss: 0.013552 [206848/1237259]
loss: 0.011032 [411648/1237259]
loss: 0.011214 [616448/1237259]
loss: 0.014489 [821248/1237259]
loss: 0.012715 [1026048/1237259]
loss: 0.012483 [1230848/1237259]
Epoch 455
-------------------------------
loss: 0.011354 [ 2048/1237259]
loss: 0.012732 [206848/1237259]
loss: 0.011587 [411648/1237259]
loss: 0.011845 [616448/1237259]
loss: 0.013311 [821248/1237259]
loss: 0.013171 [1026048/1237259]
loss: 0.012753 [1230848/1237259]
Eval results: 
recall@20: 0.0617  
ndcg@20: 0.0499  
diversity: 0.1837  


Epoch 456
-------------------------------
loss: 0.013389 [ 2048/1237259]
loss: 0.012148 [206848/1237259]
loss: 0.012163 [411648/1237259]
loss: 0.011964 [616448/1237259]
loss: 0.012497 [821248/1237259]
loss: 0.013016 [1026048/1237259]
loss: 0.013290 [1230848/1237259]
Epoch 457
-------------------------------
loss: 0.013166 [ 2048/1237259]
loss: 0.013169 [206848/1237259]
loss: 0.013078 [411648/1237259]
loss: 0.013398 [616448/1237259]
loss: 0.012538 [821248/1237259]
loss: 0.013632 [1026048/1237259]
loss: 0.013024 [1230848/1237259]
Epoch 458
-------------------------------
loss: 0.012533 [ 2048/1237259]
loss: 0.011593 [206848/1237259]
loss: 0.012124 [411648/1237259]
loss: 0.012224 [616448/1237259]
loss: 0.012200 [821248/1237259]
loss: 0.012312 [1026048/1237259]
loss: 0.012747 [1230848/1237259]
Epoch 459
-------------------------------
loss: 0.011493 [ 2048/1237259]
loss: 0.013613 [206848/1237259]
loss: 0.012372 [411648/1237259]
loss: 0.014597 [616448/1237259]
loss: 0.012571 [821248/1237259]
loss: 0.013139 [1026048/1237259]
loss: 0.012458 [1230848/1237259]
Epoch 460
-------------------------------
loss: 0.010992 [ 2048/1237259]
loss: 0.012482 [206848/1237259]
loss: 0.013228 [411648/1237259]
loss: 0.012623 [616448/1237259]
loss: 0.012956 [821248/1237259]
loss: 0.014562 [1026048/1237259]
loss: 0.011922 [1230848/1237259]
Eval results: 
recall@20: 0.0616  
ndcg@20: 0.0498  
diversity: 0.1835  


Epoch 461
-------------------------------
loss: 0.012241 [ 2048/1237259]
loss: 0.012263 [206848/1237259]
loss: 0.013089 [411648/1237259]
loss: 0.011571 [616448/1237259]
loss: 0.012388 [821248/1237259]
loss: 0.014256 [1026048/1237259]
loss: 0.012456 [1230848/1237259]
Epoch 462
-------------------------------
loss: 0.013057 [ 2048/1237259]
loss: 0.012281 [206848/1237259]
loss: 0.013001 [411648/1237259]
loss: 0.012808 [616448/1237259]
loss: 0.011466 [821248/1237259]
loss: 0.012263 [1026048/1237259]
loss: 0.013346 [1230848/1237259]
Epoch 463
-------------------------------
loss: 0.011339 [ 2048/1237259]
loss: 0.013706 [206848/1237259]
loss: 0.011749 [411648/1237259]
loss: 0.012353 [616448/1237259]
loss: 0.013304 [821248/1237259]
loss: 0.013985 [1026048/1237259]
loss: 0.012977 [1230848/1237259]
Epoch 464
-------------------------------
loss: 0.011110 [ 2048/1237259]
loss: 0.013370 [206848/1237259]
loss: 0.013822 [411648/1237259]
loss: 0.013085 [616448/1237259]
loss: 0.012993 [821248/1237259]
loss: 0.013114 [1026048/1237259]
loss: 0.013138 [1230848/1237259]
Epoch 465
-------------------------------
loss: 0.011857 [ 2048/1237259]
loss: 0.012195 [206848/1237259]
loss: 0.011492 [411648/1237259]
loss: 0.011905 [616448/1237259]
loss: 0.011735 [821248/1237259]
loss: 0.013420 [1026048/1237259]
loss: 0.012445 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0619  
ndcg@20: 0.0499  
diversity: 0.1835  


Epoch 466
-------------------------------
loss: 0.013902 [ 2048/1237259]
loss: 0.012523 [206848/1237259]
loss: 0.013401 [411648/1237259]
loss: 0.013656 [616448/1237259]
loss: 0.012788 [821248/1237259]
loss: 0.011802 [1026048/1237259]
loss: 0.013281 [1230848/1237259]
Epoch 467
-------------------------------
loss: 0.011510 [ 2048/1237259]
loss: 0.013304 [206848/1237259]
loss: 0.014427 [411648/1237259]
loss: 0.013401 [616448/1237259]
loss: 0.013537 [821248/1237259]
loss: 0.011605 [1026048/1237259]
loss: 0.013670 [1230848/1237259]
Epoch 468
-------------------------------
loss: 0.012506 [ 2048/1237259]
loss: 0.015243 [206848/1237259]
loss: 0.012913 [411648/1237259]
loss: 0.012045 [616448/1237259]
loss: 0.012237 [821248/1237259]
loss: 0.014042 [1026048/1237259]
loss: 0.012021 [1230848/1237259]
Epoch 469
-------------------------------
loss: 0.012333 [ 2048/1237259]
loss: 0.011993 [206848/1237259]
loss: 0.011489 [411648/1237259]
loss: 0.010914 [616448/1237259]
loss: 0.012881 [821248/1237259]
loss: 0.011606 [1026048/1237259]
loss: 0.012301 [1230848/1237259]
Epoch 470
-------------------------------
loss: 0.012241 [ 2048/1237259]
loss: 0.013341 [206848/1237259]
loss: 0.011207 [411648/1237259]
loss: 0.012286 [616448/1237259]
loss: 0.013118 [821248/1237259]
loss: 0.011919 [1026048/1237259]
loss: 0.011936 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0621  
ndcg@20: 0.0500  
diversity: 0.1834  


Epoch 471
-------------------------------
loss: 0.012764 [ 2048/1237259]
loss: 0.012718 [206848/1237259]
loss: 0.011776 [411648/1237259]
loss: 0.013028 [616448/1237259]
loss: 0.013597 [821248/1237259]
loss: 0.012676 [1026048/1237259]
loss: 0.013085 [1230848/1237259]
Epoch 472
-------------------------------
loss: 0.011566 [ 2048/1237259]
loss: 0.012587 [206848/1237259]
loss: 0.011772 [411648/1237259]
loss: 0.013180 [616448/1237259]
loss: 0.011399 [821248/1237259]
loss: 0.013276 [1026048/1237259]
loss: 0.012759 [1230848/1237259]
Epoch 473
-------------------------------
loss: 0.011976 [ 2048/1237259]
loss: 0.013049 [206848/1237259]
loss: 0.011521 [411648/1237259]
loss: 0.013833 [616448/1237259]
loss: 0.013819 [821248/1237259]
loss: 0.012008 [1026048/1237259]
loss: 0.013356 [1230848/1237259]
Epoch 474
-------------------------------
loss: 0.012479 [ 2048/1237259]
loss: 0.011438 [206848/1237259]
loss: 0.012755 [411648/1237259]
loss: 0.011858 [616448/1237259]
loss: 0.013304 [821248/1237259]
loss: 0.012763 [1026048/1237259]
loss: 0.011865 [1230848/1237259]
Epoch 475
-------------------------------
loss: 0.012153 [ 2048/1237259]
loss: 0.011875 [206848/1237259]
loss: 0.013256 [411648/1237259]
loss: 0.014212 [616448/1237259]
loss: 0.012270 [821248/1237259]
loss: 0.012771 [1026048/1237259]
loss: 0.012930 [1230848/1237259]
Eval results: 
recall@20: 0.0619  
ndcg@20: 0.0499  
diversity: 0.1836  


Epoch 476
-------------------------------
loss: 0.012925 [ 2048/1237259]
loss: 0.013016 [206848/1237259]
loss: 0.012521 [411648/1237259]
loss: 0.012777 [616448/1237259]
loss: 0.012833 [821248/1237259]
loss: 0.012842 [1026048/1237259]
loss: 0.010841 [1230848/1237259]
Epoch 477
-------------------------------
loss: 0.013802 [ 2048/1237259]
loss: 0.013680 [206848/1237259]
loss: 0.012757 [411648/1237259]
loss: 0.013219 [616448/1237259]
loss: 0.012227 [821248/1237259]
loss: 0.013102 [1026048/1237259]
loss: 0.013687 [1230848/1237259]
Epoch 478
-------------------------------
loss: 0.014237 [ 2048/1237259]
loss: 0.012564 [206848/1237259]
loss: 0.013324 [411648/1237259]
loss: 0.012352 [616448/1237259]
loss: 0.013750 [821248/1237259]
loss: 0.012226 [1026048/1237259]
loss: 0.012886 [1230848/1237259]
Epoch 479
-------------------------------
loss: 0.012644 [ 2048/1237259]
loss: 0.013232 [206848/1237259]
loss: 0.013929 [411648/1237259]
loss: 0.012758 [616448/1237259]
loss: 0.012240 [821248/1237259]
loss: 0.012924 [1026048/1237259]
loss: 0.012728 [1230848/1237259]
Epoch 480
-------------------------------
loss: 0.013509 [ 2048/1237259]
loss: 0.011361 [206848/1237259]
loss: 0.013389 [411648/1237259]
loss: 0.012792 [616448/1237259]
loss: 0.011857 [821248/1237259]
loss: 0.013251 [1026048/1237259]
loss: 0.010782 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0622  
ndcg@20: 0.0502  
diversity: 0.1837  


Epoch 481
-------------------------------
loss: 0.011437 [ 2048/1237259]
loss: 0.012334 [206848/1237259]
loss: 0.012345 [411648/1237259]
loss: 0.013670 [616448/1237259]
loss: 0.012289 [821248/1237259]
loss: 0.013249 [1026048/1237259]
loss: 0.012089 [1230848/1237259]
Epoch 482
-------------------------------
loss: 0.015347 [ 2048/1237259]
loss: 0.013747 [206848/1237259]
loss: 0.012839 [411648/1237259]
loss: 0.012438 [616448/1237259]
loss: 0.011784 [821248/1237259]
loss: 0.012633 [1026048/1237259]
loss: 0.012706 [1230848/1237259]
Epoch 483
-------------------------------
loss: 0.013052 [ 2048/1237259]
loss: 0.014676 [206848/1237259]
loss: 0.014882 [411648/1237259]
loss: 0.012064 [616448/1237259]
loss: 0.013203 [821248/1237259]
loss: 0.014061 [1026048/1237259]
loss: 0.013548 [1230848/1237259]
Epoch 484
-------------------------------
loss: 0.012972 [ 2048/1237259]
loss: 0.012999 [206848/1237259]
loss: 0.011696 [411648/1237259]
loss: 0.012106 [616448/1237259]
loss: 0.013013 [821248/1237259]
loss: 0.012789 [1026048/1237259]
loss: 0.012843 [1230848/1237259]
Epoch 485
-------------------------------
loss: 0.011696 [ 2048/1237259]
loss: 0.012740 [206848/1237259]
loss: 0.012798 [411648/1237259]
loss: 0.012928 [616448/1237259]
loss: 0.012407 [821248/1237259]
loss: 0.011472 [1026048/1237259]
loss: 0.012490 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0620  
ndcg@20: 0.0501  
diversity: 0.1837  


Epoch 486
-------------------------------
loss: 0.014440 [ 2048/1237259]
loss: 0.013652 [206848/1237259]
loss: 0.013713 [411648/1237259]
loss: 0.014331 [616448/1237259]
loss: 0.014163 [821248/1237259]
loss: 0.013443 [1026048/1237259]
loss: 0.012254 [1230848/1237259]
Epoch 487
-------------------------------
loss: 0.011964 [ 2048/1237259]
loss: 0.014001 [206848/1237259]
loss: 0.013402 [411648/1237259]
loss: 0.011672 [616448/1237259]
loss: 0.011812 [821248/1237259]
loss: 0.011234 [1026048/1237259]
loss: 0.013242 [1230848/1237259]
Epoch 488
-------------------------------
loss: 0.012318 [ 2048/1237259]
loss: 0.011617 [206848/1237259]
loss: 0.013361 [411648/1237259]
loss: 0.014572 [616448/1237259]
loss: 0.011388 [821248/1237259]
loss: 0.011901 [1026048/1237259]
loss: 0.013473 [1230848/1237259]
Epoch 489
-------------------------------
loss: 0.013297 [ 2048/1237259]
loss: 0.013087 [206848/1237259]
loss: 0.014262 [411648/1237259]
loss: 0.013916 [616448/1237259]
loss: 0.012220 [821248/1237259]
loss: 0.013226 [1026048/1237259]
loss: 0.012797 [1230848/1237259]
Epoch 490
-------------------------------
loss: 0.012778 [ 2048/1237259]
loss: 0.011063 [206848/1237259]
loss: 0.011276 [411648/1237259]
loss: 0.012709 [616448/1237259]
loss: 0.012522 [821248/1237259]
loss: 0.011737 [1026048/1237259]
loss: 0.011347 [1230848/1237259]
Eval results: 
recall@20: 0.0618  
ndcg@20: 0.0500  
diversity: 0.1837  


Epoch 491
-------------------------------
loss: 0.012856 [ 2048/1237259]
loss: 0.012268 [206848/1237259]
loss: 0.011604 [411648/1237259]
loss: 0.013499 [616448/1237259]
loss: 0.012818 [821248/1237259]
loss: 0.012228 [1026048/1237259]
loss: 0.012409 [1230848/1237259]
Epoch 492
-------------------------------
loss: 0.012879 [ 2048/1237259]
loss: 0.013487 [206848/1237259]
loss: 0.013083 [411648/1237259]
loss: 0.012499 [616448/1237259]
loss: 0.012944 [821248/1237259]
loss: 0.013161 [1026048/1237259]
loss: 0.011742 [1230848/1237259]
Epoch 493
-------------------------------
loss: 0.012568 [ 2048/1237259]
loss: 0.011686 [206848/1237259]
loss: 0.013296 [411648/1237259]
loss: 0.013619 [616448/1237259]
loss: 0.011765 [821248/1237259]
loss: 0.011748 [1026048/1237259]
loss: 0.013091 [1230848/1237259]
Epoch 494
-------------------------------
loss: 0.012367 [ 2048/1237259]
loss: 0.012167 [206848/1237259]
loss: 0.011496 [411648/1237259]
loss: 0.011945 [616448/1237259]
loss: 0.013011 [821248/1237259]
loss: 0.012327 [1026048/1237259]
loss: 0.012473 [1230848/1237259]
Epoch 495
-------------------------------
loss: 0.014239 [ 2048/1237259]
loss: 0.012926 [206848/1237259]
loss: 0.013419 [411648/1237259]
loss: 0.011528 [616448/1237259]
loss: 0.013108 [821248/1237259]
loss: 0.012426 [1026048/1237259]
loss: 0.012944 [1230848/1237259]
Eval results: 
recall@20: 0.0618  
ndcg@20: 0.0500  
diversity: 0.1836  


Epoch 496
-------------------------------
loss: 0.010922 [ 2048/1237259]
loss: 0.012741 [206848/1237259]
loss: 0.013091 [411648/1237259]
loss: 0.011850 [616448/1237259]
loss: 0.013164 [821248/1237259]
loss: 0.011954 [1026048/1237259]
loss: 0.011668 [1230848/1237259]
Epoch 497
-------------------------------
loss: 0.012681 [ 2048/1237259]
loss: 0.012603 [206848/1237259]
loss: 0.013014 [411648/1237259]
loss: 0.014223 [616448/1237259]
loss: 0.012524 [821248/1237259]
loss: 0.013829 [1026048/1237259]
loss: 0.012345 [1230848/1237259]
Epoch 498
-------------------------------
loss: 0.012757 [ 2048/1237259]
loss: 0.012151 [206848/1237259]
loss: 0.012630 [411648/1237259]
loss: 0.013727 [616448/1237259]
loss: 0.012930 [821248/1237259]
loss: 0.013759 [1026048/1237259]
loss: 0.012402 [1230848/1237259]
Epoch 499
-------------------------------
loss: 0.012063 [ 2048/1237259]
loss: 0.013398 [206848/1237259]
loss: 0.011967 [411648/1237259]
loss: 0.013240 [616448/1237259]
loss: 0.013091 [821248/1237259]
loss: 0.014851 [1026048/1237259]
loss: 0.013188 [1230848/1237259]
Epoch 500
-------------------------------
loss: 0.012985 [ 2048/1237259]
loss: 0.011764 [206848/1237259]
loss: 0.011788 [411648/1237259]
loss: 0.010822 [616448/1237259]
loss: 0.012781 [821248/1237259]
loss: 0.013237 [1026048/1237259]
loss: 0.011454 [1230848/1237259]
Eval results: 
recall@20: 0.0622  
ndcg@20: 0.0502  
diversity: 0.1837  


Epoch 501
-------------------------------
loss: 0.012779 [ 2048/1237259]
loss: 0.013718 [206848/1237259]
loss: 0.011692 [411648/1237259]
loss: 0.011652 [616448/1237259]
loss: 0.012298 [821248/1237259]
loss: 0.012925 [1026048/1237259]
loss: 0.013590 [1230848/1237259]
Epoch 502
-------------------------------
loss: 0.011122 [ 2048/1237259]
loss: 0.012530 [206848/1237259]
loss: 0.013416 [411648/1237259]
loss: 0.012319 [616448/1237259]
loss: 0.013492 [821248/1237259]
loss: 0.012333 [1026048/1237259]
loss: 0.013784 [1230848/1237259]
Epoch 503
-------------------------------
loss: 0.011300 [ 2048/1237259]
loss: 0.013920 [206848/1237259]
loss: 0.012636 [411648/1237259]
loss: 0.013880 [616448/1237259]
loss: 0.013291 [821248/1237259]
loss: 0.012163 [1026048/1237259]
loss: 0.011493 [1230848/1237259]
Epoch 504
-------------------------------
loss: 0.012197 [ 2048/1237259]
loss: 0.012456 [206848/1237259]
loss: 0.011947 [411648/1237259]
loss: 0.013159 [616448/1237259]
loss: 0.013125 [821248/1237259]
loss: 0.011683 [1026048/1237259]
loss: 0.012672 [1230848/1237259]
Epoch 505
-------------------------------
loss: 0.011836 [ 2048/1237259]
loss: 0.014206 [206848/1237259]
loss: 0.014309 [411648/1237259]
loss: 0.013712 [616448/1237259]
loss: 0.012927 [821248/1237259]
loss: 0.013392 [1026048/1237259]
loss: 0.014723 [1230848/1237259]
Eval results: 
recall@20: 0.0621  
ndcg@20: 0.0501  
diversity: 0.1836  


Epoch 506
-------------------------------
loss: 0.013680 [ 2048/1237259]
loss: 0.015191 [206848/1237259]
loss: 0.012822 [411648/1237259]
loss: 0.013068 [616448/1237259]
loss: 0.014471 [821248/1237259]
loss: 0.011429 [1026048/1237259]
loss: 0.012176 [1230848/1237259]
Epoch 507
-------------------------------
loss: 0.013107 [ 2048/1237259]
loss: 0.011469 [206848/1237259]
loss: 0.012923 [411648/1237259]
loss: 0.014375 [616448/1237259]
loss: 0.012953 [821248/1237259]
loss: 0.012132 [1026048/1237259]
loss: 0.013503 [1230848/1237259]
Epoch 508
-------------------------------
loss: 0.013131 [ 2048/1237259]
loss: 0.011682 [206848/1237259]
loss: 0.012517 [411648/1237259]
loss: 0.013463 [616448/1237259]
loss: 0.012804 [821248/1237259]
loss: 0.013542 [1026048/1237259]
loss: 0.012176 [1230848/1237259]
Epoch 509
-------------------------------
loss: 0.014125 [ 2048/1237259]
loss: 0.012805 [206848/1237259]
loss: 0.011166 [411648/1237259]
loss: 0.011447 [616448/1237259]
loss: 0.012930 [821248/1237259]
loss: 0.014027 [1026048/1237259]
loss: 0.012180 [1230848/1237259]
Epoch 510
-------------------------------
loss: 0.011503 [ 2048/1237259]
loss: 0.013315 [206848/1237259]
loss: 0.011868 [411648/1237259]
loss: 0.013654 [616448/1237259]
loss: 0.012680 [821248/1237259]
loss: 0.011193 [1026048/1237259]
loss: 0.013866 [1230848/1237259]
Eval results: 
recall@20: 0.0620  
ndcg@20: 0.0501  
diversity: 0.1837  


Epoch 511
-------------------------------
loss: 0.011192 [ 2048/1237259]
loss: 0.013714 [206848/1237259]
loss: 0.013612 [411648/1237259]
loss: 0.012078 [616448/1237259]
loss: 0.011487 [821248/1237259]
loss: 0.014031 [1026048/1237259]
loss: 0.012233 [1230848/1237259]
Epoch 512
-------------------------------
loss: 0.012115 [ 2048/1237259]
loss: 0.013138 [206848/1237259]
loss: 0.012479 [411648/1237259]
loss: 0.013588 [616448/1237259]
loss: 0.013460 [821248/1237259]
loss: 0.013209 [1026048/1237259]
loss: 0.012928 [1230848/1237259]
Epoch 513
-------------------------------
loss: 0.011507 [ 2048/1237259]
loss: 0.013145 [206848/1237259]
loss: 0.012871 [411648/1237259]
loss: 0.014442 [616448/1237259]
loss: 0.013545 [821248/1237259]
loss: 0.012979 [1026048/1237259]
loss: 0.013447 [1230848/1237259]
Epoch 514
-------------------------------
loss: 0.012218 [ 2048/1237259]
loss: 0.012732 [206848/1237259]
loss: 0.012782 [411648/1237259]
loss: 0.012691 [616448/1237259]
loss: 0.013664 [821248/1237259]
loss: 0.011224 [1026048/1237259]
loss: 0.012624 [1230848/1237259]
Epoch 515
-------------------------------
loss: 0.013179 [ 2048/1237259]
loss: 0.012961 [206848/1237259]
loss: 0.011934 [411648/1237259]
loss: 0.012866 [616448/1237259]
loss: 0.013940 [821248/1237259]
loss: 0.013091 [1026048/1237259]
loss: 0.011870 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0621  
ndcg@20: 0.0500  
diversity: 0.1838  


Epoch 516
-------------------------------
loss: 0.010547 [ 2048/1237259]
loss: 0.013015 [206848/1237259]
loss: 0.012630 [411648/1237259]
loss: 0.013311 [616448/1237259]
loss: 0.012761 [821248/1237259]
loss: 0.014587 [1026048/1237259]
loss: 0.013952 [1230848/1237259]
Epoch 517
-------------------------------
loss: 0.011963 [ 2048/1237259]
loss: 0.012923 [206848/1237259]
loss: 0.011351 [411648/1237259]
loss: 0.013578 [616448/1237259]
loss: 0.012963 [821248/1237259]
loss: 0.012356 [1026048/1237259]
loss: 0.012702 [1230848/1237259]
Epoch 518
-------------------------------
loss: 0.012481 [ 2048/1237259]
loss: 0.011621 [206848/1237259]
loss: 0.013395 [411648/1237259]
loss: 0.014964 [616448/1237259]
loss: 0.012394 [821248/1237259]
loss: 0.012920 [1026048/1237259]
loss: 0.013779 [1230848/1237259]
Epoch 519
-------------------------------
loss: 0.013531 [ 2048/1237259]
loss: 0.011916 [206848/1237259]
loss: 0.011119 [411648/1237259]
loss: 0.012934 [616448/1237259]
loss: 0.015659 [821248/1237259]
loss: 0.012024 [1026048/1237259]
loss: 0.012739 [1230848/1237259]
Epoch 520
-------------------------------
loss: 0.013490 [ 2048/1237259]
loss: 0.013529 [206848/1237259]
loss: 0.012391 [411648/1237259]
loss: 0.012737 [616448/1237259]
loss: 0.012368 [821248/1237259]
loss: 0.012254 [1026048/1237259]
loss: 0.013893 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0621  
ndcg@20: 0.0502  
diversity: 0.1839  


Epoch 521
-------------------------------
loss: 0.013149 [ 2048/1237259]
loss: 0.011841 [206848/1237259]
loss: 0.012631 [411648/1237259]
loss: 0.012561 [616448/1237259]
loss: 0.011927 [821248/1237259]
loss: 0.012653 [1026048/1237259]
loss: 0.011394 [1230848/1237259]
Epoch 522
-------------------------------
loss: 0.013734 [ 2048/1237259]
loss: 0.013757 [206848/1237259]
loss: 0.011945 [411648/1237259]
loss: 0.012355 [616448/1237259]
loss: 0.013562 [821248/1237259]
loss: 0.013334 [1026048/1237259]
loss: 0.012147 [1230848/1237259]
Epoch 523
-------------------------------
loss: 0.012016 [ 2048/1237259]
loss: 0.011147 [206848/1237259]
loss: 0.011199 [411648/1237259]
loss: 0.013976 [616448/1237259]
loss: 0.012352 [821248/1237259]
loss: 0.013341 [1026048/1237259]
loss: 0.012362 [1230848/1237259]
Epoch 524
-------------------------------
loss: 0.012170 [ 2048/1237259]
loss: 0.012217 [206848/1237259]
loss: 0.014160 [411648/1237259]
loss: 0.011759 [616448/1237259]
loss: 0.012136 [821248/1237259]
loss: 0.011560 [1026048/1237259]
loss: 0.012511 [1230848/1237259]
Epoch 525
-------------------------------
loss: 0.013708 [ 2048/1237259]
loss: 0.011736 [206848/1237259]
loss: 0.013082 [411648/1237259]
loss: 0.010683 [616448/1237259]
loss: 0.013404 [821248/1237259]
loss: 0.013697 [1026048/1237259]
loss: 0.012360 [1230848/1237259]
Eval results: 
recall@20: 0.0620  
ndcg@20: 0.0502  
diversity: 0.1839  


Epoch 526
-------------------------------
loss: 0.012031 [ 2048/1237259]
loss: 0.010715 [206848/1237259]
loss: 0.012623 [411648/1237259]
loss: 0.012712 [616448/1237259]
loss: 0.011170 [821248/1237259]
loss: 0.010906 [1026048/1237259]
loss: 0.012943 [1230848/1237259]
Epoch 527
-------------------------------
loss: 0.012348 [ 2048/1237259]
loss: 0.012850 [206848/1237259]
loss: 0.012665 [411648/1237259]
loss: 0.012265 [616448/1237259]
loss: 0.010295 [821248/1237259]
loss: 0.013301 [1026048/1237259]
loss: 0.013159 [1230848/1237259]
Epoch 528
-------------------------------
loss: 0.012564 [ 2048/1237259]
loss: 0.011730 [206848/1237259]
loss: 0.013253 [411648/1237259]
loss: 0.011940 [616448/1237259]
loss: 0.013572 [821248/1237259]
loss: 0.011765 [1026048/1237259]
loss: 0.013086 [1230848/1237259]
Epoch 529
-------------------------------
loss: 0.012968 [ 2048/1237259]
loss: 0.013820 [206848/1237259]
loss: 0.012243 [411648/1237259]
loss: 0.012440 [616448/1237259]
loss: 0.013335 [821248/1237259]
loss: 0.012403 [1026048/1237259]
loss: 0.011938 [1230848/1237259]
Epoch 530
-------------------------------
loss: 0.011798 [ 2048/1237259]
loss: 0.010888 [206848/1237259]
loss: 0.013260 [411648/1237259]
loss: 0.012957 [616448/1237259]
loss: 0.012083 [821248/1237259]
loss: 0.013155 [1026048/1237259]
loss: 0.011582 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0622  
ndcg@20: 0.0503  
diversity: 0.1839  


Epoch 531
-------------------------------
loss: 0.012871 [ 2048/1237259]
loss: 0.012710 [206848/1237259]
loss: 0.011607 [411648/1237259]
loss: 0.011982 [616448/1237259]
loss: 0.012657 [821248/1237259]
loss: 0.013855 [1026048/1237259]
loss: 0.011668 [1230848/1237259]
Epoch 532
-------------------------------
loss: 0.013322 [ 2048/1237259]
loss: 0.012182 [206848/1237259]
loss: 0.012647 [411648/1237259]
loss: 0.012681 [616448/1237259]
loss: 0.010848 [821248/1237259]
loss: 0.012147 [1026048/1237259]
loss: 0.010571 [1230848/1237259]
Epoch 533
-------------------------------
loss: 0.012572 [ 2048/1237259]
loss: 0.013609 [206848/1237259]
loss: 0.013229 [411648/1237259]
loss: 0.012783 [616448/1237259]
loss: 0.011670 [821248/1237259]
loss: 0.013346 [1026048/1237259]
loss: 0.012068 [1230848/1237259]
Epoch 534
-------------------------------
loss: 0.012244 [ 2048/1237259]
loss: 0.011518 [206848/1237259]
loss: 0.013324 [411648/1237259]
loss: 0.011768 [616448/1237259]
loss: 0.011657 [821248/1237259]
loss: 0.013084 [1026048/1237259]
loss: 0.012234 [1230848/1237259]
Epoch 535
-------------------------------
loss: 0.012325 [ 2048/1237259]
loss: 0.012970 [206848/1237259]
loss: 0.012439 [411648/1237259]
loss: 0.013587 [616448/1237259]
loss: 0.012129 [821248/1237259]
loss: 0.012207 [1026048/1237259]
loss: 0.014168 [1230848/1237259]
Eval results: 
recall@20: 0.0622  
ndcg@20: 0.0502  
diversity: 0.1838  


Epoch 536
-------------------------------
loss: 0.012249 [ 2048/1237259]
loss: 0.013029 [206848/1237259]
loss: 0.013803 [411648/1237259]
loss: 0.012241 [616448/1237259]
loss: 0.012207 [821248/1237259]
loss: 0.014198 [1026048/1237259]
loss: 0.013162 [1230848/1237259]
Epoch 537
-------------------------------
loss: 0.012388 [ 2048/1237259]
loss: 0.011285 [206848/1237259]
loss: 0.013012 [411648/1237259]
loss: 0.012711 [616448/1237259]
loss: 0.011462 [821248/1237259]
loss: 0.013606 [1026048/1237259]
loss: 0.013824 [1230848/1237259]
Epoch 538
-------------------------------
loss: 0.013616 [ 2048/1237259]
loss: 0.012681 [206848/1237259]
loss: 0.012650 [411648/1237259]
loss: 0.012367 [616448/1237259]
loss: 0.012348 [821248/1237259]
loss: 0.013020 [1026048/1237259]
loss: 0.012669 [1230848/1237259]
Epoch 539
-------------------------------
loss: 0.012158 [ 2048/1237259]
loss: 0.013855 [206848/1237259]
loss: 0.012676 [411648/1237259]
loss: 0.014116 [616448/1237259]
loss: 0.012915 [821248/1237259]
loss: 0.012325 [1026048/1237259]
loss: 0.013860 [1230848/1237259]
Epoch 540
-------------------------------
loss: 0.013386 [ 2048/1237259]
loss: 0.011480 [206848/1237259]
loss: 0.013623 [411648/1237259]
loss: 0.012864 [616448/1237259]
loss: 0.011797 [821248/1237259]
loss: 0.011822 [1026048/1237259]
loss: 0.013291 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0624  
ndcg@20: 0.0503  
diversity: 0.1837  


Epoch 541
-------------------------------
loss: 0.013200 [ 2048/1237259]
loss: 0.011282 [206848/1237259]
loss: 0.014491 [411648/1237259]
loss: 0.011452 [616448/1237259]
loss: 0.012398 [821248/1237259]
loss: 0.013355 [1026048/1237259]
loss: 0.012195 [1230848/1237259]
Epoch 542
-------------------------------
loss: 0.012764 [ 2048/1237259]
loss: 0.012295 [206848/1237259]
loss: 0.012578 [411648/1237259]
loss: 0.011453 [616448/1237259]
loss: 0.012941 [821248/1237259]
loss: 0.012703 [1026048/1237259]
loss: 0.013064 [1230848/1237259]
Epoch 543
-------------------------------
loss: 0.012464 [ 2048/1237259]
loss: 0.011854 [206848/1237259]
loss: 0.011937 [411648/1237259]
loss: 0.012208 [616448/1237259]
loss: 0.013258 [821248/1237259]
loss: 0.013016 [1026048/1237259]
loss: 0.012434 [1230848/1237259]
Epoch 544
-------------------------------
loss: 0.011762 [ 2048/1237259]
loss: 0.012461 [206848/1237259]
loss: 0.012425 [411648/1237259]
loss: 0.012610 [616448/1237259]
loss: 0.011673 [821248/1237259]
loss: 0.013104 [1026048/1237259]
loss: 0.012749 [1230848/1237259]
Epoch 545
-------------------------------
loss: 0.011903 [ 2048/1237259]
loss: 0.012026 [206848/1237259]
loss: 0.012352 [411648/1237259]
loss: 0.013403 [616448/1237259]
loss: 0.012895 [821248/1237259]
loss: 0.012010 [1026048/1237259]
loss: 0.012586 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0626  
ndcg@20: 0.0504  
diversity: 0.1837  


Epoch 546
-------------------------------
loss: 0.013242 [ 2048/1237259]
loss: 0.013411 [206848/1237259]
loss: 0.013238 [411648/1237259]
loss: 0.014633 [616448/1237259]
loss: 0.014207 [821248/1237259]
loss: 0.013975 [1026048/1237259]
loss: 0.011767 [1230848/1237259]
Epoch 547
-------------------------------
loss: 0.011690 [ 2048/1237259]
loss: 0.012705 [206848/1237259]
loss: 0.013612 [411648/1237259]
loss: 0.013197 [616448/1237259]
loss: 0.011865 [821248/1237259]
loss: 0.012752 [1026048/1237259]
loss: 0.013194 [1230848/1237259]
Epoch 548
-------------------------------
loss: 0.012898 [ 2048/1237259]
loss: 0.012855 [206848/1237259]
loss: 0.012279 [411648/1237259]
loss: 0.011985 [616448/1237259]
loss: 0.012005 [821248/1237259]
loss: 0.012550 [1026048/1237259]
loss: 0.013042 [1230848/1237259]
Epoch 549
-------------------------------
loss: 0.011776 [ 2048/1237259]
loss: 0.014211 [206848/1237259]
loss: 0.013649 [411648/1237259]
loss: 0.011685 [616448/1237259]
loss: 0.011813 [821248/1237259]
loss: 0.012201 [1026048/1237259]
loss: 0.013820 [1230848/1237259]
Epoch 550
-------------------------------
loss: 0.015332 [ 2048/1237259]
loss: 0.013320 [206848/1237259]
loss: 0.012491 [411648/1237259]
loss: 0.011850 [616448/1237259]
loss: 0.012729 [821248/1237259]
loss: 0.012599 [1026048/1237259]
loss: 0.012164 [1230848/1237259]
Eval results: 
recall@20: 0.0624  
ndcg@20: 0.0504  
diversity: 0.1838  


Epoch 551
-------------------------------
loss: 0.012923 [ 2048/1237259]
loss: 0.013087 [206848/1237259]
loss: 0.012698 [411648/1237259]
loss: 0.013582 [616448/1237259]
loss: 0.012018 [821248/1237259]
loss: 0.013361 [1026048/1237259]
loss: 0.012465 [1230848/1237259]
Epoch 552
-------------------------------
loss: 0.013818 [ 2048/1237259]
loss: 0.011959 [206848/1237259]
loss: 0.013190 [411648/1237259]
loss: 0.013292 [616448/1237259]
loss: 0.011713 [821248/1237259]
loss: 0.012762 [1026048/1237259]
loss: 0.011973 [1230848/1237259]
Epoch 553
-------------------------------
loss: 0.013095 [ 2048/1237259]
loss: 0.012384 [206848/1237259]
loss: 0.014333 [411648/1237259]
loss: 0.012335 [616448/1237259]
loss: 0.012900 [821248/1237259]
loss: 0.012300 [1026048/1237259]
loss: 0.012257 [1230848/1237259]
Epoch 554
-------------------------------
loss: 0.012302 [ 2048/1237259]
loss: 0.011378 [206848/1237259]
loss: 0.011771 [411648/1237259]
loss: 0.013095 [616448/1237259]
loss: 0.012152 [821248/1237259]
loss: 0.011761 [1026048/1237259]
loss: 0.012239 [1230848/1237259]
Epoch 555
-------------------------------
loss: 0.012993 [ 2048/1237259]
loss: 0.012545 [206848/1237259]
loss: 0.012575 [411648/1237259]
loss: 0.014304 [616448/1237259]
loss: 0.011552 [821248/1237259]
loss: 0.012963 [1026048/1237259]
loss: 0.013146 [1230848/1237259]
Eval results: 
recall@20: 0.0623  
ndcg@20: 0.0503  
diversity: 0.1837  


Epoch 556
-------------------------------
loss: 0.013628 [ 2048/1237259]
loss: 0.011149 [206848/1237259]
loss: 0.014107 [411648/1237259]
loss: 0.013264 [616448/1237259]
loss: 0.012584 [821248/1237259]
loss: 0.011662 [1026048/1237259]
loss: 0.012713 [1230848/1237259]
Epoch 557
-------------------------------
loss: 0.012791 [ 2048/1237259]
loss: 0.011936 [206848/1237259]
loss: 0.012645 [411648/1237259]
loss: 0.011563 [616448/1237259]
loss: 0.012282 [821248/1237259]
loss: 0.013040 [1026048/1237259]
loss: 0.013433 [1230848/1237259]
Epoch 558
-------------------------------
loss: 0.011890 [ 2048/1237259]
loss: 0.010770 [206848/1237259]
loss: 0.011535 [411648/1237259]
loss: 0.012884 [616448/1237259]
loss: 0.013175 [821248/1237259]
loss: 0.012670 [1026048/1237259]
loss: 0.014101 [1230848/1237259]
Epoch 559
-------------------------------
loss: 0.012085 [ 2048/1237259]
loss: 0.012374 [206848/1237259]
loss: 0.012818 [411648/1237259]
loss: 0.012931 [616448/1237259]
loss: 0.011342 [821248/1237259]
loss: 0.014170 [1026048/1237259]
loss: 0.012154 [1230848/1237259]
Epoch 560
-------------------------------
loss: 0.012767 [ 2048/1237259]
loss: 0.013229 [206848/1237259]
loss: 0.011809 [411648/1237259]
loss: 0.013785 [616448/1237259]
loss: 0.013343 [821248/1237259]
loss: 0.012597 [1026048/1237259]
loss: 0.012337 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0627  
ndcg@20: 0.0505  
diversity: 0.1838  


Epoch 561
-------------------------------
loss: 0.013652 [ 2048/1237259]
loss: 0.012659 [206848/1237259]
loss: 0.012402 [411648/1237259]
loss: 0.010927 [616448/1237259]
loss: 0.011470 [821248/1237259]
loss: 0.013405 [1026048/1237259]
loss: 0.014449 [1230848/1237259]
Epoch 562
-------------------------------
loss: 0.011657 [ 2048/1237259]
loss: 0.013558 [206848/1237259]
loss: 0.012325 [411648/1237259]
loss: 0.014412 [616448/1237259]
loss: 0.013367 [821248/1237259]
loss: 0.013401 [1026048/1237259]
loss: 0.012588 [1230848/1237259]
Epoch 563
-------------------------------
loss: 0.012454 [ 2048/1237259]
loss: 0.012845 [206848/1237259]
loss: 0.012075 [411648/1237259]
loss: 0.011756 [616448/1237259]
loss: 0.012191 [821248/1237259]
loss: 0.012308 [1026048/1237259]
loss: 0.012773 [1230848/1237259]
Epoch 564
-------------------------------
loss: 0.012119 [ 2048/1237259]
loss: 0.014133 [206848/1237259]
loss: 0.011746 [411648/1237259]
loss: 0.013882 [616448/1237259]
loss: 0.011714 [821248/1237259]
loss: 0.012304 [1026048/1237259]
loss: 0.012893 [1230848/1237259]
Epoch 565
-------------------------------
loss: 0.012020 [ 2048/1237259]
loss: 0.012447 [206848/1237259]
loss: 0.012709 [411648/1237259]
loss: 0.012233 [616448/1237259]
loss: 0.012618 [821248/1237259]
loss: 0.012773 [1026048/1237259]
loss: 0.012542 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0628  
ndcg@20: 0.0507  
diversity: 0.1841  


Epoch 566
-------------------------------
loss: 0.012965 [ 2048/1237259]
loss: 0.013113 [206848/1237259]
loss: 0.015204 [411648/1237259]
loss: 0.010723 [616448/1237259]
loss: 0.011392 [821248/1237259]
loss: 0.011531 [1026048/1237259]
loss: 0.013353 [1230848/1237259]
Epoch 567
-------------------------------
loss: 0.011691 [ 2048/1237259]
loss: 0.013259 [206848/1237259]
loss: 0.012169 [411648/1237259]
loss: 0.011966 [616448/1237259]
loss: 0.012283 [821248/1237259]
loss: 0.013338 [1026048/1237259]
loss: 0.013109 [1230848/1237259]
Epoch 568
-------------------------------
loss: 0.012145 [ 2048/1237259]
loss: 0.012113 [206848/1237259]
loss: 0.013325 [411648/1237259]
loss: 0.011582 [616448/1237259]
loss: 0.012674 [821248/1237259]
loss: 0.013141 [1026048/1237259]
loss: 0.013587 [1230848/1237259]
Epoch 569
-------------------------------
loss: 0.011971 [ 2048/1237259]
loss: 0.011339 [206848/1237259]
loss: 0.011047 [411648/1237259]
loss: 0.012290 [616448/1237259]
loss: 0.011647 [821248/1237259]
loss: 0.012797 [1026048/1237259]
loss: 0.012025 [1230848/1237259]
Epoch 570
-------------------------------
loss: 0.013814 [ 2048/1237259]
loss: 0.014316 [206848/1237259]
loss: 0.012229 [411648/1237259]
loss: 0.012439 [616448/1237259]
loss: 0.011704 [821248/1237259]
loss: 0.012660 [1026048/1237259]
loss: 0.012164 [1230848/1237259]
Eval results: 
recall@20: 0.0628  
ndcg@20: 0.0506  
diversity: 0.1841  


Epoch 571
-------------------------------
loss: 0.012971 [ 2048/1237259]
loss: 0.011050 [206848/1237259]
loss: 0.012585 [411648/1237259]
loss: 0.011768 [616448/1237259]
loss: 0.012332 [821248/1237259]
loss: 0.013613 [1026048/1237259]
loss: 0.011205 [1230848/1237259]
Epoch 572
-------------------------------
loss: 0.012250 [ 2048/1237259]
loss: 0.012020 [206848/1237259]
loss: 0.011713 [411648/1237259]
loss: 0.013050 [616448/1237259]
loss: 0.012327 [821248/1237259]
loss: 0.013077 [1026048/1237259]
loss: 0.014256 [1230848/1237259]
Epoch 573
-------------------------------
loss: 0.013261 [ 2048/1237259]
loss: 0.013148 [206848/1237259]
loss: 0.012643 [411648/1237259]
loss: 0.012916 [616448/1237259]
loss: 0.011818 [821248/1237259]
loss: 0.013248 [1026048/1237259]
loss: 0.011955 [1230848/1237259]
Epoch 574
-------------------------------
loss: 0.012345 [ 2048/1237259]
loss: 0.013204 [206848/1237259]
loss: 0.011510 [411648/1237259]
loss: 0.010983 [616448/1237259]
loss: 0.013396 [821248/1237259]
loss: 0.013930 [1026048/1237259]
loss: 0.012216 [1230848/1237259]
Epoch 575
-------------------------------
loss: 0.013925 [ 2048/1237259]
loss: 0.012363 [206848/1237259]
loss: 0.011834 [411648/1237259]
loss: 0.014120 [616448/1237259]
loss: 0.012466 [821248/1237259]
loss: 0.011447 [1026048/1237259]
loss: 0.011133 [1230848/1237259]
Eval results: 
recall@20: 0.0626  
ndcg@20: 0.0505  
diversity: 0.1840  


Epoch 576
-------------------------------
loss: 0.014379 [ 2048/1237259]
loss: 0.011855 [206848/1237259]
loss: 0.011730 [411648/1237259]
loss: 0.012376 [616448/1237259]
loss: 0.012983 [821248/1237259]
loss: 0.013104 [1026048/1237259]
loss: 0.011498 [1230848/1237259]
Epoch 577
-------------------------------
loss: 0.012211 [ 2048/1237259]
loss: 0.013011 [206848/1237259]
loss: 0.012447 [411648/1237259]
loss: 0.011977 [616448/1237259]
loss: 0.013211 [821248/1237259]
loss: 0.013219 [1026048/1237259]
loss: 0.012914 [1230848/1237259]
Epoch 578
-------------------------------
loss: 0.013091 [ 2048/1237259]
loss: 0.012174 [206848/1237259]
loss: 0.014664 [411648/1237259]
loss: 0.012696 [616448/1237259]
loss: 0.013679 [821248/1237259]
loss: 0.011630 [1026048/1237259]
loss: 0.011785 [1230848/1237259]
Epoch 579
-------------------------------
loss: 0.012825 [ 2048/1237259]
loss: 0.012616 [206848/1237259]
loss: 0.012954 [411648/1237259]
loss: 0.013674 [616448/1237259]
loss: 0.011339 [821248/1237259]
loss: 0.013864 [1026048/1237259]
loss: 0.012858 [1230848/1237259]
Epoch 580
-------------------------------
loss: 0.014631 [ 2048/1237259]
loss: 0.013047 [206848/1237259]
loss: 0.011525 [411648/1237259]
loss: 0.015152 [616448/1237259]
loss: 0.012740 [821248/1237259]
loss: 0.012584 [1026048/1237259]
loss: 0.012856 [1230848/1237259]
Eval results: 
recall@20: 0.0626  
ndcg@20: 0.0505  
diversity: 0.1841  


Epoch 581
-------------------------------
loss: 0.012347 [ 2048/1237259]
loss: 0.011715 [206848/1237259]
loss: 0.011586 [411648/1237259]
loss: 0.012609 [616448/1237259]
loss: 0.012707 [821248/1237259]
loss: 0.013142 [1026048/1237259]
loss: 0.012253 [1230848/1237259]
Epoch 582
-------------------------------
loss: 0.012476 [ 2048/1237259]
loss: 0.012504 [206848/1237259]
loss: 0.011796 [411648/1237259]
loss: 0.011826 [616448/1237259]
loss: 0.012069 [821248/1237259]
loss: 0.013991 [1026048/1237259]
loss: 0.012212 [1230848/1237259]
Epoch 583
-------------------------------
loss: 0.012453 [ 2048/1237259]
loss: 0.011173 [206848/1237259]
loss: 0.013777 [411648/1237259]
loss: 0.012510 [616448/1237259]
loss: 0.012384 [821248/1237259]
loss: 0.012303 [1026048/1237259]
loss: 0.013884 [1230848/1237259]
Epoch 584
-------------------------------
loss: 0.013612 [ 2048/1237259]
loss: 0.011821 [206848/1237259]
loss: 0.011622 [411648/1237259]
loss: 0.013249 [616448/1237259]
loss: 0.012525 [821248/1237259]
loss: 0.011882 [1026048/1237259]
loss: 0.013119 [1230848/1237259]
Epoch 585
-------------------------------
loss: 0.012674 [ 2048/1237259]
loss: 0.012588 [206848/1237259]
loss: 0.013765 [411648/1237259]
loss: 0.011996 [616448/1237259]
loss: 0.012188 [821248/1237259]
loss: 0.012404 [1026048/1237259]
loss: 0.011940 [1230848/1237259]
Eval results: 
recall@20: 0.0628  
ndcg@20: 0.0505  
diversity: 0.1838  


Epoch 586
-------------------------------
loss: 0.012752 [ 2048/1237259]
loss: 0.012130 [206848/1237259]
loss: 0.012839 [411648/1237259]
loss: 0.011671 [616448/1237259]
loss: 0.011693 [821248/1237259]
loss: 0.014169 [1026048/1237259]
loss: 0.011817 [1230848/1237259]
Epoch 587
-------------------------------
loss: 0.014407 [ 2048/1237259]
loss: 0.011789 [206848/1237259]
loss: 0.011743 [411648/1237259]
loss: 0.012811 [616448/1237259]
loss: 0.012885 [821248/1237259]
loss: 0.013453 [1026048/1237259]
loss: 0.013704 [1230848/1237259]
Epoch 588
-------------------------------
loss: 0.012891 [ 2048/1237259]
loss: 0.012561 [206848/1237259]
loss: 0.012052 [411648/1237259]
loss: 0.011971 [616448/1237259]
loss: 0.013770 [821248/1237259]
loss: 0.012920 [1026048/1237259]
loss: 0.012247 [1230848/1237259]
Epoch 589
-------------------------------
loss: 0.013297 [ 2048/1237259]
loss: 0.012788 [206848/1237259]
loss: 0.012782 [411648/1237259]
loss: 0.013522 [616448/1237259]
loss: 0.011716 [821248/1237259]
loss: 0.012054 [1026048/1237259]
loss: 0.012001 [1230848/1237259]
Epoch 590
-------------------------------
loss: 0.012811 [ 2048/1237259]
loss: 0.012856 [206848/1237259]
loss: 0.012095 [411648/1237259]
loss: 0.013124 [616448/1237259]
loss: 0.011543 [821248/1237259]
loss: 0.012706 [1026048/1237259]
loss: 0.013134 [1230848/1237259]
Eval results: 
recall@20: 0.0625  
ndcg@20: 0.0505  
diversity: 0.1840  


Epoch 591
-------------------------------
loss: 0.012095 [ 2048/1237259]
loss: 0.011329 [206848/1237259]
loss: 0.012759 [411648/1237259]
loss: 0.010724 [616448/1237259]
loss: 0.012354 [821248/1237259]
loss: 0.013015 [1026048/1237259]
loss: 0.011915 [1230848/1237259]
Epoch 592
-------------------------------
loss: 0.012762 [ 2048/1237259]
loss: 0.012634 [206848/1237259]
loss: 0.011576 [411648/1237259]
loss: 0.012196 [616448/1237259]
loss: 0.012057 [821248/1237259]
loss: 0.012296 [1026048/1237259]
loss: 0.013364 [1230848/1237259]
Epoch 593
-------------------------------
loss: 0.012035 [ 2048/1237259]
loss: 0.013772 [206848/1237259]
loss: 0.012132 [411648/1237259]
loss: 0.012604 [616448/1237259]
loss: 0.011205 [821248/1237259]
loss: 0.012843 [1026048/1237259]
loss: 0.012621 [1230848/1237259]
Epoch 594
-------------------------------
loss: 0.012737 [ 2048/1237259]
loss: 0.010849 [206848/1237259]
loss: 0.011113 [411648/1237259]
loss: 0.012466 [616448/1237259]
loss: 0.012581 [821248/1237259]
loss: 0.013525 [1026048/1237259]
loss: 0.012197 [1230848/1237259]
Epoch 595
-------------------------------
loss: 0.012834 [ 2048/1237259]
loss: 0.012409 [206848/1237259]
loss: 0.012381 [411648/1237259]
loss: 0.010922 [616448/1237259]
loss: 0.012099 [821248/1237259]
loss: 0.011862 [1026048/1237259]
loss: 0.012967 [1230848/1237259]
Eval results: 
recall@20: 0.0625  
ndcg@20: 0.0505  
diversity: 0.1838  


Epoch 596
-------------------------------
loss: 0.011411 [ 2048/1237259]
loss: 0.011537 [206848/1237259]
loss: 0.013312 [411648/1237259]
loss: 0.012647 [616448/1237259]
loss: 0.012232 [821248/1237259]
loss: 0.013714 [1026048/1237259]
loss: 0.012188 [1230848/1237259]
Epoch 597
-------------------------------
loss: 0.012019 [ 2048/1237259]
loss: 0.012806 [206848/1237259]
loss: 0.013406 [411648/1237259]
loss: 0.012508 [616448/1237259]
loss: 0.011563 [821248/1237259]
loss: 0.011390 [1026048/1237259]
loss: 0.013047 [1230848/1237259]
Epoch 598
-------------------------------
loss: 0.012723 [ 2048/1237259]
loss: 0.013865 [206848/1237259]
loss: 0.013726 [411648/1237259]
loss: 0.012630 [616448/1237259]
loss: 0.012088 [821248/1237259]
loss: 0.015168 [1026048/1237259]
loss: 0.013423 [1230848/1237259]
Epoch 599
-------------------------------
loss: 0.014028 [ 2048/1237259]
loss: 0.011587 [206848/1237259]
loss: 0.012730 [411648/1237259]
loss: 0.013325 [616448/1237259]
loss: 0.011490 [821248/1237259]
loss: 0.011308 [1026048/1237259]
loss: 0.014048 [1230848/1237259]
Epoch 600
-------------------------------
loss: 0.012143 [ 2048/1237259]
loss: 0.012386 [206848/1237259]
loss: 0.012034 [411648/1237259]
loss: 0.012308 [616448/1237259]
loss: 0.012474 [821248/1237259]
loss: 0.012986 [1026048/1237259]
loss: 0.010867 [1230848/1237259]
Eval results: 
recall@20: 0.0625  
ndcg@20: 0.0505  
diversity: 0.1838  


Epoch 601
-------------------------------
loss: 0.011200 [ 2048/1237259]
loss: 0.012784 [206848/1237259]
loss: 0.012872 [411648/1237259]
loss: 0.012919 [616448/1237259]
loss: 0.012170 [821248/1237259]
loss: 0.013401 [1026048/1237259]
loss: 0.011811 [1230848/1237259]
Epoch 602
-------------------------------
loss: 0.014075 [ 2048/1237259]
loss: 0.012438 [206848/1237259]
loss: 0.012802 [411648/1237259]
loss: 0.012222 [616448/1237259]
loss: 0.012994 [821248/1237259]
loss: 0.011952 [1026048/1237259]
loss: 0.012129 [1230848/1237259]
Epoch 603
-------------------------------
loss: 0.011994 [ 2048/1237259]
loss: 0.012196 [206848/1237259]
loss: 0.011705 [411648/1237259]
loss: 0.011284 [616448/1237259]
loss: 0.012119 [821248/1237259]
loss: 0.011606 [1026048/1237259]
loss: 0.012777 [1230848/1237259]
Epoch 604
-------------------------------
loss: 0.011975 [ 2048/1237259]
loss: 0.012618 [206848/1237259]
loss: 0.014032 [411648/1237259]
loss: 0.012215 [616448/1237259]
loss: 0.012249 [821248/1237259]
loss: 0.012419 [1026048/1237259]
loss: 0.012418 [1230848/1237259]
Epoch 605
-------------------------------
loss: 0.012355 [ 2048/1237259]
loss: 0.012310 [206848/1237259]
loss: 0.011976 [411648/1237259]
loss: 0.011743 [616448/1237259]
loss: 0.012884 [821248/1237259]
loss: 0.011816 [1026048/1237259]
loss: 0.013421 [1230848/1237259]
Eval results: 
recall@20: 0.0624  
ndcg@20: 0.0504  
diversity: 0.1838  


Epoch 606
-------------------------------
loss: 0.012972 [ 2048/1237259]
loss: 0.011433 [206848/1237259]
loss: 0.012563 [411648/1237259]
loss: 0.012509 [616448/1237259]
loss: 0.011188 [821248/1237259]
loss: 0.013767 [1026048/1237259]
loss: 0.013705 [1230848/1237259]
Epoch 607
-------------------------------
loss: 0.012830 [ 2048/1237259]
loss: 0.013031 [206848/1237259]
loss: 0.013360 [411648/1237259]
loss: 0.012799 [616448/1237259]
loss: 0.011828 [821248/1237259]
loss: 0.012859 [1026048/1237259]
loss: 0.012668 [1230848/1237259]
Epoch 608
-------------------------------
loss: 0.012798 [ 2048/1237259]
loss: 0.012061 [206848/1237259]
loss: 0.012689 [411648/1237259]
loss: 0.013042 [616448/1237259]
loss: 0.012395 [821248/1237259]
loss: 0.012558 [1026048/1237259]
loss: 0.013436 [1230848/1237259]
Epoch 609
-------------------------------
loss: 0.015821 [ 2048/1237259]
loss: 0.012190 [206848/1237259]
loss: 0.012571 [411648/1237259]
loss: 0.012124 [616448/1237259]
loss: 0.011937 [821248/1237259]
loss: 0.012688 [1026048/1237259]
loss: 0.011864 [1230848/1237259]
Epoch 610
-------------------------------
loss: 0.012207 [ 2048/1237259]
loss: 0.013305 [206848/1237259]
loss: 0.012758 [411648/1237259]
loss: 0.012880 [616448/1237259]
loss: 0.011926 [821248/1237259]
loss: 0.011992 [1026048/1237259]
loss: 0.012908 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0624  
ndcg@20: 0.0506  
diversity: 0.1841  


Epoch 611
-------------------------------
loss: 0.013236 [ 2048/1237259]
loss: 0.012999 [206848/1237259]
loss: 0.012363 [411648/1237259]
loss: 0.011091 [616448/1237259]
loss: 0.011240 [821248/1237259]
loss: 0.013145 [1026048/1237259]
loss: 0.012685 [1230848/1237259]
Epoch 612
-------------------------------
loss: 0.012415 [ 2048/1237259]
loss: 0.011572 [206848/1237259]
loss: 0.013459 [411648/1237259]
loss: 0.012833 [616448/1237259]
loss: 0.012677 [821248/1237259]
loss: 0.013761 [1026048/1237259]
loss: 0.012023 [1230848/1237259]
Epoch 613
-------------------------------
loss: 0.012341 [ 2048/1237259]
loss: 0.011719 [206848/1237259]
loss: 0.011217 [411648/1237259]
loss: 0.011380 [616448/1237259]
loss: 0.011232 [821248/1237259]
loss: 0.012431 [1026048/1237259]
loss: 0.011828 [1230848/1237259]
Epoch 614
-------------------------------
loss: 0.013707 [ 2048/1237259]
loss: 0.013173 [206848/1237259]
loss: 0.012894 [411648/1237259]
loss: 0.011999 [616448/1237259]
loss: 0.011640 [821248/1237259]
loss: 0.011964 [1026048/1237259]
loss: 0.012457 [1230848/1237259]
Epoch 615
-------------------------------
loss: 0.013236 [ 2048/1237259]
loss: 0.012090 [206848/1237259]
loss: 0.012524 [411648/1237259]
loss: 0.013048 [616448/1237259]
loss: 0.011862 [821248/1237259]
loss: 0.012695 [1026048/1237259]
loss: 0.013324 [1230848/1237259]
Eval results: 
recall@20: 0.0626  
ndcg@20: 0.0507  
diversity: 0.1840  


Epoch 616
-------------------------------
loss: 0.011606 [ 2048/1237259]
loss: 0.011178 [206848/1237259]
loss: 0.012674 [411648/1237259]
loss: 0.012985 [616448/1237259]
loss: 0.013130 [821248/1237259]
loss: 0.012549 [1026048/1237259]
loss: 0.011666 [1230848/1237259]
Epoch 617
-------------------------------
loss: 0.012428 [ 2048/1237259]
loss: 0.012740 [206848/1237259]
loss: 0.012048 [411648/1237259]
loss: 0.012890 [616448/1237259]
loss: 0.013037 [821248/1237259]
loss: 0.013461 [1026048/1237259]
loss: 0.012103 [1230848/1237259]
Epoch 618
-------------------------------
loss: 0.013532 [ 2048/1237259]
loss: 0.012216 [206848/1237259]
loss: 0.013477 [411648/1237259]
loss: 0.013728 [616448/1237259]
loss: 0.013612 [821248/1237259]
loss: 0.011808 [1026048/1237259]
loss: 0.012620 [1230848/1237259]
Epoch 619
-------------------------------
loss: 0.012200 [ 2048/1237259]
loss: 0.013913 [206848/1237259]
loss: 0.012675 [411648/1237259]
loss: 0.012336 [616448/1237259]
loss: 0.012689 [821248/1237259]
loss: 0.012951 [1026048/1237259]
loss: 0.011767 [1230848/1237259]
Epoch 620
-------------------------------
loss: 0.013822 [ 2048/1237259]
loss: 0.012739 [206848/1237259]
loss: 0.011769 [411648/1237259]
loss: 0.013789 [616448/1237259]
loss: 0.012397 [821248/1237259]
loss: 0.012073 [1026048/1237259]
loss: 0.013321 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0627  
ndcg@20: 0.0507  
diversity: 0.1842  


Epoch 621
-------------------------------
loss: 0.013241 [ 2048/1237259]
loss: 0.012090 [206848/1237259]
loss: 0.012578 [411648/1237259]
loss: 0.012821 [616448/1237259]
loss: 0.012909 [821248/1237259]
loss: 0.012774 [1026048/1237259]
loss: 0.012112 [1230848/1237259]
Epoch 622
-------------------------------
loss: 0.012664 [ 2048/1237259]
loss: 0.012359 [206848/1237259]
loss: 0.012355 [411648/1237259]
loss: 0.013193 [616448/1237259]
loss: 0.012063 [821248/1237259]
loss: 0.012431 [1026048/1237259]
loss: 0.011093 [1230848/1237259]
Epoch 623
-------------------------------
loss: 0.010846 [ 2048/1237259]
loss: 0.011836 [206848/1237259]
loss: 0.011953 [411648/1237259]
loss: 0.013585 [616448/1237259]
loss: 0.012650 [821248/1237259]
loss: 0.012511 [1026048/1237259]
loss: 0.012403 [1230848/1237259]
Epoch 624
-------------------------------
loss: 0.013102 [ 2048/1237259]
loss: 0.015861 [206848/1237259]
loss: 0.012815 [411648/1237259]
loss: 0.012391 [616448/1237259]
loss: 0.011535 [821248/1237259]
loss: 0.011431 [1026048/1237259]
loss: 0.013126 [1230848/1237259]
Epoch 625
-------------------------------
loss: 0.012270 [ 2048/1237259]
loss: 0.013379 [206848/1237259]
loss: 0.013423 [411648/1237259]
loss: 0.012633 [616448/1237259]
loss: 0.013656 [821248/1237259]
loss: 0.010951 [1026048/1237259]
loss: 0.013166 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0631  
ndcg@20: 0.0509  
diversity: 0.1841  


Epoch 626
-------------------------------
loss: 0.011697 [ 2048/1237259]
loss: 0.011523 [206848/1237259]
loss: 0.012097 [411648/1237259]
loss: 0.013075 [616448/1237259]
loss: 0.013453 [821248/1237259]
loss: 0.012711 [1026048/1237259]
loss: 0.012587 [1230848/1237259]
Epoch 627
-------------------------------
loss: 0.012832 [ 2048/1237259]
loss: 0.012521 [206848/1237259]
loss: 0.013018 [411648/1237259]
loss: 0.012996 [616448/1237259]
loss: 0.011696 [821248/1237259]
loss: 0.012067 [1026048/1237259]
loss: 0.013416 [1230848/1237259]
Epoch 628
-------------------------------
loss: 0.012274 [ 2048/1237259]
loss: 0.012872 [206848/1237259]
loss: 0.011803 [411648/1237259]
loss: 0.011755 [616448/1237259]
loss: 0.012071 [821248/1237259]
loss: 0.011892 [1026048/1237259]
loss: 0.013742 [1230848/1237259]
Epoch 629
-------------------------------
loss: 0.012805 [ 2048/1237259]
loss: 0.012331 [206848/1237259]
loss: 0.013179 [411648/1237259]
loss: 0.012197 [616448/1237259]
loss: 0.015353 [821248/1237259]
loss: 0.012693 [1026048/1237259]
loss: 0.013733 [1230848/1237259]
Epoch 630
-------------------------------
loss: 0.012007 [ 2048/1237259]
loss: 0.012278 [206848/1237259]
loss: 0.013500 [411648/1237259]
loss: 0.012696 [616448/1237259]
loss: 0.012039 [821248/1237259]
loss: 0.013106 [1026048/1237259]
loss: 0.012599 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0510  
diversity: 0.1841  


Epoch 631
-------------------------------
loss: 0.012041 [ 2048/1237259]
loss: 0.012187 [206848/1237259]
loss: 0.012816 [411648/1237259]
loss: 0.013125 [616448/1237259]
loss: 0.013227 [821248/1237259]
loss: 0.012536 [1026048/1237259]
loss: 0.011622 [1230848/1237259]
Epoch 632
-------------------------------
loss: 0.011612 [ 2048/1237259]
loss: 0.010920 [206848/1237259]
loss: 0.012254 [411648/1237259]
loss: 0.013039 [616448/1237259]
loss: 0.013033 [821248/1237259]
loss: 0.011849 [1026048/1237259]
loss: 0.013330 [1230848/1237259]
Epoch 633
-------------------------------
loss: 0.011733 [ 2048/1237259]
loss: 0.012329 [206848/1237259]
loss: 0.013068 [411648/1237259]
loss: 0.011606 [616448/1237259]
loss: 0.014529 [821248/1237259]
loss: 0.011363 [1026048/1237259]
loss: 0.013989 [1230848/1237259]
Epoch 634
-------------------------------
loss: 0.013099 [ 2048/1237259]
loss: 0.012240 [206848/1237259]
loss: 0.012158 [411648/1237259]
loss: 0.013008 [616448/1237259]
loss: 0.012066 [821248/1237259]
loss: 0.011013 [1026048/1237259]
loss: 0.012425 [1230848/1237259]
Epoch 635
-------------------------------
loss: 0.013016 [ 2048/1237259]
loss: 0.012057 [206848/1237259]
loss: 0.012416 [411648/1237259]
loss: 0.012715 [616448/1237259]
loss: 0.011635 [821248/1237259]
loss: 0.013304 [1026048/1237259]
loss: 0.012955 [1230848/1237259]
Eval results: 
recall@20: 0.0626  
ndcg@20: 0.0509  
diversity: 0.1839  


Epoch 636
-------------------------------
loss: 0.012838 [ 2048/1237259]
loss: 0.011480 [206848/1237259]
loss: 0.012342 [411648/1237259]
loss: 0.012345 [616448/1237259]
loss: 0.012345 [821248/1237259]
loss: 0.012231 [1026048/1237259]
loss: 0.011911 [1230848/1237259]
Epoch 637
-------------------------------
loss: 0.013243 [ 2048/1237259]
loss: 0.012945 [206848/1237259]
loss: 0.012820 [411648/1237259]
loss: 0.012544 [616448/1237259]
loss: 0.012853 [821248/1237259]
loss: 0.012560 [1026048/1237259]
loss: 0.013197 [1230848/1237259]
Epoch 638
-------------------------------
loss: 0.011312 [ 2048/1237259]
loss: 0.013427 [206848/1237259]
loss: 0.011511 [411648/1237259]
loss: 0.012051 [616448/1237259]
loss: 0.012207 [821248/1237259]
loss: 0.014179 [1026048/1237259]
loss: 0.013609 [1230848/1237259]
Epoch 639
-------------------------------
loss: 0.011213 [ 2048/1237259]
loss: 0.013391 [206848/1237259]
loss: 0.011973 [411648/1237259]
loss: 0.012769 [616448/1237259]
loss: 0.013746 [821248/1237259]
loss: 0.013078 [1026048/1237259]
loss: 0.013752 [1230848/1237259]
Epoch 640
-------------------------------
loss: 0.012241 [ 2048/1237259]
loss: 0.012126 [206848/1237259]
loss: 0.012626 [411648/1237259]
loss: 0.012298 [616448/1237259]
loss: 0.013122 [821248/1237259]
loss: 0.012601 [1026048/1237259]
loss: 0.011419 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0510  
diversity: 0.1841  


Epoch 641
-------------------------------
loss: 0.013832 [ 2048/1237259]
loss: 0.012966 [206848/1237259]
loss: 0.013815 [411648/1237259]
loss: 0.012186 [616448/1237259]
loss: 0.012111 [821248/1237259]
loss: 0.012508 [1026048/1237259]
loss: 0.011800 [1230848/1237259]
Epoch 642
-------------------------------
loss: 0.012858 [ 2048/1237259]
loss: 0.011291 [206848/1237259]
loss: 0.012597 [411648/1237259]
loss: 0.013459 [616448/1237259]
loss: 0.011495 [821248/1237259]
loss: 0.013758 [1026048/1237259]
loss: 0.012491 [1230848/1237259]
Epoch 643
-------------------------------
loss: 0.012182 [ 2048/1237259]
loss: 0.013850 [206848/1237259]
loss: 0.012402 [411648/1237259]
loss: 0.012660 [616448/1237259]
loss: 0.011169 [821248/1237259]
loss: 0.011637 [1026048/1237259]
loss: 0.011714 [1230848/1237259]
Epoch 644
-------------------------------
loss: 0.013592 [ 2048/1237259]
loss: 0.012374 [206848/1237259]
loss: 0.012258 [411648/1237259]
loss: 0.012160 [616448/1237259]
loss: 0.012279 [821248/1237259]
loss: 0.011881 [1026048/1237259]
loss: 0.012720 [1230848/1237259]
Epoch 645
-------------------------------
loss: 0.012019 [ 2048/1237259]
loss: 0.010808 [206848/1237259]
loss: 0.011037 [411648/1237259]
loss: 0.011729 [616448/1237259]
loss: 0.011886 [821248/1237259]
loss: 0.011761 [1026048/1237259]
loss: 0.011720 [1230848/1237259]
Eval results: 
recall@20: 0.0627  
ndcg@20: 0.0508  
diversity: 0.1840  


Epoch 646
-------------------------------
loss: 0.013513 [ 2048/1237259]
loss: 0.011508 [206848/1237259]
loss: 0.012428 [411648/1237259]
loss: 0.012555 [616448/1237259]
loss: 0.013017 [821248/1237259]
loss: 0.012700 [1026048/1237259]
loss: 0.012965 [1230848/1237259]
Epoch 647
-------------------------------
loss: 0.014079 [ 2048/1237259]
loss: 0.012410 [206848/1237259]
loss: 0.012823 [411648/1237259]
loss: 0.011886 [616448/1237259]
loss: 0.012824 [821248/1237259]
loss: 0.012672 [1026048/1237259]
loss: 0.012413 [1230848/1237259]
Epoch 648
-------------------------------
loss: 0.012694 [ 2048/1237259]
loss: 0.012451 [206848/1237259]
loss: 0.012307 [411648/1237259]
loss: 0.012068 [616448/1237259]
loss: 0.011659 [821248/1237259]
loss: 0.013102 [1026048/1237259]
loss: 0.013898 [1230848/1237259]
Epoch 649
-------------------------------
loss: 0.012622 [ 2048/1237259]
loss: 0.011511 [206848/1237259]
loss: 0.012586 [411648/1237259]
loss: 0.012566 [616448/1237259]
loss: 0.011402 [821248/1237259]
loss: 0.013649 [1026048/1237259]
loss: 0.012812 [1230848/1237259]
Epoch 650
-------------------------------
loss: 0.012576 [ 2048/1237259]
loss: 0.012059 [206848/1237259]
loss: 0.012649 [411648/1237259]
loss: 0.013076 [616448/1237259]
loss: 0.013415 [821248/1237259]
loss: 0.011832 [1026048/1237259]
loss: 0.013227 [1230848/1237259]
Eval results: 
recall@20: 0.0625  
ndcg@20: 0.0506  
diversity: 0.1840  


Epoch 651
-------------------------------
loss: 0.011526 [ 2048/1237259]
loss: 0.013594 [206848/1237259]
loss: 0.011433 [411648/1237259]
loss: 0.013609 [616448/1237259]
loss: 0.013431 [821248/1237259]
loss: 0.012023 [1026048/1237259]
loss: 0.013463 [1230848/1237259]
Epoch 652
-------------------------------
loss: 0.013469 [ 2048/1237259]
loss: 0.011268 [206848/1237259]
loss: 0.011973 [411648/1237259]
loss: 0.011290 [616448/1237259]
loss: 0.012122 [821248/1237259]
loss: 0.011073 [1026048/1237259]
loss: 0.012364 [1230848/1237259]
Epoch 653
-------------------------------
loss: 0.012996 [ 2048/1237259]
loss: 0.012550 [206848/1237259]
loss: 0.012442 [411648/1237259]
loss: 0.012683 [616448/1237259]
loss: 0.012587 [821248/1237259]
loss: 0.011707 [1026048/1237259]
loss: 0.011629 [1230848/1237259]
Epoch 654
-------------------------------
loss: 0.012638 [ 2048/1237259]
loss: 0.012932 [206848/1237259]
loss: 0.011301 [411648/1237259]
loss: 0.012455 [616448/1237259]
loss: 0.012704 [821248/1237259]
loss: 0.012504 [1026048/1237259]
loss: 0.011655 [1230848/1237259]
Epoch 655
-------------------------------
loss: 0.011061 [ 2048/1237259]
loss: 0.011750 [206848/1237259]
loss: 0.011299 [411648/1237259]
loss: 0.013472 [616448/1237259]
loss: 0.012338 [821248/1237259]
loss: 0.012648 [1026048/1237259]
loss: 0.012947 [1230848/1237259]
Eval results: 
recall@20: 0.0627  
ndcg@20: 0.0509  
diversity: 0.1839  


Epoch 656
-------------------------------
loss: 0.012715 [ 2048/1237259]
loss: 0.012301 [206848/1237259]
loss: 0.012684 [411648/1237259]
loss: 0.011260 [616448/1237259]
loss: 0.013337 [821248/1237259]
loss: 0.013009 [1026048/1237259]
loss: 0.010573 [1230848/1237259]
Epoch 657
-------------------------------
loss: 0.013349 [ 2048/1237259]
loss: 0.012767 [206848/1237259]
loss: 0.013423 [411648/1237259]
loss: 0.011589 [616448/1237259]
loss: 0.013783 [821248/1237259]
loss: 0.012319 [1026048/1237259]
loss: 0.011991 [1230848/1237259]
Epoch 658
-------------------------------
loss: 0.011500 [ 2048/1237259]
loss: 0.013012 [206848/1237259]
loss: 0.012322 [411648/1237259]
loss: 0.012623 [616448/1237259]
loss: 0.012664 [821248/1237259]
loss: 0.013177 [1026048/1237259]
loss: 0.012468 [1230848/1237259]
Epoch 659
-------------------------------
loss: 0.011924 [ 2048/1237259]
loss: 0.013016 [206848/1237259]
loss: 0.014935 [411648/1237259]
loss: 0.014509 [616448/1237259]
loss: 0.012798 [821248/1237259]
loss: 0.013825 [1026048/1237259]
loss: 0.012154 [1230848/1237259]
Epoch 660
-------------------------------
loss: 0.010924 [ 2048/1237259]
loss: 0.013452 [206848/1237259]
loss: 0.011253 [411648/1237259]
loss: 0.011519 [616448/1237259]
loss: 0.010551 [821248/1237259]
loss: 0.013074 [1026048/1237259]
loss: 0.012011 [1230848/1237259]
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0510  
diversity: 0.1841  


Epoch 661
-------------------------------
loss: 0.012644 [ 2048/1237259]
loss: 0.013338 [206848/1237259]
loss: 0.014132 [411648/1237259]
loss: 0.012579 [616448/1237259]
loss: 0.011975 [821248/1237259]
loss: 0.012609 [1026048/1237259]
loss: 0.013502 [1230848/1237259]
Epoch 662
-------------------------------
loss: 0.013403 [ 2048/1237259]
loss: 0.012726 [206848/1237259]
loss: 0.011413 [411648/1237259]
loss: 0.012058 [616448/1237259]
loss: 0.011808 [821248/1237259]
loss: 0.012805 [1026048/1237259]
loss: 0.012179 [1230848/1237259]
Epoch 663
-------------------------------
loss: 0.012488 [ 2048/1237259]
loss: 0.013085 [206848/1237259]
loss: 0.012827 [411648/1237259]
loss: 0.013361 [616448/1237259]
loss: 0.011623 [821248/1237259]
loss: 0.011367 [1026048/1237259]
loss: 0.013602 [1230848/1237259]
Epoch 664
-------------------------------
loss: 0.013685 [ 2048/1237259]
loss: 0.012581 [206848/1237259]
loss: 0.011953 [411648/1237259]
loss: 0.013239 [616448/1237259]
loss: 0.013829 [821248/1237259]
loss: 0.013369 [1026048/1237259]
loss: 0.012841 [1230848/1237259]
Epoch 665
-------------------------------
loss: 0.012743 [ 2048/1237259]
loss: 0.012365 [206848/1237259]
loss: 0.012251 [411648/1237259]
loss: 0.012313 [616448/1237259]
loss: 0.013309 [821248/1237259]
loss: 0.011388 [1026048/1237259]
loss: 0.011914 [1230848/1237259]
Eval results: 
recall@20: 0.0627  
ndcg@20: 0.0509  
diversity: 0.1839  


Epoch 666
-------------------------------
loss: 0.012473 [ 2048/1237259]
loss: 0.010627 [206848/1237259]
loss: 0.013233 [411648/1237259]
loss: 0.012848 [616448/1237259]
loss: 0.012173 [821248/1237259]
loss: 0.012932 [1026048/1237259]
loss: 0.012824 [1230848/1237259]
Epoch 667
-------------------------------
loss: 0.013019 [ 2048/1237259]
loss: 0.012621 [206848/1237259]
loss: 0.010629 [411648/1237259]
loss: 0.012231 [616448/1237259]
loss: 0.012233 [821248/1237259]
loss: 0.012117 [1026048/1237259]
loss: 0.012491 [1230848/1237259]
Epoch 668
-------------------------------
loss: 0.012388 [ 2048/1237259]
loss: 0.012617 [206848/1237259]
loss: 0.012322 [411648/1237259]
loss: 0.015157 [616448/1237259]
loss: 0.013230 [821248/1237259]
loss: 0.012219 [1026048/1237259]
loss: 0.014030 [1230848/1237259]
Epoch 669
-------------------------------
loss: 0.012374 [ 2048/1237259]
loss: 0.011965 [206848/1237259]
loss: 0.011667 [411648/1237259]
loss: 0.012815 [616448/1237259]
loss: 0.011531 [821248/1237259]
loss: 0.013978 [1026048/1237259]
loss: 0.011861 [1230848/1237259]
Epoch 670
-------------------------------
loss: 0.011529 [ 2048/1237259]
loss: 0.012420 [206848/1237259]
loss: 0.012049 [411648/1237259]
loss: 0.012354 [616448/1237259]
loss: 0.012669 [821248/1237259]
loss: 0.011372 [1026048/1237259]
loss: 0.011505 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0510  
diversity: 0.1842  


Epoch 671
-------------------------------
loss: 0.012221 [ 2048/1237259]
loss: 0.012725 [206848/1237259]
loss: 0.013641 [411648/1237259]
loss: 0.013554 [616448/1237259]
loss: 0.012695 [821248/1237259]
loss: 0.011603 [1026048/1237259]
loss: 0.013699 [1230848/1237259]
Epoch 672
-------------------------------
loss: 0.012104 [ 2048/1237259]
loss: 0.012604 [206848/1237259]
loss: 0.010843 [411648/1237259]
loss: 0.011989 [616448/1237259]
loss: 0.011856 [821248/1237259]
loss: 0.012526 [1026048/1237259]
loss: 0.012174 [1230848/1237259]
Epoch 673
-------------------------------
loss: 0.013431 [ 2048/1237259]
loss: 0.011216 [206848/1237259]
loss: 0.012414 [411648/1237259]
loss: 0.013663 [616448/1237259]
loss: 0.012012 [821248/1237259]
loss: 0.011404 [1026048/1237259]
loss: 0.011388 [1230848/1237259]
Epoch 674
-------------------------------
loss: 0.011856 [ 2048/1237259]
loss: 0.013255 [206848/1237259]
loss: 0.012799 [411648/1237259]
loss: 0.012311 [616448/1237259]
loss: 0.012510 [821248/1237259]
loss: 0.012228 [1026048/1237259]
loss: 0.012391 [1230848/1237259]
Epoch 675
-------------------------------
loss: 0.012811 [ 2048/1237259]
loss: 0.013487 [206848/1237259]
loss: 0.011774 [411648/1237259]
loss: 0.011837 [616448/1237259]
loss: 0.013081 [821248/1237259]
loss: 0.013004 [1026048/1237259]
loss: 0.011671 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0509  
diversity: 0.1840  


Epoch 676
-------------------------------
loss: 0.013953 [ 2048/1237259]
loss: 0.011868 [206848/1237259]
loss: 0.012750 [411648/1237259]
loss: 0.012623 [616448/1237259]
loss: 0.012018 [821248/1237259]
loss: 0.012692 [1026048/1237259]
loss: 0.012458 [1230848/1237259]
Epoch 677
-------------------------------
loss: 0.011931 [ 2048/1237259]
loss: 0.013324 [206848/1237259]
loss: 0.011246 [411648/1237259]
loss: 0.013204 [616448/1237259]
loss: 0.012295 [821248/1237259]
loss: 0.013032 [1026048/1237259]
loss: 0.013705 [1230848/1237259]
Epoch 678
-------------------------------
loss: 0.012708 [ 2048/1237259]
loss: 0.013916 [206848/1237259]
loss: 0.012071 [411648/1237259]
loss: 0.012176 [616448/1237259]
loss: 0.011766 [821248/1237259]
loss: 0.013469 [1026048/1237259]
loss: 0.012354 [1230848/1237259]
Epoch 679
-------------------------------
loss: 0.012571 [ 2048/1237259]
loss: 0.012528 [206848/1237259]
loss: 0.012473 [411648/1237259]
loss: 0.011348 [616448/1237259]
loss: 0.011789 [821248/1237259]
loss: 0.011827 [1026048/1237259]
loss: 0.012534 [1230848/1237259]
Epoch 680
-------------------------------
loss: 0.011120 [ 2048/1237259]
loss: 0.013855 [206848/1237259]
loss: 0.012228 [411648/1237259]
loss: 0.012936 [616448/1237259]
loss: 0.012111 [821248/1237259]
loss: 0.012280 [1026048/1237259]
loss: 0.013526 [1230848/1237259]
Eval results: 
recall@20: 0.0628  
ndcg@20: 0.0508  
diversity: 0.1839  


Epoch 681
-------------------------------
loss: 0.012521 [ 2048/1237259]
loss: 0.012137 [206848/1237259]
loss: 0.011607 [411648/1237259]
loss: 0.012996 [616448/1237259]
loss: 0.011733 [821248/1237259]
loss: 0.011442 [1026048/1237259]
loss: 0.011963 [1230848/1237259]
Epoch 682
-------------------------------
loss: 0.012742 [ 2048/1237259]
loss: 0.012889 [206848/1237259]
loss: 0.011656 [411648/1237259]
loss: 0.013789 [616448/1237259]
loss: 0.013331 [821248/1237259]
loss: 0.011514 [1026048/1237259]
loss: 0.013295 [1230848/1237259]
Epoch 683
-------------------------------
loss: 0.013912 [ 2048/1237259]
loss: 0.013485 [206848/1237259]
loss: 0.013071 [411648/1237259]
loss: 0.013143 [616448/1237259]
loss: 0.012738 [821248/1237259]
loss: 0.012917 [1026048/1237259]
loss: 0.012573 [1230848/1237259]
Epoch 684
-------------------------------
loss: 0.013157 [ 2048/1237259]
loss: 0.010694 [206848/1237259]
loss: 0.012038 [411648/1237259]
loss: 0.011497 [616448/1237259]
loss: 0.013512 [821248/1237259]
loss: 0.011649 [1026048/1237259]
loss: 0.011711 [1230848/1237259]
Epoch 685
-------------------------------
loss: 0.012636 [ 2048/1237259]
loss: 0.012157 [206848/1237259]
loss: 0.011909 [411648/1237259]
loss: 0.013104 [616448/1237259]
loss: 0.012914 [821248/1237259]
loss: 0.013441 [1026048/1237259]
loss: 0.012508 [1230848/1237259]
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0509  
diversity: 0.1840  


Epoch 686
-------------------------------
loss: 0.012001 [ 2048/1237259]
loss: 0.014557 [206848/1237259]
loss: 0.013343 [411648/1237259]
loss: 0.012862 [616448/1237259]
loss: 0.013112 [821248/1237259]
loss: 0.010959 [1026048/1237259]
loss: 0.012874 [1230848/1237259]
Epoch 687
-------------------------------
loss: 0.013314 [ 2048/1237259]
loss: 0.014072 [206848/1237259]
loss: 0.011767 [411648/1237259]
loss: 0.011684 [616448/1237259]
loss: 0.012950 [821248/1237259]
loss: 0.012253 [1026048/1237259]
loss: 0.012187 [1230848/1237259]
Epoch 688
-------------------------------
loss: 0.011253 [ 2048/1237259]
loss: 0.013710 [206848/1237259]
loss: 0.011728 [411648/1237259]
loss: 0.012967 [616448/1237259]
loss: 0.012207 [821248/1237259]
loss: 0.012119 [1026048/1237259]
loss: 0.013409 [1230848/1237259]
Epoch 689
-------------------------------
loss: 0.011245 [ 2048/1237259]
loss: 0.011196 [206848/1237259]
loss: 0.011881 [411648/1237259]
loss: 0.011822 [616448/1237259]
loss: 0.011930 [821248/1237259]
loss: 0.012268 [1026048/1237259]
loss: 0.013711 [1230848/1237259]
Epoch 690
-------------------------------
loss: 0.012137 [ 2048/1237259]
loss: 0.012243 [206848/1237259]
loss: 0.013449 [411648/1237259]
loss: 0.013022 [616448/1237259]
loss: 0.012962 [821248/1237259]
loss: 0.013371 [1026048/1237259]
loss: 0.013468 [1230848/1237259]
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0510  
diversity: 0.1840  


Epoch 691
-------------------------------
loss: 0.011542 [ 2048/1237259]
loss: 0.012171 [206848/1237259]
loss: 0.012795 [411648/1237259]
loss: 0.011384 [616448/1237259]
loss: 0.013838 [821248/1237259]
loss: 0.013626 [1026048/1237259]
loss: 0.013274 [1230848/1237259]
Epoch 692
-------------------------------
loss: 0.011187 [ 2048/1237259]
loss: 0.012600 [206848/1237259]
loss: 0.012595 [411648/1237259]
loss: 0.012732 [616448/1237259]
loss: 0.012620 [821248/1237259]
loss: 0.012109 [1026048/1237259]
loss: 0.012749 [1230848/1237259]
Epoch 693
-------------------------------
loss: 0.013347 [ 2048/1237259]
loss: 0.011308 [206848/1237259]
loss: 0.013368 [411648/1237259]
loss: 0.012347 [616448/1237259]
loss: 0.012499 [821248/1237259]
loss: 0.012716 [1026048/1237259]
loss: 0.013933 [1230848/1237259]
Epoch 694
-------------------------------
loss: 0.011549 [ 2048/1237259]
loss: 0.013093 [206848/1237259]
loss: 0.014868 [411648/1237259]
loss: 0.013183 [616448/1237259]
loss: 0.013169 [821248/1237259]
loss: 0.012821 [1026048/1237259]
loss: 0.012769 [1230848/1237259]
Epoch 695
-------------------------------
loss: 0.012438 [ 2048/1237259]
loss: 0.013210 [206848/1237259]
loss: 0.012561 [411648/1237259]
loss: 0.012746 [616448/1237259]
loss: 0.011753 [821248/1237259]
loss: 0.011696 [1026048/1237259]
loss: 0.011509 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0510  
diversity: 0.1841  


Epoch 696
-------------------------------
loss: 0.013072 [ 2048/1237259]
loss: 0.012431 [206848/1237259]
loss: 0.012761 [411648/1237259]
loss: 0.011834 [616448/1237259]
loss: 0.012445 [821248/1237259]
loss: 0.013328 [1026048/1237259]
loss: 0.012679 [1230848/1237259]
Epoch 697
-------------------------------
loss: 0.012406 [ 2048/1237259]
loss: 0.012349 [206848/1237259]
loss: 0.011649 [411648/1237259]
loss: 0.012793 [616448/1237259]
loss: 0.012370 [821248/1237259]
loss: 0.012746 [1026048/1237259]
loss: 0.013385 [1230848/1237259]
Epoch 698
-------------------------------
loss: 0.011908 [ 2048/1237259]
loss: 0.012009 [206848/1237259]
loss: 0.014520 [411648/1237259]
loss: 0.013329 [616448/1237259]
loss: 0.013495 [821248/1237259]
loss: 0.011825 [1026048/1237259]
loss: 0.013505 [1230848/1237259]
Epoch 699
-------------------------------
loss: 0.014287 [ 2048/1237259]
loss: 0.011701 [206848/1237259]
loss: 0.011979 [411648/1237259]
loss: 0.011137 [616448/1237259]
loss: 0.012149 [821248/1237259]
loss: 0.014144 [1026048/1237259]
loss: 0.012722 [1230848/1237259]
Epoch 700
-------------------------------
loss: 0.012218 [ 2048/1237259]
loss: 0.012262 [206848/1237259]
loss: 0.013197 [411648/1237259]
loss: 0.011890 [616448/1237259]
loss: 0.011777 [821248/1237259]
loss: 0.012968 [1026048/1237259]
loss: 0.012554 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0509  
diversity: 0.1840  


Epoch 701
-------------------------------
loss: 0.011971 [ 2048/1237259]
loss: 0.012038 [206848/1237259]
loss: 0.012083 [411648/1237259]
loss: 0.014410 [616448/1237259]
loss: 0.012583 [821248/1237259]
loss: 0.014175 [1026048/1237259]
loss: 0.014557 [1230848/1237259]
Epoch 702
-------------------------------
loss: 0.011883 [ 2048/1237259]
loss: 0.011419 [206848/1237259]
loss: 0.012833 [411648/1237259]
loss: 0.013000 [616448/1237259]
loss: 0.012823 [821248/1237259]
loss: 0.013037 [1026048/1237259]
loss: 0.013231 [1230848/1237259]
Epoch 703
-------------------------------
loss: 0.011879 [ 2048/1237259]
loss: 0.013775 [206848/1237259]
loss: 0.011750 [411648/1237259]
loss: 0.012130 [616448/1237259]
loss: 0.012503 [821248/1237259]
loss: 0.012575 [1026048/1237259]
loss: 0.012684 [1230848/1237259]
Epoch 704
-------------------------------
loss: 0.011939 [ 2048/1237259]
loss: 0.013181 [206848/1237259]
loss: 0.013845 [411648/1237259]
loss: 0.012191 [616448/1237259]
loss: 0.012534 [821248/1237259]
loss: 0.013008 [1026048/1237259]
loss: 0.011278 [1230848/1237259]
Epoch 705
-------------------------------
loss: 0.012851 [ 2048/1237259]
loss: 0.011404 [206848/1237259]
loss: 0.012999 [411648/1237259]
loss: 0.013066 [616448/1237259]
loss: 0.012945 [821248/1237259]
loss: 0.012456 [1026048/1237259]
loss: 0.014215 [1230848/1237259]
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0509  
diversity: 0.1840  


Epoch 706
-------------------------------
loss: 0.013187 [ 2048/1237259]
loss: 0.011856 [206848/1237259]
loss: 0.012043 [411648/1237259]
loss: 0.013785 [616448/1237259]
loss: 0.011544 [821248/1237259]
loss: 0.012149 [1026048/1237259]
loss: 0.012726 [1230848/1237259]
Epoch 707
-------------------------------
loss: 0.013132 [ 2048/1237259]
loss: 0.012794 [206848/1237259]
loss: 0.011868 [411648/1237259]
loss: 0.013154 [616448/1237259]
loss: 0.012409 [821248/1237259]
loss: 0.012364 [1026048/1237259]
loss: 0.012443 [1230848/1237259]
Epoch 708
-------------------------------
loss: 0.012005 [ 2048/1237259]
loss: 0.010901 [206848/1237259]
loss: 0.012689 [411648/1237259]
loss: 0.012472 [616448/1237259]
loss: 0.012344 [821248/1237259]
loss: 0.013033 [1026048/1237259]
loss: 0.012822 [1230848/1237259]
Epoch 709
-------------------------------
loss: 0.013623 [ 2048/1237259]
loss: 0.012862 [206848/1237259]
loss: 0.014964 [411648/1237259]
loss: 0.013444 [616448/1237259]
loss: 0.011188 [821248/1237259]
loss: 0.012840 [1026048/1237259]
loss: 0.013097 [1230848/1237259]
Epoch 710
-------------------------------
loss: 0.012167 [ 2048/1237259]
loss: 0.012528 [206848/1237259]
loss: 0.011111 [411648/1237259]
loss: 0.011843 [616448/1237259]
loss: 0.013214 [821248/1237259]
loss: 0.013461 [1026048/1237259]
loss: 0.012900 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0509  
diversity: 0.1839  


Epoch 711
-------------------------------
loss: 0.011541 [ 2048/1237259]
loss: 0.012450 [206848/1237259]
loss: 0.010983 [411648/1237259]
loss: 0.012852 [616448/1237259]
loss: 0.013113 [821248/1237259]
loss: 0.011116 [1026048/1237259]
loss: 0.013269 [1230848/1237259]
Epoch 712
-------------------------------
loss: 0.012822 [ 2048/1237259]
loss: 0.012026 [206848/1237259]
loss: 0.011126 [411648/1237259]
loss: 0.014477 [616448/1237259]
loss: 0.012310 [821248/1237259]
loss: 0.011479 [1026048/1237259]
loss: 0.011193 [1230848/1237259]
Epoch 713
-------------------------------
loss: 0.013023 [ 2048/1237259]
loss: 0.013624 [206848/1237259]
loss: 0.011221 [411648/1237259]
loss: 0.010979 [616448/1237259]
loss: 0.011389 [821248/1237259]
loss: 0.012731 [1026048/1237259]
loss: 0.011833 [1230848/1237259]
Epoch 714
-------------------------------
loss: 0.012384 [ 2048/1237259]
loss: 0.012913 [206848/1237259]
loss: 0.012405 [411648/1237259]
loss: 0.012869 [616448/1237259]
loss: 0.012621 [821248/1237259]
loss: 0.013622 [1026048/1237259]
loss: 0.013474 [1230848/1237259]
Epoch 715
-------------------------------
loss: 0.014048 [ 2048/1237259]
loss: 0.012609 [206848/1237259]
loss: 0.011707 [411648/1237259]
loss: 0.012677 [616448/1237259]
loss: 0.011677 [821248/1237259]
loss: 0.012736 [1026048/1237259]
loss: 0.012460 [1230848/1237259]
Eval results: 
recall@20: 0.0628  
ndcg@20: 0.0508  
diversity: 0.1839  


Epoch 716
-------------------------------
loss: 0.012317 [ 2048/1237259]
loss: 0.011548 [206848/1237259]
loss: 0.013667 [411648/1237259]
loss: 0.011740 [616448/1237259]
loss: 0.012790 [821248/1237259]
loss: 0.012330 [1026048/1237259]
loss: 0.010422 [1230848/1237259]
Epoch 717
-------------------------------
loss: 0.012309 [ 2048/1237259]
loss: 0.012691 [206848/1237259]
loss: 0.014750 [411648/1237259]
loss: 0.012050 [616448/1237259]
loss: 0.012824 [821248/1237259]
loss: 0.012480 [1026048/1237259]
loss: 0.013244 [1230848/1237259]
Epoch 718
-------------------------------
loss: 0.012607 [ 2048/1237259]
loss: 0.012795 [206848/1237259]
loss: 0.011384 [411648/1237259]
loss: 0.012090 [616448/1237259]
loss: 0.011670 [821248/1237259]
loss: 0.013219 [1026048/1237259]
loss: 0.013010 [1230848/1237259]
Epoch 719
-------------------------------
loss: 0.012868 [ 2048/1237259]
loss: 0.014006 [206848/1237259]
loss: 0.012873 [411648/1237259]
loss: 0.012455 [616448/1237259]
loss: 0.011712 [821248/1237259]
loss: 0.012550 [1026048/1237259]
loss: 0.011628 [1230848/1237259]
Epoch 720
-------------------------------
loss: 0.013009 [ 2048/1237259]
loss: 0.012207 [206848/1237259]
loss: 0.011935 [411648/1237259]
loss: 0.012754 [616448/1237259]
loss: 0.012181 [821248/1237259]
loss: 0.013412 [1026048/1237259]
loss: 0.011609 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0510  
diversity: 0.1840  


Epoch 721
-------------------------------
loss: 0.013559 [ 2048/1237259]
loss: 0.012560 [206848/1237259]
loss: 0.012465 [411648/1237259]
loss: 0.013308 [616448/1237259]
loss: 0.012899 [821248/1237259]
loss: 0.012451 [1026048/1237259]
loss: 0.012062 [1230848/1237259]
Epoch 722
-------------------------------
loss: 0.013517 [ 2048/1237259]
loss: 0.011900 [206848/1237259]
loss: 0.012975 [411648/1237259]
loss: 0.012842 [616448/1237259]
loss: 0.012325 [821248/1237259]
loss: 0.012077 [1026048/1237259]
loss: 0.013243 [1230848/1237259]
Epoch 723
-------------------------------
loss: 0.012950 [ 2048/1237259]
loss: 0.015214 [206848/1237259]
loss: 0.012503 [411648/1237259]
loss: 0.013758 [616448/1237259]
loss: 0.012907 [821248/1237259]
loss: 0.012721 [1026048/1237259]
loss: 0.014379 [1230848/1237259]
Epoch 724
-------------------------------
loss: 0.012122 [ 2048/1237259]
loss: 0.012726 [206848/1237259]
loss: 0.011766 [411648/1237259]
loss: 0.012342 [616448/1237259]
loss: 0.012673 [821248/1237259]
loss: 0.012664 [1026048/1237259]
loss: 0.014012 [1230848/1237259]
Epoch 725
-------------------------------
loss: 0.013121 [ 2048/1237259]
loss: 0.012207 [206848/1237259]
loss: 0.012510 [411648/1237259]
loss: 0.012457 [616448/1237259]
loss: 0.013184 [821248/1237259]
loss: 0.012135 [1026048/1237259]
loss: 0.011441 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0510  
diversity: 0.1841  


Epoch 726
-------------------------------
loss: 0.013971 [ 2048/1237259]
loss: 0.015080 [206848/1237259]
loss: 0.013960 [411648/1237259]
loss: 0.012432 [616448/1237259]
loss: 0.012571 [821248/1237259]
loss: 0.012330 [1026048/1237259]
loss: 0.011234 [1230848/1237259]
Epoch 727
-------------------------------
loss: 0.012573 [ 2048/1237259]
loss: 0.012690 [206848/1237259]
loss: 0.010936 [411648/1237259]
loss: 0.013758 [616448/1237259]
loss: 0.012244 [821248/1237259]
loss: 0.012612 [1026048/1237259]
loss: 0.013013 [1230848/1237259]
Epoch 728
-------------------------------
loss: 0.012785 [ 2048/1237259]
loss: 0.011958 [206848/1237259]
loss: 0.012650 [411648/1237259]
loss: 0.013205 [616448/1237259]
loss: 0.014020 [821248/1237259]
loss: 0.014642 [1026048/1237259]
loss: 0.011377 [1230848/1237259]
Epoch 729
-------------------------------
loss: 0.012651 [ 2048/1237259]
loss: 0.011052 [206848/1237259]
loss: 0.012341 [411648/1237259]
loss: 0.012431 [616448/1237259]
loss: 0.012446 [821248/1237259]
loss: 0.011844 [1026048/1237259]
loss: 0.013134 [1230848/1237259]
Epoch 730
-------------------------------
loss: 0.012164 [ 2048/1237259]
loss: 0.011498 [206848/1237259]
loss: 0.012985 [411648/1237259]
loss: 0.012315 [616448/1237259]
loss: 0.012086 [821248/1237259]
loss: 0.013095 [1026048/1237259]
loss: 0.014273 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0633  
ndcg@20: 0.0511  
diversity: 0.1841  


Epoch 731
-------------------------------
loss: 0.012252 [ 2048/1237259]
loss: 0.012522 [206848/1237259]
loss: 0.012355 [411648/1237259]
loss: 0.013045 [616448/1237259]
loss: 0.014156 [821248/1237259]
loss: 0.012586 [1026048/1237259]
loss: 0.011421 [1230848/1237259]
Epoch 732
-------------------------------
loss: 0.013636 [ 2048/1237259]
loss: 0.012224 [206848/1237259]
loss: 0.012912 [411648/1237259]
loss: 0.014382 [616448/1237259]
loss: 0.013265 [821248/1237259]
loss: 0.011899 [1026048/1237259]
loss: 0.011151 [1230848/1237259]
Epoch 733
-------------------------------
loss: 0.012107 [ 2048/1237259]
loss: 0.010650 [206848/1237259]
loss: 0.013023 [411648/1237259]
loss: 0.013249 [616448/1237259]
loss: 0.013099 [821248/1237259]
loss: 0.012062 [1026048/1237259]
loss: 0.011515 [1230848/1237259]
Epoch 734
-------------------------------
loss: 0.012256 [ 2048/1237259]
loss: 0.011205 [206848/1237259]
loss: 0.012880 [411648/1237259]
loss: 0.012570 [616448/1237259]
loss: 0.013358 [821248/1237259]
loss: 0.013625 [1026048/1237259]
loss: 0.013103 [1230848/1237259]
Epoch 735
-------------------------------
loss: 0.013954 [ 2048/1237259]
loss: 0.011749 [206848/1237259]
loss: 0.012657 [411648/1237259]
loss: 0.012238 [616448/1237259]
loss: 0.012200 [821248/1237259]
loss: 0.011484 [1026048/1237259]
loss: 0.014763 [1230848/1237259]
Eval results: 
recall@20: 0.0628  
ndcg@20: 0.0509  
diversity: 0.1841  


Epoch 736
-------------------------------
loss: 0.012870 [ 2048/1237259]
loss: 0.012846 [206848/1237259]
loss: 0.012629 [411648/1237259]
loss: 0.012974 [616448/1237259]
loss: 0.012092 [821248/1237259]
loss: 0.012185 [1026048/1237259]
loss: 0.013177 [1230848/1237259]
Epoch 737
-------------------------------
loss: 0.013171 [ 2048/1237259]
loss: 0.012925 [206848/1237259]
loss: 0.013700 [411648/1237259]
loss: 0.011509 [616448/1237259]
loss: 0.012250 [821248/1237259]
loss: 0.012468 [1026048/1237259]
loss: 0.014570 [1230848/1237259]
Epoch 738
-------------------------------
loss: 0.012638 [ 2048/1237259]
loss: 0.012241 [206848/1237259]
loss: 0.011796 [411648/1237259]
loss: 0.012485 [616448/1237259]
loss: 0.012441 [821248/1237259]
loss: 0.013556 [1026048/1237259]
loss: 0.013075 [1230848/1237259]
Epoch 739
-------------------------------
loss: 0.011540 [ 2048/1237259]
loss: 0.011389 [206848/1237259]
loss: 0.012568 [411648/1237259]
loss: 0.012250 [616448/1237259]
loss: 0.012512 [821248/1237259]
loss: 0.012070 [1026048/1237259]
loss: 0.012694 [1230848/1237259]
Epoch 740
-------------------------------
loss: 0.014563 [ 2048/1237259]
loss: 0.012652 [206848/1237259]
loss: 0.012273 [411648/1237259]
loss: 0.014288 [616448/1237259]
loss: 0.013347 [821248/1237259]
loss: 0.012509 [1026048/1237259]
loss: 0.012340 [1230848/1237259]
Eval results: 
recall@20: 0.0627  
ndcg@20: 0.0509  
diversity: 0.1841  


Epoch 741
-------------------------------
loss: 0.011130 [ 2048/1237259]
loss: 0.012373 [206848/1237259]
loss: 0.012922 [411648/1237259]
loss: 0.011741 [616448/1237259]
loss: 0.011447 [821248/1237259]
loss: 0.012621 [1026048/1237259]
loss: 0.013193 [1230848/1237259]
Epoch 742
-------------------------------
loss: 0.012285 [ 2048/1237259]
loss: 0.013177 [206848/1237259]
loss: 0.012868 [411648/1237259]
loss: 0.013432 [616448/1237259]
loss: 0.012858 [821248/1237259]
loss: 0.012542 [1026048/1237259]
loss: 0.013659 [1230848/1237259]
Epoch 743
-------------------------------
loss: 0.012434 [ 2048/1237259]
loss: 0.011919 [206848/1237259]
loss: 0.013048 [411648/1237259]
loss: 0.011736 [616448/1237259]
loss: 0.011203 [821248/1237259]
loss: 0.012191 [1026048/1237259]
loss: 0.012354 [1230848/1237259]
Epoch 744
-------------------------------
loss: 0.014215 [ 2048/1237259]
loss: 0.011193 [206848/1237259]
loss: 0.013028 [411648/1237259]
loss: 0.013591 [616448/1237259]
loss: 0.013051 [821248/1237259]
loss: 0.012114 [1026048/1237259]
loss: 0.012509 [1230848/1237259]
Epoch 745
-------------------------------
loss: 0.010929 [ 2048/1237259]
loss: 0.012800 [206848/1237259]
loss: 0.012722 [411648/1237259]
loss: 0.012229 [616448/1237259]
loss: 0.012525 [821248/1237259]
loss: 0.013547 [1026048/1237259]
loss: 0.012808 [1230848/1237259]
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0509  
diversity: 0.1842  


Epoch 746
-------------------------------
loss: 0.011900 [ 2048/1237259]
loss: 0.013374 [206848/1237259]
loss: 0.011556 [411648/1237259]
loss: 0.011757 [616448/1237259]
loss: 0.012349 [821248/1237259]
loss: 0.012384 [1026048/1237259]
loss: 0.012320 [1230848/1237259]
Epoch 747
-------------------------------
loss: 0.012444 [ 2048/1237259]
loss: 0.013387 [206848/1237259]
loss: 0.010952 [411648/1237259]
loss: 0.011887 [616448/1237259]
loss: 0.012355 [821248/1237259]
loss: 0.012236 [1026048/1237259]
loss: 0.012995 [1230848/1237259]
Epoch 748
-------------------------------
loss: 0.013495 [ 2048/1237259]
loss: 0.013005 [206848/1237259]
loss: 0.013398 [411648/1237259]
loss: 0.013201 [616448/1237259]
loss: 0.014695 [821248/1237259]
loss: 0.011899 [1026048/1237259]
loss: 0.011853 [1230848/1237259]
Epoch 749
-------------------------------
loss: 0.011304 [ 2048/1237259]
loss: 0.012561 [206848/1237259]
loss: 0.011943 [411648/1237259]
loss: 0.011529 [616448/1237259]
loss: 0.013961 [821248/1237259]
loss: 0.013954 [1026048/1237259]
loss: 0.012378 [1230848/1237259]
Epoch 750
-------------------------------
loss: 0.012828 [ 2048/1237259]
loss: 0.012610 [206848/1237259]
loss: 0.012817 [411648/1237259]
loss: 0.012382 [616448/1237259]
loss: 0.011510 [821248/1237259]
loss: 0.011925 [1026048/1237259]
loss: 0.011832 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0633  
ndcg@20: 0.0511  
diversity: 0.1841  


Epoch 751
-------------------------------
loss: 0.011526 [ 2048/1237259]
loss: 0.012667 [206848/1237259]
loss: 0.012168 [411648/1237259]
loss: 0.013437 [616448/1237259]
loss: 0.012034 [821248/1237259]
loss: 0.011393 [1026048/1237259]
loss: 0.011466 [1230848/1237259]
Epoch 752
-------------------------------
loss: 0.010963 [ 2048/1237259]
loss: 0.011690 [206848/1237259]
loss: 0.011967 [411648/1237259]
loss: 0.012583 [616448/1237259]
loss: 0.012156 [821248/1237259]
loss: 0.013468 [1026048/1237259]
loss: 0.012162 [1230848/1237259]
Epoch 753
-------------------------------
loss: 0.014250 [ 2048/1237259]
loss: 0.011667 [206848/1237259]
loss: 0.013171 [411648/1237259]
loss: 0.013269 [616448/1237259]
loss: 0.013672 [821248/1237259]
loss: 0.012530 [1026048/1237259]
loss: 0.012019 [1230848/1237259]
Epoch 754
-------------------------------
loss: 0.012087 [ 2048/1237259]
loss: 0.013651 [206848/1237259]
loss: 0.013284 [411648/1237259]
loss: 0.012619 [616448/1237259]
loss: 0.012828 [821248/1237259]
loss: 0.011748 [1026048/1237259]
loss: 0.012498 [1230848/1237259]
Epoch 755
-------------------------------
loss: 0.013584 [ 2048/1237259]
loss: 0.011767 [206848/1237259]
loss: 0.011337 [411648/1237259]
loss: 0.013357 [616448/1237259]
loss: 0.011176 [821248/1237259]
loss: 0.012092 [1026048/1237259]
loss: 0.012011 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0633  
ndcg@20: 0.0511  
diversity: 0.1840  


Epoch 756
-------------------------------
loss: 0.012435 [ 2048/1237259]
loss: 0.013130 [206848/1237259]
loss: 0.013584 [411648/1237259]
loss: 0.012258 [616448/1237259]
loss: 0.012615 [821248/1237259]
loss: 0.011555 [1026048/1237259]
loss: 0.012665 [1230848/1237259]
Epoch 757
-------------------------------
loss: 0.012067 [ 2048/1237259]
loss: 0.011325 [206848/1237259]
loss: 0.012987 [411648/1237259]
loss: 0.013598 [616448/1237259]
loss: 0.011756 [821248/1237259]
loss: 0.013999 [1026048/1237259]
loss: 0.012748 [1230848/1237259]
Epoch 758
-------------------------------
loss: 0.012979 [ 2048/1237259]
loss: 0.012170 [206848/1237259]
loss: 0.012426 [411648/1237259]
loss: 0.011817 [616448/1237259]
loss: 0.013019 [821248/1237259]
loss: 0.013741 [1026048/1237259]
loss: 0.012577 [1230848/1237259]
Epoch 759
-------------------------------
loss: 0.011714 [ 2048/1237259]
loss: 0.013576 [206848/1237259]
loss: 0.012864 [411648/1237259]
loss: 0.013769 [616448/1237259]
loss: 0.012686 [821248/1237259]
loss: 0.011294 [1026048/1237259]
loss: 0.012615 [1230848/1237259]
Epoch 760
-------------------------------
loss: 0.012500 [ 2048/1237259]
loss: 0.012406 [206848/1237259]
loss: 0.013093 [411648/1237259]
loss: 0.012138 [616448/1237259]
loss: 0.013180 [821248/1237259]
loss: 0.011287 [1026048/1237259]
loss: 0.011968 [1230848/1237259]
Eval results: 
recall@20: 0.0631  
ndcg@20: 0.0508  
diversity: 0.1840  


Epoch 761
-------------------------------
loss: 0.013557 [ 2048/1237259]
loss: 0.012416 [206848/1237259]
loss: 0.012294 [411648/1237259]
loss: 0.012237 [616448/1237259]
loss: 0.012803 [821248/1237259]
loss: 0.011136 [1026048/1237259]
loss: 0.012734 [1230848/1237259]
Epoch 762
-------------------------------
loss: 0.012641 [ 2048/1237259]
loss: 0.013005 [206848/1237259]
loss: 0.011881 [411648/1237259]
loss: 0.010673 [616448/1237259]
loss: 0.013349 [821248/1237259]
loss: 0.012382 [1026048/1237259]
loss: 0.012292 [1230848/1237259]
Epoch 763
-------------------------------
loss: 0.014978 [ 2048/1237259]
loss: 0.013299 [206848/1237259]
loss: 0.013887 [411648/1237259]
loss: 0.011123 [616448/1237259]
loss: 0.012200 [821248/1237259]
loss: 0.011796 [1026048/1237259]
loss: 0.013132 [1230848/1237259]
Epoch 764
-------------------------------
loss: 0.011240 [ 2048/1237259]
loss: 0.013308 [206848/1237259]
loss: 0.012112 [411648/1237259]
loss: 0.013386 [616448/1237259]
loss: 0.013002 [821248/1237259]
loss: 0.012594 [1026048/1237259]
loss: 0.012897 [1230848/1237259]
Epoch 765
-------------------------------
loss: 0.012280 [ 2048/1237259]
loss: 0.012981 [206848/1237259]
loss: 0.010910 [411648/1237259]
loss: 0.011791 [616448/1237259]
loss: 0.012911 [821248/1237259]
loss: 0.012858 [1026048/1237259]
loss: 0.011240 [1230848/1237259]
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0509  
diversity: 0.1841  


Epoch 766
-------------------------------
loss: 0.012681 [ 2048/1237259]
loss: 0.012770 [206848/1237259]
loss: 0.013580 [411648/1237259]
loss: 0.012440 [616448/1237259]
loss: 0.013241 [821248/1237259]
loss: 0.011215 [1026048/1237259]
loss: 0.012653 [1230848/1237259]
Epoch 767
-------------------------------
loss: 0.011613 [ 2048/1237259]
loss: 0.012715 [206848/1237259]
loss: 0.011519 [411648/1237259]
loss: 0.012334 [616448/1237259]
loss: 0.012532 [821248/1237259]
loss: 0.013342 [1026048/1237259]
loss: 0.012988 [1230848/1237259]
Epoch 768
-------------------------------
loss: 0.011643 [ 2048/1237259]
loss: 0.011890 [206848/1237259]
loss: 0.012538 [411648/1237259]
loss: 0.011332 [616448/1237259]
loss: 0.012252 [821248/1237259]
loss: 0.012343 [1026048/1237259]
loss: 0.012692 [1230848/1237259]
Epoch 769
-------------------------------
loss: 0.012540 [ 2048/1237259]
loss: 0.012793 [206848/1237259]
loss: 0.013538 [411648/1237259]
loss: 0.013345 [616448/1237259]
loss: 0.012249 [821248/1237259]
loss: 0.011393 [1026048/1237259]
loss: 0.012969 [1230848/1237259]
Epoch 770
-------------------------------
loss: 0.011870 [ 2048/1237259]
loss: 0.012123 [206848/1237259]
loss: 0.012050 [411648/1237259]
loss: 0.013682 [616448/1237259]
loss: 0.012656 [821248/1237259]
loss: 0.012466 [1026048/1237259]
loss: 0.011576 [1230848/1237259]
Eval results: 
recall@20: 0.0627  
ndcg@20: 0.0507  
diversity: 0.1840  


Epoch 771
-------------------------------
loss: 0.012014 [ 2048/1237259]
loss: 0.012429 [206848/1237259]
loss: 0.011161 [411648/1237259]
loss: 0.012185 [616448/1237259]
loss: 0.012874 [821248/1237259]
loss: 0.012392 [1026048/1237259]
loss: 0.012028 [1230848/1237259]
Epoch 772
-------------------------------
loss: 0.012317 [ 2048/1237259]
loss: 0.011932 [206848/1237259]
loss: 0.013098 [411648/1237259]
loss: 0.011262 [616448/1237259]
loss: 0.012268 [821248/1237259]
loss: 0.012487 [1026048/1237259]
loss: 0.012968 [1230848/1237259]
Epoch 773
-------------------------------
loss: 0.013714 [ 2048/1237259]
loss: 0.012743 [206848/1237259]
loss: 0.011928 [411648/1237259]
loss: 0.013278 [616448/1237259]
loss: 0.011919 [821248/1237259]
loss: 0.011225 [1026048/1237259]
loss: 0.012119 [1230848/1237259]
Epoch 774
-------------------------------
loss: 0.011737 [ 2048/1237259]
loss: 0.013061 [206848/1237259]
loss: 0.012218 [411648/1237259]
loss: 0.013356 [616448/1237259]
loss: 0.012798 [821248/1237259]
loss: 0.014208 [1026048/1237259]
loss: 0.011369 [1230848/1237259]
Epoch 775
-------------------------------
loss: 0.012286 [ 2048/1237259]
loss: 0.014002 [206848/1237259]
loss: 0.012620 [411648/1237259]
loss: 0.013318 [616448/1237259]
loss: 0.012014 [821248/1237259]
loss: 0.012310 [1026048/1237259]
loss: 0.013625 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0509  
diversity: 0.1841  


Epoch 776
-------------------------------
loss: 0.012679 [ 2048/1237259]
loss: 0.012821 [206848/1237259]
loss: 0.011788 [411648/1237259]
loss: 0.012211 [616448/1237259]
loss: 0.012375 [821248/1237259]
loss: 0.011168 [1026048/1237259]
loss: 0.012327 [1230848/1237259]
Epoch 777
-------------------------------
loss: 0.011991 [ 2048/1237259]
loss: 0.012764 [206848/1237259]
loss: 0.010664 [411648/1237259]
loss: 0.012179 [616448/1237259]
loss: 0.012129 [821248/1237259]
loss: 0.013304 [1026048/1237259]
loss: 0.012794 [1230848/1237259]
Epoch 778
-------------------------------
loss: 0.011303 [ 2048/1237259]
loss: 0.013378 [206848/1237259]
loss: 0.011994 [411648/1237259]
loss: 0.011240 [616448/1237259]
loss: 0.011062 [821248/1237259]
loss: 0.013113 [1026048/1237259]
loss: 0.012900 [1230848/1237259]
Epoch 779
-------------------------------
loss: 0.011838 [ 2048/1237259]
loss: 0.012449 [206848/1237259]
loss: 0.011528 [411648/1237259]
loss: 0.012614 [616448/1237259]
loss: 0.012104 [821248/1237259]
loss: 0.013661 [1026048/1237259]
loss: 0.012138 [1230848/1237259]
Epoch 780
-------------------------------
loss: 0.012627 [ 2048/1237259]
loss: 0.012049 [206848/1237259]
loss: 0.012188 [411648/1237259]
loss: 0.012721 [616448/1237259]
loss: 0.013422 [821248/1237259]
loss: 0.012326 [1026048/1237259]
loss: 0.011862 [1230848/1237259]
Eval results: 
recall@20: 0.0632  
ndcg@20: 0.0509  
diversity: 0.1838  


Epoch 781
-------------------------------
loss: 0.011683 [ 2048/1237259]
loss: 0.013289 [206848/1237259]
loss: 0.012243 [411648/1237259]
loss: 0.012847 [616448/1237259]
loss: 0.012308 [821248/1237259]
loss: 0.013597 [1026048/1237259]
loss: 0.013618 [1230848/1237259]
Epoch 782
-------------------------------
loss: 0.011681 [ 2048/1237259]
loss: 0.012906 [206848/1237259]
loss: 0.012518 [411648/1237259]
loss: 0.012655 [616448/1237259]
loss: 0.011648 [821248/1237259]
loss: 0.013245 [1026048/1237259]
loss: 0.012416 [1230848/1237259]
Epoch 783
-------------------------------
loss: 0.012122 [ 2048/1237259]
loss: 0.015179 [206848/1237259]
loss: 0.011972 [411648/1237259]
loss: 0.013171 [616448/1237259]
loss: 0.011302 [821248/1237259]
loss: 0.011925 [1026048/1237259]
loss: 0.011657 [1230848/1237259]
Epoch 784
-------------------------------
loss: 0.013808 [ 2048/1237259]
loss: 0.011928 [206848/1237259]
loss: 0.014416 [411648/1237259]
loss: 0.012263 [616448/1237259]
loss: 0.011224 [821248/1237259]
loss: 0.012570 [1026048/1237259]
loss: 0.013004 [1230848/1237259]
Epoch 785
-------------------------------
loss: 0.011926 [ 2048/1237259]
loss: 0.011911 [206848/1237259]
loss: 0.012255 [411648/1237259]
loss: 0.012987 [616448/1237259]
loss: 0.012128 [821248/1237259]
loss: 0.012218 [1026048/1237259]
loss: 0.011665 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0509  
diversity: 0.1840  


Epoch 786
-------------------------------
loss: 0.013159 [ 2048/1237259]
loss: 0.012119 [206848/1237259]
loss: 0.013049 [411648/1237259]
loss: 0.012244 [616448/1237259]
loss: 0.013128 [821248/1237259]
loss: 0.011335 [1026048/1237259]
loss: 0.012175 [1230848/1237259]
Epoch 787
-------------------------------
loss: 0.012047 [ 2048/1237259]
loss: 0.015191 [206848/1237259]
loss: 0.011823 [411648/1237259]
loss: 0.012582 [616448/1237259]
loss: 0.012690 [821248/1237259]
loss: 0.012919 [1026048/1237259]
loss: 0.012807 [1230848/1237259]
Epoch 788
-------------------------------
loss: 0.012294 [ 2048/1237259]
loss: 0.013026 [206848/1237259]
loss: 0.011017 [411648/1237259]
loss: 0.014229 [616448/1237259]
loss: 0.011550 [821248/1237259]
loss: 0.012143 [1026048/1237259]
loss: 0.012569 [1230848/1237259]
Epoch 789
-------------------------------
loss: 0.012277 [ 2048/1237259]
loss: 0.012011 [206848/1237259]
loss: 0.013087 [411648/1237259]
loss: 0.012533 [616448/1237259]
loss: 0.011977 [821248/1237259]
loss: 0.012326 [1026048/1237259]
loss: 0.012112 [1230848/1237259]
Epoch 790
-------------------------------
loss: 0.012853 [ 2048/1237259]
loss: 0.012991 [206848/1237259]
loss: 0.013007 [411648/1237259]
loss: 0.012186 [616448/1237259]
loss: 0.011407 [821248/1237259]
loss: 0.011396 [1026048/1237259]
loss: 0.012975 [1230848/1237259]
Eval results: 
recall@20: 0.0632  
ndcg@20: 0.0510  
diversity: 0.1840  


Epoch 791
-------------------------------
loss: 0.012709 [ 2048/1237259]
loss: 0.012558 [206848/1237259]
loss: 0.011806 [411648/1237259]
loss: 0.013027 [616448/1237259]
loss: 0.011694 [821248/1237259]
loss: 0.012934 [1026048/1237259]
loss: 0.011854 [1230848/1237259]
Epoch 792
-------------------------------
loss: 0.011643 [ 2048/1237259]
loss: 0.011569 [206848/1237259]
loss: 0.011589 [411648/1237259]
loss: 0.013509 [616448/1237259]
loss: 0.011120 [821248/1237259]
loss: 0.012214 [1026048/1237259]
loss: 0.012419 [1230848/1237259]
Epoch 793
-------------------------------
loss: 0.012145 [ 2048/1237259]
loss: 0.012577 [206848/1237259]
loss: 0.011188 [411648/1237259]
loss: 0.011171 [616448/1237259]
loss: 0.012553 [821248/1237259]
loss: 0.015231 [1026048/1237259]
loss: 0.012463 [1230848/1237259]
Epoch 794
-------------------------------
loss: 0.012527 [ 2048/1237259]
loss: 0.013014 [206848/1237259]
loss: 0.015639 [411648/1237259]
loss: 0.011470 [616448/1237259]
loss: 0.013822 [821248/1237259]
loss: 0.011547 [1026048/1237259]
loss: 0.011051 [1230848/1237259]
Epoch 795
-------------------------------
loss: 0.011409 [ 2048/1237259]
loss: 0.011240 [206848/1237259]
loss: 0.012065 [411648/1237259]
loss: 0.012589 [616448/1237259]
loss: 0.013436 [821248/1237259]
loss: 0.011557 [1026048/1237259]
loss: 0.013551 [1230848/1237259]
Eval results: 
recall@20: 0.0631  
ndcg@20: 0.0511  
diversity: 0.1842  


Epoch 796
-------------------------------
loss: 0.013520 [ 2048/1237259]
loss: 0.011728 [206848/1237259]
loss: 0.014013 [411648/1237259]
loss: 0.013105 [616448/1237259]
loss: 0.012105 [821248/1237259]
loss: 0.013825 [1026048/1237259]
loss: 0.012654 [1230848/1237259]
Epoch 797
-------------------------------
loss: 0.012567 [ 2048/1237259]
loss: 0.013150 [206848/1237259]
loss: 0.011219 [411648/1237259]
loss: 0.012723 [616448/1237259]
loss: 0.012808 [821248/1237259]
loss: 0.012185 [1026048/1237259]
loss: 0.012228 [1230848/1237259]
Epoch 798
-------------------------------
loss: 0.012019 [ 2048/1237259]
loss: 0.013529 [206848/1237259]
loss: 0.012930 [411648/1237259]
loss: 0.011979 [616448/1237259]
loss: 0.013174 [821248/1237259]
loss: 0.014068 [1026048/1237259]
loss: 0.012407 [1230848/1237259]
Epoch 799
-------------------------------
loss: 0.012839 [ 2048/1237259]
loss: 0.010811 [206848/1237259]
loss: 0.012958 [411648/1237259]
loss: 0.012378 [616448/1237259]
loss: 0.011847 [821248/1237259]
loss: 0.011736 [1026048/1237259]
loss: 0.011399 [1230848/1237259]
Epoch 800
-------------------------------
loss: 0.013531 [ 2048/1237259]
loss: 0.013489 [206848/1237259]
loss: 0.011875 [411648/1237259]
loss: 0.012658 [616448/1237259]
loss: 0.013267 [821248/1237259]
loss: 0.014780 [1026048/1237259]
loss: 0.011729 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0510  
diversity: 0.1841  


Epoch 801
-------------------------------
loss: 0.011677 [ 2048/1237259]
loss: 0.011866 [206848/1237259]
loss: 0.012770 [411648/1237259]
loss: 0.012858 [616448/1237259]
loss: 0.012341 [821248/1237259]
loss: 0.013519 [1026048/1237259]
loss: 0.011903 [1230848/1237259]
Epoch 802
-------------------------------
loss: 0.011984 [ 2048/1237259]
loss: 0.011942 [206848/1237259]
loss: 0.012415 [411648/1237259]
loss: 0.012981 [616448/1237259]
loss: 0.014449 [821248/1237259]
loss: 0.011464 [1026048/1237259]
loss: 0.011507 [1230848/1237259]
Epoch 803
-------------------------------
loss: 0.011416 [ 2048/1237259]
loss: 0.012547 [206848/1237259]
loss: 0.012662 [411648/1237259]
loss: 0.012187 [616448/1237259]
loss: 0.012157 [821248/1237259]
loss: 0.012875 [1026048/1237259]
loss: 0.012916 [1230848/1237259]
Epoch 804
-------------------------------
loss: 0.011213 [ 2048/1237259]
loss: 0.011623 [206848/1237259]
loss: 0.011328 [411648/1237259]
loss: 0.012056 [616448/1237259]
loss: 0.013727 [821248/1237259]
loss: 0.012162 [1026048/1237259]
loss: 0.013569 [1230848/1237259]
Epoch 805
-------------------------------
loss: 0.014012 [ 2048/1237259]
loss: 0.012768 [206848/1237259]
loss: 0.012784 [411648/1237259]
loss: 0.012463 [616448/1237259]
loss: 0.014455 [821248/1237259]
loss: 0.012893 [1026048/1237259]
loss: 0.012561 [1230848/1237259]
Eval results: 
recall@20: 0.0631  
ndcg@20: 0.0510  
diversity: 0.1840  


Epoch 806
-------------------------------
loss: 0.011605 [ 2048/1237259]
loss: 0.011691 [206848/1237259]
loss: 0.012867 [411648/1237259]
loss: 0.011079 [616448/1237259]
loss: 0.012303 [821248/1237259]
loss: 0.011918 [1026048/1237259]
loss: 0.013617 [1230848/1237259]
Epoch 807
-------------------------------
loss: 0.011967 [ 2048/1237259]
loss: 0.014394 [206848/1237259]
loss: 0.012893 [411648/1237259]
loss: 0.011216 [616448/1237259]
loss: 0.012611 [821248/1237259]
loss: 0.012369 [1026048/1237259]
loss: 0.013042 [1230848/1237259]
Epoch 808
-------------------------------
loss: 0.011913 [ 2048/1237259]
loss: 0.012296 [206848/1237259]
loss: 0.011105 [411648/1237259]
loss: 0.011925 [616448/1237259]
loss: 0.012330 [821248/1237259]
loss: 0.011965 [1026048/1237259]
loss: 0.012458 [1230848/1237259]
Epoch 809
-------------------------------
loss: 0.012777 [ 2048/1237259]
loss: 0.013222 [206848/1237259]
loss: 0.011136 [411648/1237259]
loss: 0.014144 [616448/1237259]
loss: 0.012068 [821248/1237259]
loss: 0.012201 [1026048/1237259]
loss: 0.011689 [1230848/1237259]
Epoch 810
-------------------------------
loss: 0.012086 [ 2048/1237259]
loss: 0.011877 [206848/1237259]
loss: 0.010949 [411648/1237259]
loss: 0.011893 [616448/1237259]
loss: 0.014458 [821248/1237259]
loss: 0.011517 [1026048/1237259]
loss: 0.012100 [1230848/1237259]
Eval results: 
recall@20: 0.0632  
ndcg@20: 0.0511  
diversity: 0.1841  


Epoch 811
-------------------------------
loss: 0.012033 [ 2048/1237259]
loss: 0.012126 [206848/1237259]
loss: 0.010836 [411648/1237259]
loss: 0.012047 [616448/1237259]
loss: 0.011023 [821248/1237259]
loss: 0.011608 [1026048/1237259]
loss: 0.012626 [1230848/1237259]
Epoch 812
-------------------------------
loss: 0.012012 [ 2048/1237259]
loss: 0.011596 [206848/1237259]
loss: 0.012109 [411648/1237259]
loss: 0.012467 [616448/1237259]
loss: 0.013438 [821248/1237259]
loss: 0.013765 [1026048/1237259]
loss: 0.011903 [1230848/1237259]
Epoch 813
-------------------------------
loss: 0.012240 [ 2048/1237259]
loss: 0.011463 [206848/1237259]
loss: 0.012062 [411648/1237259]
loss: 0.012527 [616448/1237259]
loss: 0.012134 [821248/1237259]
loss: 0.011449 [1026048/1237259]
loss: 0.013177 [1230848/1237259]
Epoch 814
-------------------------------
loss: 0.012764 [ 2048/1237259]
loss: 0.011255 [206848/1237259]
loss: 0.011337 [411648/1237259]
loss: 0.012051 [616448/1237259]
loss: 0.011775 [821248/1237259]
loss: 0.015668 [1026048/1237259]
loss: 0.012291 [1230848/1237259]
Epoch 815
-------------------------------
loss: 0.014687 [ 2048/1237259]
loss: 0.011187 [206848/1237259]
loss: 0.011632 [411648/1237259]
loss: 0.010755 [616448/1237259]
loss: 0.012880 [821248/1237259]
loss: 0.012442 [1026048/1237259]
loss: 0.012697 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0635  
ndcg@20: 0.0512  
diversity: 0.1841  


Epoch 816
-------------------------------
loss: 0.012892 [ 2048/1237259]
loss: 0.012865 [206848/1237259]
loss: 0.012623 [411648/1237259]
loss: 0.012732 [616448/1237259]
loss: 0.013988 [821248/1237259]
loss: 0.012601 [1026048/1237259]
loss: 0.011977 [1230848/1237259]
Epoch 817
-------------------------------
loss: 0.014318 [ 2048/1237259]
loss: 0.013055 [206848/1237259]
loss: 0.012750 [411648/1237259]
loss: 0.012453 [616448/1237259]
loss: 0.011726 [821248/1237259]
loss: 0.012141 [1026048/1237259]
loss: 0.014216 [1230848/1237259]
Epoch 818
-------------------------------
loss: 0.010664 [ 2048/1237259]
loss: 0.012369 [206848/1237259]
loss: 0.014615 [411648/1237259]
loss: 0.012503 [616448/1237259]
loss: 0.012265 [821248/1237259]
loss: 0.012942 [1026048/1237259]
loss: 0.012833 [1230848/1237259]
Epoch 819
-------------------------------
loss: 0.012406 [ 2048/1237259]
loss: 0.011558 [206848/1237259]
loss: 0.012986 [411648/1237259]
loss: 0.013005 [616448/1237259]
loss: 0.013453 [821248/1237259]
loss: 0.013450 [1026048/1237259]
loss: 0.011877 [1230848/1237259]
Epoch 820
-------------------------------
loss: 0.012579 [ 2048/1237259]
loss: 0.012771 [206848/1237259]
loss: 0.012711 [411648/1237259]
loss: 0.012996 [616448/1237259]
loss: 0.012002 [821248/1237259]
loss: 0.011910 [1026048/1237259]
loss: 0.014579 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0634  
ndcg@20: 0.0513  
diversity: 0.1838  


Epoch 821
-------------------------------
loss: 0.011377 [ 2048/1237259]
loss: 0.013309 [206848/1237259]
loss: 0.011811 [411648/1237259]
loss: 0.012706 [616448/1237259]
loss: 0.012462 [821248/1237259]
loss: 0.012897 [1026048/1237259]
loss: 0.013460 [1230848/1237259]
Epoch 822
-------------------------------
loss: 0.013014 [ 2048/1237259]
loss: 0.012575 [206848/1237259]
loss: 0.012011 [411648/1237259]
loss: 0.011840 [616448/1237259]
loss: 0.013582 [821248/1237259]
loss: 0.012809 [1026048/1237259]
loss: 0.011478 [1230848/1237259]
Epoch 823
-------------------------------
loss: 0.012882 [ 2048/1237259]
loss: 0.012481 [206848/1237259]
loss: 0.010652 [411648/1237259]
loss: 0.013837 [616448/1237259]
loss: 0.011376 [821248/1237259]
loss: 0.013484 [1026048/1237259]
loss: 0.012105 [1230848/1237259]
Epoch 824
-------------------------------
loss: 0.013868 [ 2048/1237259]
loss: 0.012403 [206848/1237259]
loss: 0.012792 [411648/1237259]
loss: 0.012019 [616448/1237259]
loss: 0.012839 [821248/1237259]
loss: 0.011235 [1026048/1237259]
loss: 0.012645 [1230848/1237259]
Epoch 825
-------------------------------
loss: 0.013929 [ 2048/1237259]
loss: 0.012802 [206848/1237259]
loss: 0.013502 [411648/1237259]
loss: 0.011301 [616448/1237259]
loss: 0.013000 [821248/1237259]
loss: 0.012040 [1026048/1237259]
loss: 0.012518 [1230848/1237259]
Eval results: 
recall@20: 0.0632  
ndcg@20: 0.0511  
diversity: 0.1839  


Epoch 826
-------------------------------
loss: 0.011034 [ 2048/1237259]
loss: 0.011663 [206848/1237259]
loss: 0.011716 [411648/1237259]
loss: 0.012920 [616448/1237259]
loss: 0.011470 [821248/1237259]
loss: 0.012331 [1026048/1237259]
loss: 0.011468 [1230848/1237259]
Epoch 827
-------------------------------
loss: 0.012248 [ 2048/1237259]
loss: 0.013147 [206848/1237259]
loss: 0.012210 [411648/1237259]
loss: 0.011160 [616448/1237259]
loss: 0.012052 [821248/1237259]
loss: 0.012681 [1026048/1237259]
loss: 0.013151 [1230848/1237259]
Epoch 828
-------------------------------
loss: 0.012221 [ 2048/1237259]
loss: 0.013954 [206848/1237259]
loss: 0.011364 [411648/1237259]
loss: 0.013262 [616448/1237259]
loss: 0.012260 [821248/1237259]
loss: 0.012019 [1026048/1237259]
loss: 0.012149 [1230848/1237259]
Epoch 829
-------------------------------
loss: 0.011014 [ 2048/1237259]
loss: 0.013442 [206848/1237259]
loss: 0.012661 [411648/1237259]
loss: 0.012053 [616448/1237259]
loss: 0.012759 [821248/1237259]
loss: 0.010877 [1026048/1237259]
loss: 0.011941 [1230848/1237259]
Epoch 830
-------------------------------
loss: 0.011172 [ 2048/1237259]
loss: 0.012035 [206848/1237259]
loss: 0.013140 [411648/1237259]
loss: 0.010760 [616448/1237259]
loss: 0.011948 [821248/1237259]
loss: 0.013342 [1026048/1237259]
loss: 0.012407 [1230848/1237259]
Eval results: 
recall@20: 0.0633  
ndcg@20: 0.0512  
diversity: 0.1841  


Epoch 831
-------------------------------
loss: 0.013031 [ 2048/1237259]
loss: 0.013359 [206848/1237259]
loss: 0.014104 [411648/1237259]
loss: 0.014189 [616448/1237259]
loss: 0.012498 [821248/1237259]
loss: 0.011838 [1026048/1237259]
loss: 0.012747 [1230848/1237259]
Epoch 832
-------------------------------
loss: 0.012842 [ 2048/1237259]
loss: 0.012819 [206848/1237259]
loss: 0.012856 [411648/1237259]
loss: 0.011688 [616448/1237259]
loss: 0.012078 [821248/1237259]
loss: 0.010967 [1026048/1237259]
loss: 0.011616 [1230848/1237259]
Epoch 833
-------------------------------
loss: 0.013345 [ 2048/1237259]
loss: 0.013719 [206848/1237259]
loss: 0.011512 [411648/1237259]
loss: 0.012290 [616448/1237259]
loss: 0.012222 [821248/1237259]
loss: 0.012795 [1026048/1237259]
loss: 0.010835 [1230848/1237259]
Epoch 834
-------------------------------
loss: 0.011266 [ 2048/1237259]
loss: 0.013053 [206848/1237259]
loss: 0.011949 [411648/1237259]
loss: 0.012568 [616448/1237259]
loss: 0.011648 [821248/1237259]
loss: 0.012899 [1026048/1237259]
loss: 0.011375 [1230848/1237259]
Epoch 835
-------------------------------
loss: 0.012201 [ 2048/1237259]
loss: 0.012973 [206848/1237259]
loss: 0.012745 [411648/1237259]
loss: 0.011947 [616448/1237259]
loss: 0.012267 [821248/1237259]
loss: 0.012703 [1026048/1237259]
loss: 0.012315 [1230848/1237259]
Eval results: 
recall@20: 0.0633  
ndcg@20: 0.0513  
diversity: 0.1841  


Epoch 836
-------------------------------
loss: 0.012471 [ 2048/1237259]
loss: 0.011176 [206848/1237259]
loss: 0.011940 [411648/1237259]
loss: 0.010810 [616448/1237259]
loss: 0.013584 [821248/1237259]
loss: 0.012912 [1026048/1237259]
loss: 0.012800 [1230848/1237259]
Epoch 837
-------------------------------
loss: 0.012358 [ 2048/1237259]
loss: 0.012740 [206848/1237259]
loss: 0.011934 [411648/1237259]
loss: 0.012230 [616448/1237259]
loss: 0.012499 [821248/1237259]
loss: 0.012175 [1026048/1237259]
loss: 0.012438 [1230848/1237259]
Epoch 838
-------------------------------
loss: 0.011661 [ 2048/1237259]
loss: 0.011230 [206848/1237259]
loss: 0.012562 [411648/1237259]
loss: 0.011447 [616448/1237259]
loss: 0.013195 [821248/1237259]
loss: 0.011975 [1026048/1237259]
loss: 0.012330 [1230848/1237259]
Epoch 839
-------------------------------
loss: 0.013808 [ 2048/1237259]
loss: 0.012650 [206848/1237259]
loss: 0.013677 [411648/1237259]
loss: 0.011354 [616448/1237259]
loss: 0.011722 [821248/1237259]
loss: 0.012642 [1026048/1237259]
loss: 0.011590 [1230848/1237259]
Epoch 840
-------------------------------
loss: 0.011859 [ 2048/1237259]
loss: 0.013266 [206848/1237259]
loss: 0.012255 [411648/1237259]
loss: 0.013651 [616448/1237259]
loss: 0.011308 [821248/1237259]
loss: 0.011714 [1026048/1237259]
loss: 0.013404 [1230848/1237259]
Eval results: 
recall@20: 0.0633  
ndcg@20: 0.0512  
diversity: 0.1840  


Epoch 841
-------------------------------
loss: 0.012391 [ 2048/1237259]
loss: 0.013054 [206848/1237259]
loss: 0.010784 [411648/1237259]
loss: 0.011875 [616448/1237259]
loss: 0.011934 [821248/1237259]
loss: 0.012802 [1026048/1237259]
loss: 0.012456 [1230848/1237259]
Epoch 842
-------------------------------
loss: 0.013422 [ 2048/1237259]
loss: 0.012179 [206848/1237259]
loss: 0.012786 [411648/1237259]
loss: 0.014296 [616448/1237259]
loss: 0.012093 [821248/1237259]
loss: 0.012042 [1026048/1237259]
loss: 0.013067 [1230848/1237259]
Epoch 843
-------------------------------
loss: 0.012196 [ 2048/1237259]
loss: 0.012493 [206848/1237259]
loss: 0.011896 [411648/1237259]
loss: 0.013004 [616448/1237259]
loss: 0.012627 [821248/1237259]
loss: 0.012334 [1026048/1237259]
loss: 0.014095 [1230848/1237259]
Epoch 844
-------------------------------
loss: 0.012265 [ 2048/1237259]
loss: 0.012201 [206848/1237259]
loss: 0.010782 [411648/1237259]
loss: 0.012951 [616448/1237259]
loss: 0.012626 [821248/1237259]
loss: 0.011826 [1026048/1237259]
loss: 0.012435 [1230848/1237259]
Epoch 845
-------------------------------
loss: 0.013837 [ 2048/1237259]
loss: 0.012759 [206848/1237259]
loss: 0.013332 [411648/1237259]
loss: 0.013861 [616448/1237259]
loss: 0.011429 [821248/1237259]
loss: 0.012584 [1026048/1237259]
loss: 0.012634 [1230848/1237259]
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0511  
diversity: 0.1839  


Epoch 846
-------------------------------
loss: 0.013105 [ 2048/1237259]
loss: 0.013264 [206848/1237259]
loss: 0.013697 [411648/1237259]
loss: 0.012270 [616448/1237259]
loss: 0.012603 [821248/1237259]
loss: 0.011029 [1026048/1237259]
loss: 0.013851 [1230848/1237259]
Epoch 847
-------------------------------
loss: 0.013576 [ 2048/1237259]
loss: 0.011832 [206848/1237259]
loss: 0.011091 [411648/1237259]
loss: 0.013073 [616448/1237259]
loss: 0.013088 [821248/1237259]
loss: 0.011717 [1026048/1237259]
loss: 0.012085 [1230848/1237259]
Epoch 848
-------------------------------
loss: 0.012631 [ 2048/1237259]
loss: 0.012183 [206848/1237259]
loss: 0.011074 [411648/1237259]
loss: 0.012077 [616448/1237259]
loss: 0.011194 [821248/1237259]
loss: 0.013160 [1026048/1237259]
loss: 0.011917 [1230848/1237259]
Epoch 849
-------------------------------
loss: 0.013220 [ 2048/1237259]
loss: 0.012718 [206848/1237259]
loss: 0.011841 [411648/1237259]
loss: 0.013179 [616448/1237259]
loss: 0.011926 [821248/1237259]
loss: 0.012570 [1026048/1237259]
loss: 0.012787 [1230848/1237259]
Epoch 850
-------------------------------
loss: 0.010998 [ 2048/1237259]
loss: 0.011496 [206848/1237259]
loss: 0.011861 [411648/1237259]
loss: 0.011255 [616448/1237259]
loss: 0.012711 [821248/1237259]
loss: 0.013688 [1026048/1237259]
loss: 0.011986 [1230848/1237259]
Eval results: 
recall@20: 0.0631  
ndcg@20: 0.0510  
diversity: 0.1841  


Epoch 851
-------------------------------
loss: 0.011981 [ 2048/1237259]
loss: 0.014114 [206848/1237259]
loss: 0.011916 [411648/1237259]
loss: 0.013278 [616448/1237259]
loss: 0.012172 [821248/1237259]
loss: 0.012999 [1026048/1237259]
loss: 0.014225 [1230848/1237259]
Epoch 852
-------------------------------
loss: 0.012516 [ 2048/1237259]
loss: 0.012581 [206848/1237259]
loss: 0.011370 [411648/1237259]
loss: 0.012878 [616448/1237259]
loss: 0.011989 [821248/1237259]
loss: 0.012843 [1026048/1237259]
loss: 0.012095 [1230848/1237259]
Epoch 853
-------------------------------
loss: 0.012968 [ 2048/1237259]
loss: 0.010732 [206848/1237259]
loss: 0.011829 [411648/1237259]
loss: 0.010841 [616448/1237259]
loss: 0.014820 [821248/1237259]
loss: 0.012470 [1026048/1237259]
loss: 0.012478 [1230848/1237259]
Epoch 854
-------------------------------
loss: 0.012008 [ 2048/1237259]
loss: 0.013217 [206848/1237259]
loss: 0.012455 [411648/1237259]
loss: 0.011673 [616448/1237259]
loss: 0.013152 [821248/1237259]
loss: 0.013448 [1026048/1237259]
loss: 0.012702 [1230848/1237259]
Epoch 855
-------------------------------
loss: 0.012516 [ 2048/1237259]
loss: 0.013118 [206848/1237259]
loss: 0.013492 [411648/1237259]
loss: 0.013099 [616448/1237259]
loss: 0.011187 [821248/1237259]
loss: 0.013363 [1026048/1237259]
loss: 0.013668 [1230848/1237259]
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0509  
diversity: 0.1841  


Epoch 856
-------------------------------
loss: 0.012339 [ 2048/1237259]
loss: 0.012524 [206848/1237259]
loss: 0.013277 [411648/1237259]
loss: 0.013955 [616448/1237259]
loss: 0.012481 [821248/1237259]
loss: 0.011820 [1026048/1237259]
loss: 0.011820 [1230848/1237259]
Epoch 857
-------------------------------
loss: 0.011829 [ 2048/1237259]
loss: 0.011527 [206848/1237259]
loss: 0.012309 [411648/1237259]
loss: 0.012563 [616448/1237259]
loss: 0.013280 [821248/1237259]
loss: 0.011672 [1026048/1237259]
loss: 0.012592 [1230848/1237259]
Epoch 858
-------------------------------
loss: 0.013231 [ 2048/1237259]
loss: 0.012397 [206848/1237259]
loss: 0.012100 [411648/1237259]
loss: 0.011549 [616448/1237259]
loss: 0.012672 [821248/1237259]
loss: 0.011489 [1026048/1237259]
loss: 0.013022 [1230848/1237259]
Epoch 859
-------------------------------
loss: 0.011562 [ 2048/1237259]
loss: 0.012161 [206848/1237259]
loss: 0.011188 [411648/1237259]
loss: 0.012057 [616448/1237259]
loss: 0.011296 [821248/1237259]
loss: 0.013333 [1026048/1237259]
loss: 0.014119 [1230848/1237259]
Epoch 860
-------------------------------
loss: 0.011008 [ 2048/1237259]
loss: 0.012870 [206848/1237259]
loss: 0.012905 [411648/1237259]
loss: 0.013499 [616448/1237259]
loss: 0.011480 [821248/1237259]
loss: 0.011940 [1026048/1237259]
loss: 0.012681 [1230848/1237259]
Eval results: 
recall@20: 0.0634  
ndcg@20: 0.0512  
diversity: 0.1840  


Epoch 861
-------------------------------
loss: 0.012043 [ 2048/1237259]
loss: 0.012392 [206848/1237259]
loss: 0.014226 [411648/1237259]
loss: 0.011899 [616448/1237259]
loss: 0.013042 [821248/1237259]
loss: 0.012793 [1026048/1237259]
loss: 0.013423 [1230848/1237259]
Epoch 862
-------------------------------
loss: 0.010614 [ 2048/1237259]
loss: 0.012153 [206848/1237259]
loss: 0.011561 [411648/1237259]
loss: 0.012146 [616448/1237259]
loss: 0.011771 [821248/1237259]
loss: 0.011913 [1026048/1237259]
loss: 0.011688 [1230848/1237259]
Epoch 863
-------------------------------
loss: 0.013283 [ 2048/1237259]
loss: 0.012222 [206848/1237259]
loss: 0.013961 [411648/1237259]
loss: 0.012648 [616448/1237259]
loss: 0.013048 [821248/1237259]
loss: 0.010968 [1026048/1237259]
loss: 0.013474 [1230848/1237259]
Epoch 864
-------------------------------
loss: 0.011041 [ 2048/1237259]
loss: 0.011641 [206848/1237259]
loss: 0.012702 [411648/1237259]
loss: 0.012626 [616448/1237259]
loss: 0.012046 [821248/1237259]
loss: 0.012497 [1026048/1237259]
loss: 0.012201 [1230848/1237259]
Epoch 865
-------------------------------
loss: 0.012714 [ 2048/1237259]
loss: 0.012321 [206848/1237259]
loss: 0.013422 [411648/1237259]
loss: 0.011234 [616448/1237259]
loss: 0.013173 [821248/1237259]
loss: 0.012807 [1026048/1237259]
loss: 0.013938 [1230848/1237259]
Eval results: 
recall@20: 0.0632  
ndcg@20: 0.0512  
diversity: 0.1841  


Epoch 866
-------------------------------
loss: 0.012915 [ 2048/1237259]
loss: 0.012978 [206848/1237259]
loss: 0.012912 [411648/1237259]
loss: 0.013394 [616448/1237259]
loss: 0.012383 [821248/1237259]
loss: 0.012120 [1026048/1237259]
loss: 0.012835 [1230848/1237259]
Epoch 867
-------------------------------
loss: 0.013996 [ 2048/1237259]
loss: 0.012008 [206848/1237259]
loss: 0.013289 [411648/1237259]
loss: 0.011767 [616448/1237259]
loss: 0.012599 [821248/1237259]
loss: 0.011440 [1026048/1237259]
loss: 0.012868 [1230848/1237259]
Epoch 868
-------------------------------
loss: 0.012704 [ 2048/1237259]
loss: 0.012681 [206848/1237259]
loss: 0.011611 [411648/1237259]
loss: 0.012032 [616448/1237259]
loss: 0.011906 [821248/1237259]
loss: 0.011350 [1026048/1237259]
loss: 0.011548 [1230848/1237259]
Epoch 869
-------------------------------
loss: 0.012785 [ 2048/1237259]
loss: 0.014126 [206848/1237259]
loss: 0.011595 [411648/1237259]
loss: 0.011971 [616448/1237259]
loss: 0.012836 [821248/1237259]
loss: 0.011796 [1026048/1237259]
loss: 0.013262 [1230848/1237259]
Epoch 870
-------------------------------
loss: 0.012341 [ 2048/1237259]
loss: 0.011234 [206848/1237259]
loss: 0.012835 [411648/1237259]
loss: 0.011424 [616448/1237259]
loss: 0.012510 [821248/1237259]
loss: 0.011859 [1026048/1237259]
loss: 0.013233 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0633  
ndcg@20: 0.0513  
diversity: 0.1841  


Epoch 871
-------------------------------
loss: 0.011080 [ 2048/1237259]
loss: 0.012120 [206848/1237259]
loss: 0.012521 [411648/1237259]
loss: 0.012455 [616448/1237259]
loss: 0.012734 [821248/1237259]
loss: 0.012283 [1026048/1237259]
loss: 0.012414 [1230848/1237259]
Epoch 872
-------------------------------
loss: 0.011939 [ 2048/1237259]
loss: 0.013873 [206848/1237259]
loss: 0.012062 [411648/1237259]
loss: 0.012332 [616448/1237259]
loss: 0.011705 [821248/1237259]
loss: 0.012600 [1026048/1237259]
loss: 0.011257 [1230848/1237259]
Epoch 873
-------------------------------
loss: 0.011421 [ 2048/1237259]
loss: 0.013144 [206848/1237259]
loss: 0.011836 [411648/1237259]
loss: 0.013653 [616448/1237259]
loss: 0.012441 [821248/1237259]
loss: 0.012269 [1026048/1237259]
loss: 0.012733 [1230848/1237259]
Epoch 874
-------------------------------
loss: 0.012513 [ 2048/1237259]
loss: 0.012386 [206848/1237259]
loss: 0.012069 [411648/1237259]
loss: 0.014213 [616448/1237259]
loss: 0.012376 [821248/1237259]
loss: 0.012444 [1026048/1237259]
loss: 0.012478 [1230848/1237259]
Epoch 875
-------------------------------
loss: 0.012752 [ 2048/1237259]
loss: 0.011254 [206848/1237259]
loss: 0.013238 [411648/1237259]
loss: 0.012555 [616448/1237259]
loss: 0.011795 [821248/1237259]
loss: 0.014195 [1026048/1237259]
loss: 0.012205 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0636  
ndcg@20: 0.0514  
diversity: 0.1841  


Epoch 876
-------------------------------
loss: 0.012055 [ 2048/1237259]
loss: 0.011226 [206848/1237259]
loss: 0.011677 [411648/1237259]
loss: 0.013483 [616448/1237259]
loss: 0.011924 [821248/1237259]
loss: 0.011575 [1026048/1237259]
loss: 0.012779 [1230848/1237259]
Epoch 877
-------------------------------
loss: 0.012166 [ 2048/1237259]
loss: 0.013398 [206848/1237259]
loss: 0.011487 [411648/1237259]
loss: 0.013178 [616448/1237259]
loss: 0.011961 [821248/1237259]
loss: 0.012545 [1026048/1237259]
loss: 0.012939 [1230848/1237259]
Epoch 878
-------------------------------
loss: 0.012803 [ 2048/1237259]
loss: 0.012726 [206848/1237259]
loss: 0.012897 [411648/1237259]
loss: 0.011147 [616448/1237259]
loss: 0.012248 [821248/1237259]
loss: 0.011155 [1026048/1237259]
loss: 0.011745 [1230848/1237259]
Epoch 879
-------------------------------
loss: 0.011012 [ 2048/1237259]
