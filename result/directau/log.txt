Folder created at: ./directau/best_model
Your Device: cuda
Yelp2018

#user = 31668
#item = 38048

#interactions
    (train) 1237259
    (test)  324147
    (total) 1561406

Sparsity = 0.0012958757851778645

Epoch 0
-------------------------------
loss: -1.976110 [ 2048/1237259]
loss: -2.643587 [206848/1237259]
loss: -2.692559 [411648/1237259]
loss: -2.687423 [616448/1237259]
loss: -2.697272 [821248/1237259]
loss: -2.718110 [1026048/1237259]
loss: -2.704472 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0497  
diversity: 0.2080  


Epoch 1
-------------------------------
loss: -2.838569 [ 2048/1237259]
loss: -2.806821 [206848/1237259]
loss: -2.790790 [411648/1237259]
loss: -2.781441 [616448/1237259]
loss: -2.764806 [821248/1237259]
loss: -2.758159 [1026048/1237259]
loss: -2.763028 [1230848/1237259]
Epoch 2
-------------------------------
loss: -2.852269 [ 2048/1237259]
loss: -2.827384 [206848/1237259]
loss: -2.816765 [411648/1237259]
loss: -2.807957 [616448/1237259]
loss: -2.799603 [821248/1237259]
loss: -2.774185 [1026048/1237259]
loss: -2.789547 [1230848/1237259]
Epoch 3
-------------------------------
loss: -2.878147 [ 2048/1237259]
loss: -2.831900 [206848/1237259]
loss: -2.831395 [411648/1237259]
loss: -2.816036 [616448/1237259]
loss: -2.794461 [821248/1237259]
loss: -2.782827 [1026048/1237259]
loss: -2.797021 [1230848/1237259]
Epoch 4
-------------------------------
loss: -2.876044 [ 2048/1237259]
loss: -2.842783 [206848/1237259]
loss: -2.812249 [411648/1237259]
loss: -2.805032 [616448/1237259]
loss: -2.802685 [821248/1237259]
loss: -2.803968 [1026048/1237259]
loss: -2.804588 [1230848/1237259]
Epoch 5
-------------------------------
loss: -2.846525 [ 2048/1237259]
loss: -2.861810 [206848/1237259]
loss: -2.834299 [411648/1237259]
loss: -2.828848 [616448/1237259]
loss: -2.807423 [821248/1237259]
loss: -2.822472 [1026048/1237259]
loss: -2.800542 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0665  
ndcg@20: 0.0549  
diversity: 0.2064  


Epoch 6
-------------------------------
loss: -2.865287 [ 2048/1237259]
loss: -2.869817 [206848/1237259]
loss: -2.853599 [411648/1237259]
loss: -2.828015 [616448/1237259]
loss: -2.814580 [821248/1237259]
loss: -2.794499 [1026048/1237259]
loss: -2.803577 [1230848/1237259]
Epoch 7
-------------------------------
loss: -2.872533 [ 2048/1237259]
loss: -2.843990 [206848/1237259]
loss: -2.825475 [411648/1237259]
loss: -2.832449 [616448/1237259]
loss: -2.809679 [821248/1237259]
loss: -2.797499 [1026048/1237259]
loss: -2.808461 [1230848/1237259]
Epoch 8
-------------------------------
loss: -2.869939 [ 2048/1237259]
loss: -2.857206 [206848/1237259]
loss: -2.834675 [411648/1237259]
loss: -2.832585 [616448/1237259]
loss: -2.805433 [821248/1237259]
loss: -2.812062 [1026048/1237259]
loss: -2.796129 [1230848/1237259]
Epoch 9
-------------------------------
loss: -2.868834 [ 2048/1237259]
loss: -2.861418 [206848/1237259]
loss: -2.843840 [411648/1237259]
loss: -2.830154 [616448/1237259]
loss: -2.809382 [821248/1237259]
loss: -2.815205 [1026048/1237259]
loss: -2.811862 [1230848/1237259]
Epoch 10
-------------------------------
loss: -2.869053 [ 2048/1237259]
loss: -2.861497 [206848/1237259]
loss: -2.841071 [411648/1237259]
loss: -2.812254 [616448/1237259]
loss: -2.831673 [821248/1237259]
loss: -2.832049 [1026048/1237259]
loss: -2.813215 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0673  
ndcg@20: 0.0559  
diversity: 0.2057  


Epoch 11
-------------------------------
loss: -2.868631 [ 2048/1237259]
loss: -2.855037 [206848/1237259]
loss: -2.844724 [411648/1237259]
loss: -2.828645 [616448/1237259]
loss: -2.837149 [821248/1237259]
loss: -2.819300 [1026048/1237259]
loss: -2.818642 [1230848/1237259]
Epoch 12
-------------------------------
loss: -2.863269 [ 2048/1237259]
loss: -2.866376 [206848/1237259]
loss: -2.830867 [411648/1237259]
loss: -2.840550 [616448/1237259]
loss: -2.847013 [821248/1237259]
loss: -2.827705 [1026048/1237259]
loss: -2.820107 [1230848/1237259]
Epoch 13
-------------------------------
loss: -2.864053 [ 2048/1237259]
loss: -2.870479 [206848/1237259]
loss: -2.868682 [411648/1237259]
loss: -2.841444 [616448/1237259]
loss: -2.841310 [821248/1237259]
loss: -2.813824 [1026048/1237259]
loss: -2.803454 [1230848/1237259]
Epoch 14
-------------------------------
loss: -2.868351 [ 2048/1237259]
loss: -2.867216 [206848/1237259]
loss: -2.863153 [411648/1237259]
loss: -2.856701 [616448/1237259]
loss: -2.851797 [821248/1237259]
loss: -2.806362 [1026048/1237259]
loss: -2.822189 [1230848/1237259]
Epoch 15
-------------------------------
loss: -2.863312 [ 2048/1237259]
loss: -2.857981 [206848/1237259]
loss: -2.857332 [411648/1237259]
loss: -2.847962 [616448/1237259]
loss: -2.841861 [821248/1237259]
loss: -2.825665 [1026048/1237259]
loss: -2.810356 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0679  
ndcg@20: 0.0564  
diversity: 0.2055  


Epoch 16
-------------------------------
loss: -2.866405 [ 2048/1237259]
loss: -2.853041 [206848/1237259]
loss: -2.852088 [411648/1237259]
loss: -2.835126 [616448/1237259]
loss: -2.823633 [821248/1237259]
loss: -2.840257 [1026048/1237259]
loss: -2.822810 [1230848/1237259]
Epoch 17
-------------------------------
loss: -2.872531 [ 2048/1237259]
loss: -2.851582 [206848/1237259]
loss: -2.862993 [411648/1237259]
loss: -2.843861 [616448/1237259]
loss: -2.847619 [821248/1237259]
loss: -2.826606 [1026048/1237259]
loss: -2.835491 [1230848/1237259]
Epoch 18
-------------------------------
loss: -2.860560 [ 2048/1237259]
loss: -2.865276 [206848/1237259]
loss: -2.869129 [411648/1237259]
loss: -2.839555 [616448/1237259]
loss: -2.823170 [821248/1237259]
loss: -2.838127 [1026048/1237259]
loss: -2.831208 [1230848/1237259]
Epoch 19
-------------------------------
loss: -2.875062 [ 2048/1237259]
loss: -2.868987 [206848/1237259]
loss: -2.865316 [411648/1237259]
loss: -2.826168 [616448/1237259]
loss: -2.846554 [821248/1237259]
loss: -2.832270 [1026048/1237259]
loss: -2.834120 [1230848/1237259]
Epoch 20
-------------------------------
loss: -2.865613 [ 2048/1237259]
loss: -2.863964 [206848/1237259]
loss: -2.839207 [411648/1237259]
loss: -2.845788 [616448/1237259]
loss: -2.838597 [821248/1237259]
loss: -2.838191 [1026048/1237259]
loss: -2.820018 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0682  
ndcg@20: 0.0567  
diversity: 0.2054  


Epoch 21
-------------------------------
loss: -2.883958 [ 2048/1237259]
loss: -2.861714 [206848/1237259]
loss: -2.840007 [411648/1237259]
loss: -2.844692 [616448/1237259]
loss: -2.843710 [821248/1237259]
loss: -2.834653 [1026048/1237259]
loss: -2.823911 [1230848/1237259]
Epoch 22
-------------------------------
loss: -2.885534 [ 2048/1237259]
loss: -2.854421 [206848/1237259]
loss: -2.852695 [411648/1237259]
loss: -2.836008 [616448/1237259]
loss: -2.820870 [821248/1237259]
loss: -2.835828 [1026048/1237259]
loss: -2.812650 [1230848/1237259]
Epoch 23
-------------------------------
loss: -2.878716 [ 2048/1237259]
loss: -2.867579 [206848/1237259]
loss: -2.847634 [411648/1237259]
loss: -2.857291 [616448/1237259]
loss: -2.835660 [821248/1237259]
loss: -2.834208 [1026048/1237259]
loss: -2.822792 [1230848/1237259]
Epoch 24
-------------------------------
loss: -2.876177 [ 2048/1237259]
loss: -2.871911 [206848/1237259]
loss: -2.857147 [411648/1237259]
loss: -2.861494 [616448/1237259]
loss: -2.843662 [821248/1237259]
loss: -2.829144 [1026048/1237259]
loss: -2.819047 [1230848/1237259]
Epoch 25
-------------------------------
loss: -2.865071 [ 2048/1237259]
loss: -2.865177 [206848/1237259]
loss: -2.862234 [411648/1237259]
loss: -2.857494 [616448/1237259]
loss: -2.817725 [821248/1237259]
loss: -2.832206 [1026048/1237259]
loss: -2.824188 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0684  
ndcg@20: 0.0568  
diversity: 0.2054  


Epoch 26
-------------------------------
loss: -2.860437 [ 2048/1237259]
loss: -2.862612 [206848/1237259]
loss: -2.853516 [411648/1237259]
loss: -2.848007 [616448/1237259]
loss: -2.842935 [821248/1237259]
loss: -2.835957 [1026048/1237259]
loss: -2.846132 [1230848/1237259]
Epoch 27
-------------------------------
loss: -2.863791 [ 2048/1237259]
loss: -2.861783 [206848/1237259]
loss: -2.852756 [411648/1237259]
loss: -2.846858 [616448/1237259]
loss: -2.845803 [821248/1237259]
loss: -2.842616 [1026048/1237259]
loss: -2.847610 [1230848/1237259]
Epoch 28
-------------------------------
loss: -2.863127 [ 2048/1237259]
loss: -2.870726 [206848/1237259]
loss: -2.843448 [411648/1237259]
loss: -2.855853 [616448/1237259]
loss: -2.844843 [821248/1237259]
loss: -2.841719 [1026048/1237259]
loss: -2.832323 [1230848/1237259]
Epoch 29
-------------------------------
loss: -2.878902 [ 2048/1237259]
loss: -2.855774 [206848/1237259]
loss: -2.860815 [411648/1237259]
loss: -2.837277 [616448/1237259]
loss: -2.849363 [821248/1237259]
loss: -2.831510 [1026048/1237259]
loss: -2.833645 [1230848/1237259]
Epoch 30
-------------------------------
loss: -2.885425 [ 2048/1237259]
loss: -2.859236 [206848/1237259]
loss: -2.848296 [411648/1237259]
loss: -2.867597 [616448/1237259]
loss: -2.859626 [821248/1237259]
loss: -2.837764 [1026048/1237259]
loss: -2.839273 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0685  
ndcg@20: 0.0570  
diversity: 0.2054  


Epoch 31
-------------------------------
loss: -2.888723 [ 2048/1237259]
loss: -2.855317 [206848/1237259]
loss: -2.860946 [411648/1237259]
loss: -2.844810 [616448/1237259]
loss: -2.819323 [821248/1237259]
loss: -2.839979 [1026048/1237259]
loss: -2.823374 [1230848/1237259]
Epoch 32
-------------------------------
loss: -2.885524 [ 2048/1237259]
loss: -2.884440 [206848/1237259]
loss: -2.870939 [411648/1237259]
loss: -2.841356 [616448/1237259]
loss: -2.847740 [821248/1237259]
loss: -2.846612 [1026048/1237259]
loss: -2.847278 [1230848/1237259]
Epoch 33
-------------------------------
loss: -2.867743 [ 2048/1237259]
loss: -2.864690 [206848/1237259]
loss: -2.871401 [411648/1237259]
loss: -2.863269 [616448/1237259]
loss: -2.840694 [821248/1237259]
loss: -2.853065 [1026048/1237259]
loss: -2.844382 [1230848/1237259]
Epoch 34
-------------------------------
loss: -2.872130 [ 2048/1237259]
loss: -2.845868 [206848/1237259]
loss: -2.850156 [411648/1237259]
loss: -2.851855 [616448/1237259]
loss: -2.868243 [821248/1237259]
loss: -2.836302 [1026048/1237259]
loss: -2.842972 [1230848/1237259]
Epoch 35
-------------------------------
loss: -2.888889 [ 2048/1237259]
loss: -2.866803 [206848/1237259]
loss: -2.857592 [411648/1237259]
loss: -2.845248 [616448/1237259]
loss: -2.848291 [821248/1237259]
loss: -2.843503 [1026048/1237259]
loss: -2.839855 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0687  
ndcg@20: 0.0571  
diversity: 0.2053  


Epoch 36
-------------------------------
loss: -2.859017 [ 2048/1237259]
loss: -2.865058 [206848/1237259]
loss: -2.868008 [411648/1237259]
loss: -2.853685 [616448/1237259]
loss: -2.845268 [821248/1237259]
loss: -2.846180 [1026048/1237259]
loss: -2.833365 [1230848/1237259]
Epoch 37
-------------------------------
loss: -2.867561 [ 2048/1237259]
loss: -2.851596 [206848/1237259]
loss: -2.873317 [411648/1237259]
loss: -2.863114 [616448/1237259]
loss: -2.857300 [821248/1237259]
loss: -2.841714 [1026048/1237259]
loss: -2.835745 [1230848/1237259]
Epoch 38
-------------------------------
loss: -2.875700 [ 2048/1237259]
loss: -2.864798 [206848/1237259]
loss: -2.846391 [411648/1237259]
loss: -2.865397 [616448/1237259]
loss: -2.849646 [821248/1237259]
loss: -2.844196 [1026048/1237259]
loss: -2.837754 [1230848/1237259]
Epoch 39
-------------------------------
loss: -2.884815 [ 2048/1237259]
loss: -2.880929 [206848/1237259]
loss: -2.863578 [411648/1237259]
loss: -2.861488 [616448/1237259]
loss: -2.840072 [821248/1237259]
loss: -2.836962 [1026048/1237259]
loss: -2.836976 [1230848/1237259]
Epoch 40
-------------------------------
loss: -2.891679 [ 2048/1237259]
loss: -2.858193 [206848/1237259]
loss: -2.867199 [411648/1237259]
loss: -2.842014 [616448/1237259]
loss: -2.852603 [821248/1237259]
loss: -2.858301 [1026048/1237259]
loss: -2.846106 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0688  
ndcg@20: 0.0571  
diversity: 0.2053  


Epoch 41
-------------------------------
loss: -2.876761 [ 2048/1237259]
loss: -2.868730 [206848/1237259]
loss: -2.855277 [411648/1237259]
loss: -2.859984 [616448/1237259]
loss: -2.859169 [821248/1237259]
loss: -2.832594 [1026048/1237259]
loss: -2.834661 [1230848/1237259]
Epoch 42
-------------------------------
loss: -2.881600 [ 2048/1237259]
loss: -2.871189 [206848/1237259]
loss: -2.870986 [411648/1237259]
loss: -2.838674 [616448/1237259]
loss: -2.863350 [821248/1237259]
loss: -2.828607 [1026048/1237259]
loss: -2.843309 [1230848/1237259]
Epoch 43
-------------------------------
loss: -2.889527 [ 2048/1237259]
loss: -2.866700 [206848/1237259]
loss: -2.869863 [411648/1237259]
loss: -2.846762 [616448/1237259]
loss: -2.838944 [821248/1237259]
loss: -2.850039 [1026048/1237259]
loss: -2.841439 [1230848/1237259]
Epoch 44
-------------------------------
loss: -2.872675 [ 2048/1237259]
loss: -2.872283 [206848/1237259]
loss: -2.862422 [411648/1237259]
loss: -2.852282 [616448/1237259]
loss: -2.853640 [821248/1237259]
loss: -2.840075 [1026048/1237259]
loss: -2.850616 [1230848/1237259]
Epoch 45
-------------------------------
loss: -2.874852 [ 2048/1237259]
loss: -2.880967 [206848/1237259]
loss: -2.861465 [411648/1237259]
loss: -2.847497 [616448/1237259]
loss: -2.846937 [821248/1237259]
loss: -2.843138 [1026048/1237259]
loss: -2.842846 [1230848/1237259]
Eval results: 
recall@20: 0.0687  
ndcg@20: 0.0571  
diversity: 0.2053  


Epoch 46
-------------------------------
loss: -2.872842 [ 2048/1237259]
loss: -2.846810 [206848/1237259]
loss: -2.861369 [411648/1237259]
loss: -2.843968 [616448/1237259]
loss: -2.853677 [821248/1237259]
loss: -2.837562 [1026048/1237259]
loss: -2.853040 [1230848/1237259]
Epoch 47
-------------------------------
loss: -2.894321 [ 2048/1237259]
loss: -2.858556 [206848/1237259]
loss: -2.858007 [411648/1237259]
loss: -2.846635 [616448/1237259]
loss: -2.852896 [821248/1237259]
loss: -2.847732 [1026048/1237259]
loss: -2.830279 [1230848/1237259]
Epoch 48
-------------------------------
loss: -2.873569 [ 2048/1237259]
loss: -2.880572 [206848/1237259]
loss: -2.862130 [411648/1237259]
loss: -2.852448 [616448/1237259]
loss: -2.848053 [821248/1237259]
loss: -2.849213 [1026048/1237259]
loss: -2.846728 [1230848/1237259]
Epoch 49
-------------------------------
loss: -2.887890 [ 2048/1237259]
loss: -2.877138 [206848/1237259]
loss: -2.859942 [411648/1237259]
loss: -2.862256 [616448/1237259]
loss: -2.836457 [821248/1237259]
loss: -2.846138 [1026048/1237259]
loss: -2.835382 [1230848/1237259]
Epoch 50
-------------------------------
loss: -2.879152 [ 2048/1237259]
loss: -2.861871 [206848/1237259]
loss: -2.858231 [411648/1237259]
loss: -2.861467 [616448/1237259]
loss: -2.851986 [821248/1237259]
loss: -2.848446 [1026048/1237259]
loss: -2.844901 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0688  
ndcg@20: 0.0572  
diversity: 0.2053  


Epoch 51
-------------------------------
loss: -2.886003 [ 2048/1237259]
loss: -2.874472 [206848/1237259]
loss: -2.866254 [411648/1237259]
loss: -2.852720 [616448/1237259]
loss: -2.851501 [821248/1237259]
loss: -2.839077 [1026048/1237259]
loss: -2.836302 [1230848/1237259]
Epoch 52
-------------------------------
loss: -2.882705 [ 2048/1237259]
loss: -2.872395 [206848/1237259]
loss: -2.853657 [411648/1237259]
loss: -2.836131 [616448/1237259]
loss: -2.853328 [821248/1237259]
loss: -2.854883 [1026048/1237259]
loss: -2.853298 [1230848/1237259]
Epoch 53
-------------------------------
loss: -2.878480 [ 2048/1237259]
loss: -2.868445 [206848/1237259]
loss: -2.872819 [411648/1237259]
loss: -2.850544 [616448/1237259]
loss: -2.857852 [821248/1237259]
loss: -2.851801 [1026048/1237259]
loss: -2.850436 [1230848/1237259]
Epoch 54
-------------------------------
loss: -2.879774 [ 2048/1237259]
loss: -2.854650 [206848/1237259]
loss: -2.857392 [411648/1237259]
loss: -2.858043 [616448/1237259]
loss: -2.856081 [821248/1237259]
loss: -2.837951 [1026048/1237259]
loss: -2.852694 [1230848/1237259]
Epoch 55
-------------------------------
loss: -2.878403 [ 2048/1237259]
loss: -2.866331 [206848/1237259]
loss: -2.862006 [411648/1237259]
loss: -2.871534 [616448/1237259]
loss: -2.837224 [821248/1237259]
loss: -2.864524 [1026048/1237259]
loss: -2.844591 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0689  
ndcg@20: 0.0572  
diversity: 0.2053  


Epoch 56
-------------------------------
loss: -2.875512 [ 2048/1237259]
loss: -2.872639 [206848/1237259]
loss: -2.852481 [411648/1237259]
loss: -2.854004 [616448/1237259]
loss: -2.853559 [821248/1237259]
loss: -2.852660 [1026048/1237259]
loss: -2.842373 [1230848/1237259]
Epoch 57
-------------------------------
loss: -2.874716 [ 2048/1237259]
loss: -2.870777 [206848/1237259]
loss: -2.857062 [411648/1237259]
loss: -2.871128 [616448/1237259]
loss: -2.846134 [821248/1237259]
loss: -2.849382 [1026048/1237259]
loss: -2.858763 [1230848/1237259]
Epoch 58
-------------------------------
loss: -2.873460 [ 2048/1237259]
loss: -2.869421 [206848/1237259]
loss: -2.864006 [411648/1237259]
loss: -2.849941 [616448/1237259]
loss: -2.867613 [821248/1237259]
loss: -2.865526 [1026048/1237259]
loss: -2.838592 [1230848/1237259]
Epoch 59
-------------------------------
loss: -2.876265 [ 2048/1237259]
loss: -2.867878 [206848/1237259]
loss: -2.864040 [411648/1237259]
loss: -2.850659 [616448/1237259]
loss: -2.872031 [821248/1237259]
loss: -2.844677 [1026048/1237259]
loss: -2.832693 [1230848/1237259]
Epoch 60
-------------------------------
loss: -2.870093 [ 2048/1237259]
loss: -2.877150 [206848/1237259]
loss: -2.878904 [411648/1237259]
loss: -2.864963 [616448/1237259]
loss: -2.836243 [821248/1237259]
loss: -2.850065 [1026048/1237259]
loss: -2.844776 [1230848/1237259]
Eval results: 
recall@20: 0.0688  
ndcg@20: 0.0572  
diversity: 0.2053  


Epoch 61
-------------------------------
loss: -2.895224 [ 2048/1237259]
loss: -2.865765 [206848/1237259]
loss: -2.862925 [411648/1237259]
loss: -2.852633 [616448/1237259]
loss: -2.844195 [821248/1237259]
loss: -2.861116 [1026048/1237259]
loss: -2.839342 [1230848/1237259]
Epoch 62
-------------------------------
loss: -2.876883 [ 2048/1237259]
loss: -2.876079 [206848/1237259]
loss: -2.864992 [411648/1237259]
loss: -2.839385 [616448/1237259]
loss: -2.837960 [821248/1237259]
loss: -2.849334 [1026048/1237259]
loss: -2.845516 [1230848/1237259]
Epoch 63
-------------------------------
loss: -2.883717 [ 2048/1237259]
loss: -2.863057 [206848/1237259]
loss: -2.885856 [411648/1237259]
loss: -2.862695 [616448/1237259]
loss: -2.855103 [821248/1237259]
loss: -2.851033 [1026048/1237259]
loss: -2.841815 [1230848/1237259]
Epoch 64
-------------------------------
loss: -2.890948 [ 2048/1237259]
loss: -2.868526 [206848/1237259]
loss: -2.877590 [411648/1237259]
loss: -2.856294 [616448/1237259]
loss: -2.860022 [821248/1237259]
loss: -2.852451 [1026048/1237259]
loss: -2.857619 [1230848/1237259]
Epoch 65
-------------------------------
loss: -2.876545 [ 2048/1237259]
loss: -2.883039 [206848/1237259]
loss: -2.858393 [411648/1237259]
loss: -2.859739 [616448/1237259]
loss: -2.849595 [821248/1237259]
loss: -2.841618 [1026048/1237259]
loss: -2.835021 [1230848/1237259]
Eval results: 
recall@20: 0.0689  
ndcg@20: 0.0572  
diversity: 0.2053  


Epoch 66
-------------------------------
loss: -2.864594 [ 2048/1237259]
loss: -2.857775 [206848/1237259]
loss: -2.866043 [411648/1237259]
loss: -2.862836 [616448/1237259]
loss: -2.853540 [821248/1237259]
loss: -2.856054 [1026048/1237259]
loss: -2.850445 [1230848/1237259]
Epoch 67
-------------------------------
loss: -2.856919 [ 2048/1237259]
loss: -2.856552 [206848/1237259]
loss: -2.851863 [411648/1237259]
loss: -2.874682 [616448/1237259]
loss: -2.854417 [821248/1237259]
loss: -2.865407 [1026048/1237259]
loss: -2.834911 [1230848/1237259]
Epoch 68
-------------------------------
loss: -2.874318 [ 2048/1237259]
loss: -2.875712 [206848/1237259]
loss: -2.869819 [411648/1237259]
loss: -2.864923 [616448/1237259]
loss: -2.852929 [821248/1237259]
loss: -2.841845 [1026048/1237259]
loss: -2.846563 [1230848/1237259]
Epoch 69
-------------------------------
loss: -2.874302 [ 2048/1237259]
loss: -2.859274 [206848/1237259]
loss: -2.855239 [411648/1237259]
loss: -2.837725 [616448/1237259]
loss: -2.857231 [821248/1237259]
loss: -2.854765 [1026048/1237259]
loss: -2.842589 [1230848/1237259]
Epoch 70
-------------------------------
loss: -2.871015 [ 2048/1237259]
loss: -2.867127 [206848/1237259]
loss: -2.866761 [411648/1237259]
loss: -2.843043 [616448/1237259]
loss: -2.855625 [821248/1237259]
loss: -2.857107 [1026048/1237259]
loss: -2.840156 [1230848/1237259]
Eval results: 
recall@20: 0.0689  
ndcg@20: 0.0572  
diversity: 0.2053  


Epoch 71
-------------------------------
loss: -2.865168 [ 2048/1237259]
loss: -2.862111 [206848/1237259]
loss: -2.865899 [411648/1237259]
loss: -2.877081 [616448/1237259]
loss: -2.849463 [821248/1237259]
loss: -2.853404 [1026048/1237259]
loss: -2.851230 [1230848/1237259]
Epoch 72
-------------------------------
loss: -2.890204 [ 2048/1237259]
loss: -2.881012 [206848/1237259]
loss: -2.864616 [411648/1237259]
loss: -2.866271 [616448/1237259]
loss: -2.858695 [821248/1237259]
loss: -2.859923 [1026048/1237259]
loss: -2.846160 [1230848/1237259]
Epoch 73
-------------------------------
loss: -2.876247 [ 2048/1237259]
loss: -2.856874 [206848/1237259]
loss: -2.866583 [411648/1237259]
loss: -2.873178 [616448/1237259]
loss: -2.872049 [821248/1237259]
loss: -2.861786 [1026048/1237259]
loss: -2.857916 [1230848/1237259]
Epoch 74
-------------------------------
loss: -2.862581 [ 2048/1237259]
loss: -2.868722 [206848/1237259]
loss: -2.863630 [411648/1237259]
loss: -2.873924 [616448/1237259]
loss: -2.871434 [821248/1237259]
loss: -2.860885 [1026048/1237259]
loss: -2.838893 [1230848/1237259]
Epoch 75
-------------------------------
loss: -2.870414 [ 2048/1237259]
loss: -2.876689 [206848/1237259]
loss: -2.868415 [411648/1237259]
loss: -2.874646 [616448/1237259]
loss: -2.862334 [821248/1237259]
loss: -2.840667 [1026048/1237259]
loss: -2.832179 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0689  
ndcg@20: 0.0573  
diversity: 0.2053  


Epoch 76
-------------------------------
loss: -2.876864 [ 2048/1237259]
loss: -2.870350 [206848/1237259]
loss: -2.867772 [411648/1237259]
loss: -2.864532 [616448/1237259]
loss: -2.862732 [821248/1237259]
loss: -2.845226 [1026048/1237259]
loss: -2.833308 [1230848/1237259]
Epoch 77
-------------------------------
loss: -2.877775 [ 2048/1237259]
loss: -2.873902 [206848/1237259]
loss: -2.867839 [411648/1237259]
loss: -2.854242 [616448/1237259]
loss: -2.849909 [821248/1237259]
loss: -2.853258 [1026048/1237259]
loss: -2.840641 [1230848/1237259]
Epoch 78
-------------------------------
loss: -2.870711 [ 2048/1237259]
loss: -2.871783 [206848/1237259]
loss: -2.872261 [411648/1237259]
loss: -2.855865 [616448/1237259]
loss: -2.847873 [821248/1237259]
loss: -2.844722 [1026048/1237259]
loss: -2.857793 [1230848/1237259]
Epoch 79
-------------------------------
loss: -2.889064 [ 2048/1237259]
loss: -2.881881 [206848/1237259]
loss: -2.872789 [411648/1237259]
loss: -2.866587 [616448/1237259]
loss: -2.879644 [821248/1237259]
loss: -2.863931 [1026048/1237259]
loss: -2.849761 [1230848/1237259]
Epoch 80
-------------------------------
loss: -2.872432 [ 2048/1237259]
loss: -2.862867 [206848/1237259]
loss: -2.874648 [411648/1237259]
loss: -2.863472 [616448/1237259]
loss: -2.851890 [821248/1237259]
loss: -2.847476 [1026048/1237259]
loss: -2.846828 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0690  
ndcg@20: 0.0574  
diversity: 0.2053  


Epoch 81
-------------------------------
loss: -2.892740 [ 2048/1237259]
loss: -2.875283 [206848/1237259]
loss: -2.855393 [411648/1237259]
loss: -2.856874 [616448/1237259]
loss: -2.844382 [821248/1237259]
loss: -2.861732 [1026048/1237259]
loss: -2.847270 [1230848/1237259]
Epoch 82
-------------------------------
loss: -2.878140 [ 2048/1237259]
loss: -2.876073 [206848/1237259]
loss: -2.850117 [411648/1237259]
loss: -2.854403 [616448/1237259]
loss: -2.858167 [821248/1237259]
loss: -2.858805 [1026048/1237259]
loss: -2.859041 [1230848/1237259]
Epoch 83
-------------------------------
loss: -2.879962 [ 2048/1237259]
loss: -2.861162 [206848/1237259]
loss: -2.857149 [411648/1237259]
loss: -2.859187 [616448/1237259]
loss: -2.862943 [821248/1237259]
loss: -2.855835 [1026048/1237259]
loss: -2.829683 [1230848/1237259]
Epoch 84
-------------------------------
loss: -2.881612 [ 2048/1237259]
loss: -2.858541 [206848/1237259]
loss: -2.865459 [411648/1237259]
loss: -2.859699 [616448/1237259]
loss: -2.855668 [821248/1237259]
loss: -2.857016 [1026048/1237259]
loss: -2.869410 [1230848/1237259]
Epoch 85
-------------------------------
loss: -2.878283 [ 2048/1237259]
loss: -2.879185 [206848/1237259]
loss: -2.867502 [411648/1237259]
loss: -2.859677 [616448/1237259]
loss: -2.870974 [821248/1237259]
loss: -2.856651 [1026048/1237259]
loss: -2.854930 [1230848/1237259]
Eval results: 
recall@20: 0.0689  
ndcg@20: 0.0573  
diversity: 0.2052  


Epoch 86
-------------------------------
loss: -2.881499 [ 2048/1237259]
loss: -2.881209 [206848/1237259]
loss: -2.864615 [411648/1237259]
loss: -2.867917 [616448/1237259]
loss: -2.850303 [821248/1237259]
loss: -2.864445 [1026048/1237259]
loss: -2.844514 [1230848/1237259]
Epoch 87
-------------------------------
loss: -2.874100 [ 2048/1237259]
loss: -2.857827 [206848/1237259]
loss: -2.880396 [411648/1237259]
loss: -2.852865 [616448/1237259]
loss: -2.852149 [821248/1237259]
loss: -2.840296 [1026048/1237259]
loss: -2.843647 [1230848/1237259]
Epoch 88
-------------------------------
loss: -2.885050 [ 2048/1237259]
loss: -2.863662 [206848/1237259]
loss: -2.850864 [411648/1237259]
loss: -2.860654 [616448/1237259]
loss: -2.860231 [821248/1237259]
loss: -2.853685 [1026048/1237259]
loss: -2.842411 [1230848/1237259]
Epoch 89
-------------------------------
loss: -2.864104 [ 2048/1237259]
loss: -2.862125 [206848/1237259]
loss: -2.869349 [411648/1237259]
loss: -2.865160 [616448/1237259]
loss: -2.852862 [821248/1237259]
loss: -2.855169 [1026048/1237259]
loss: -2.857058 [1230848/1237259]
Epoch 90
-------------------------------
loss: -2.865630 [ 2048/1237259]
loss: -2.873798 [206848/1237259]
loss: -2.866763 [411648/1237259]
loss: -2.863294 [616448/1237259]
loss: -2.855940 [821248/1237259]
loss: -2.855833 [1026048/1237259]
loss: -2.857260 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0690  
ndcg@20: 0.0573  
diversity: 0.2052  


Epoch 91
-------------------------------
loss: -2.874628 [ 2048/1237259]
loss: -2.885180 [206848/1237259]
loss: -2.887717 [411648/1237259]
loss: -2.856152 [616448/1237259]
loss: -2.861961 [821248/1237259]
loss: -2.850424 [1026048/1237259]
loss: -2.852292 [1230848/1237259]
Epoch 92
-------------------------------
loss: -2.858710 [ 2048/1237259]
loss: -2.863990 [206848/1237259]
loss: -2.869080 [411648/1237259]
loss: -2.854079 [616448/1237259]
loss: -2.858000 [821248/1237259]
loss: -2.835200 [1026048/1237259]
loss: -2.844572 [1230848/1237259]
Epoch 93
-------------------------------
loss: -2.873711 [ 2048/1237259]
loss: -2.876936 [206848/1237259]
loss: -2.862262 [411648/1237259]
loss: -2.855909 [616448/1237259]
loss: -2.872081 [821248/1237259]
loss: -2.875955 [1026048/1237259]
loss: -2.844320 [1230848/1237259]
Epoch 94
-------------------------------
loss: -2.883357 [ 2048/1237259]
loss: -2.871743 [206848/1237259]
loss: -2.869118 [411648/1237259]
loss: -2.865766 [616448/1237259]
loss: -2.859710 [821248/1237259]
loss: -2.848009 [1026048/1237259]
loss: -2.861373 [1230848/1237259]
Epoch 95
-------------------------------
loss: -2.876716 [ 2048/1237259]
loss: -2.869295 [206848/1237259]
loss: -2.870363 [411648/1237259]
loss: -2.857844 [616448/1237259]
loss: -2.854102 [821248/1237259]
loss: -2.835677 [1026048/1237259]
loss: -2.841560 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0691  
ndcg@20: 0.0573  
diversity: 0.2052  


Epoch 96
-------------------------------
loss: -2.874601 [ 2048/1237259]
loss: -2.870857 [206848/1237259]
loss: -2.864210 [411648/1237259]
loss: -2.875679 [616448/1237259]
loss: -2.858096 [821248/1237259]
loss: -2.848752 [1026048/1237259]
loss: -2.866134 [1230848/1237259]
Epoch 97
-------------------------------
loss: -2.881943 [ 2048/1237259]
loss: -2.872192 [206848/1237259]
loss: -2.873197 [411648/1237259]
loss: -2.877007 [616448/1237259]
loss: -2.860673 [821248/1237259]
loss: -2.868495 [1026048/1237259]
loss: -2.851726 [1230848/1237259]
Epoch 98
-------------------------------
loss: -2.857544 [ 2048/1237259]
loss: -2.871819 [206848/1237259]
loss: -2.867742 [411648/1237259]
loss: -2.871774 [616448/1237259]
loss: -2.857278 [821248/1237259]
loss: -2.843282 [1026048/1237259]
loss: -2.847379 [1230848/1237259]
Epoch 99
-------------------------------
loss: -2.879538 [ 2048/1237259]
loss: -2.878239 [206848/1237259]
loss: -2.869663 [411648/1237259]
loss: -2.860173 [616448/1237259]
loss: -2.860834 [821248/1237259]
loss: -2.846400 [1026048/1237259]
loss: -2.859637 [1230848/1237259]
Epoch 100
-------------------------------
loss: -2.887531 [ 2048/1237259]
loss: -2.871256 [206848/1237259]
loss: -2.861516 [411648/1237259]
loss: -2.860237 [616448/1237259]
loss: -2.855895 [821248/1237259]
loss: -2.849953 [1026048/1237259]
loss: -2.858836 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0691  
ndcg@20: 0.0574  
diversity: 0.2052  


Epoch 101
-------------------------------
loss: -2.886515 [ 2048/1237259]
loss: -2.877501 [206848/1237259]
loss: -2.864586 [411648/1237259]
loss: -2.854020 [616448/1237259]
loss: -2.868127 [821248/1237259]
loss: -2.865909 [1026048/1237259]
loss: -2.851806 [1230848/1237259]
Epoch 102
-------------------------------
loss: -2.869848 [ 2048/1237259]
loss: -2.873691 [206848/1237259]
loss: -2.867857 [411648/1237259]
loss: -2.871227 [616448/1237259]
loss: -2.856707 [821248/1237259]
loss: -2.861146 [1026048/1237259]
loss: -2.851854 [1230848/1237259]
Epoch 103
-------------------------------
loss: -2.866111 [ 2048/1237259]
loss: -2.870411 [206848/1237259]
loss: -2.866622 [411648/1237259]
loss: -2.857987 [616448/1237259]
loss: -2.859445 [821248/1237259]
loss: -2.855653 [1026048/1237259]
loss: -2.850459 [1230848/1237259]
Epoch 104
-------------------------------
loss: -2.877056 [ 2048/1237259]
loss: -2.878166 [206848/1237259]
loss: -2.882006 [411648/1237259]
loss: -2.867359 [616448/1237259]
loss: -2.858703 [821248/1237259]
loss: -2.865404 [1026048/1237259]
loss: -2.859986 [1230848/1237259]
Epoch 105
-------------------------------
loss: -2.883630 [ 2048/1237259]
loss: -2.868283 [206848/1237259]
loss: -2.866381 [411648/1237259]
loss: -2.855919 [616448/1237259]
loss: -2.862906 [821248/1237259]
loss: -2.858811 [1026048/1237259]
loss: -2.850294 [1230848/1237259]
Eval results: 
recall@20: 0.0690  
ndcg@20: 0.0573  
diversity: 0.2051  


Epoch 106
-------------------------------
loss: -2.881190 [ 2048/1237259]
loss: -2.874387 [206848/1237259]
loss: -2.852219 [411648/1237259]
loss: -2.848862 [616448/1237259]
loss: -2.872207 [821248/1237259]
loss: -2.854970 [1026048/1237259]
loss: -2.840312 [1230848/1237259]
Epoch 107
-------------------------------
loss: -2.888974 [ 2048/1237259]
loss: -2.882267 [206848/1237259]
loss: -2.875914 [411648/1237259]
loss: -2.856071 [616448/1237259]
loss: -2.850437 [821248/1237259]
loss: -2.861436 [1026048/1237259]
loss: -2.847679 [1230848/1237259]
Epoch 108
-------------------------------
loss: -2.887110 [ 2048/1237259]
loss: -2.872460 [206848/1237259]
loss: -2.856918 [411648/1237259]
loss: -2.873876 [616448/1237259]
loss: -2.861003 [821248/1237259]
loss: -2.852387 [1026048/1237259]
loss: -2.862006 [1230848/1237259]
Epoch 109
-------------------------------
loss: -2.868968 [ 2048/1237259]
loss: -2.867474 [206848/1237259]
loss: -2.871682 [411648/1237259]
loss: -2.858532 [616448/1237259]
loss: -2.869369 [821248/1237259]
loss: -2.866841 [1026048/1237259]
loss: -2.861877 [1230848/1237259]
Epoch 110
-------------------------------
loss: -2.883532 [ 2048/1237259]
loss: -2.888491 [206848/1237259]
loss: -2.856424 [411648/1237259]
loss: -2.856876 [616448/1237259]
loss: -2.862778 [821248/1237259]
loss: -2.851423 [1026048/1237259]
loss: -2.858283 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0692  
ndcg@20: 0.0575  
diversity: 0.2051  


Epoch 111
-------------------------------
loss: -2.862217 [ 2048/1237259]
loss: -2.860519 [206848/1237259]
loss: -2.875563 [411648/1237259]
loss: -2.857853 [616448/1237259]
loss: -2.872337 [821248/1237259]
loss: -2.873129 [1026048/1237259]
loss: -2.834783 [1230848/1237259]
Epoch 112
-------------------------------
loss: -2.883176 [ 2048/1237259]
loss: -2.862523 [206848/1237259]
loss: -2.871228 [411648/1237259]
loss: -2.866275 [616448/1237259]
loss: -2.861693 [821248/1237259]
loss: -2.864178 [1026048/1237259]
loss: -2.848075 [1230848/1237259]
Epoch 113
-------------------------------
loss: -2.881233 [ 2048/1237259]
loss: -2.860315 [206848/1237259]
loss: -2.868534 [411648/1237259]
loss: -2.865794 [616448/1237259]
loss: -2.855597 [821248/1237259]
loss: -2.855121 [1026048/1237259]
loss: -2.861265 [1230848/1237259]
Epoch 114
-------------------------------
loss: -2.877721 [ 2048/1237259]
loss: -2.860053 [206848/1237259]
loss: -2.859279 [411648/1237259]
loss: -2.869545 [616448/1237259]
loss: -2.859404 [821248/1237259]
loss: -2.858958 [1026048/1237259]
loss: -2.844383 [1230848/1237259]
Epoch 115
-------------------------------
loss: -2.868652 [ 2048/1237259]
loss: -2.882700 [206848/1237259]
loss: -2.875741 [411648/1237259]
loss: -2.855999 [616448/1237259]
loss: -2.870457 [821248/1237259]
loss: -2.847350 [1026048/1237259]
loss: -2.846921 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0692  
ndcg@20: 0.0575  
diversity: 0.2051  


Epoch 116
-------------------------------
loss: -2.870059 [ 2048/1237259]
loss: -2.868871 [206848/1237259]
loss: -2.872150 [411648/1237259]
loss: -2.854353 [616448/1237259]
loss: -2.848533 [821248/1237259]
loss: -2.853016 [1026048/1237259]
loss: -2.831042 [1230848/1237259]
Epoch 117
-------------------------------
loss: -2.883775 [ 2048/1237259]
loss: -2.870556 [206848/1237259]
loss: -2.869572 [411648/1237259]
loss: -2.869761 [616448/1237259]
loss: -2.862800 [821248/1237259]
loss: -2.861305 [1026048/1237259]
loss: -2.844683 [1230848/1237259]
Epoch 118
-------------------------------
loss: -2.876563 [ 2048/1237259]
loss: -2.875300 [206848/1237259]
loss: -2.858818 [411648/1237259]
loss: -2.869555 [616448/1237259]
loss: -2.858854 [821248/1237259]
loss: -2.855625 [1026048/1237259]
loss: -2.849350 [1230848/1237259]
Epoch 119
-------------------------------
loss: -2.875997 [ 2048/1237259]
loss: -2.866782 [206848/1237259]
loss: -2.874268 [411648/1237259]
loss: -2.859752 [616448/1237259]
loss: -2.851406 [821248/1237259]
loss: -2.858691 [1026048/1237259]
loss: -2.852662 [1230848/1237259]
Epoch 120
-------------------------------
loss: -2.887110 [ 2048/1237259]
loss: -2.876186 [206848/1237259]
loss: -2.867405 [411648/1237259]
loss: -2.858817 [616448/1237259]
loss: -2.873805 [821248/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0692  
ndcg@20: 0.0575  
diversity: 0.2051  


Epoch 121
-------------------------------
loss: -2.870483 [ 2048/1237259]
loss: -2.889841 [206848/1237259]
loss: -2.862480 [411648/1237259]
loss: -2.875661 [616448/1237259]
loss: -2.857527 [821248/1237259]
loss: -2.860338 [1026048/1237259]
loss: -2.858714 [1230848/1237259]
Epoch 122
-------------------------------
loss: -2.897440 [ 2048/1237259]
loss: -2.859010 [206848/1237259]
loss: -2.866185 [411648/1237259]
loss: -2.857935 [616448/1237259]
loss: -2.861082 [821248/1237259]
loss: -2.871809 [1026048/1237259]
loss: -2.852223 [1230848/1237259]
Epoch 123
-------------------------------
loss: -2.885421 [ 2048/1237259]
loss: -2.860593 [206848/1237259]
loss: -2.855822 [411648/1237259]
loss: -2.870954 [616448/1237259]
loss: -2.872735 [821248/1237259]
loss: -2.870784 [1026048/1237259]
loss: -2.842342 [1230848/1237259]
Epoch 124
-------------------------------
loss: -2.870335 [ 2048/1237259]
loss: -2.861844 [206848/1237259]
loss: -2.840563 [411648/1237259]
loss: -2.860542 [616448/1237259]
loss: -2.855345 [821248/1237259]
loss: -2.866845 [1026048/1237259]
loss: -2.854892 [1230848/1237259]
Epoch 125
-------------------------------
loss: -2.879257 [ 2048/1237259]
loss: -2.870075 [206848/1237259]
loss: -2.877448 [411648/1237259]
loss: -2.861946 [616448/1237259]
loss: -2.857091 [821248/1237259]
loss: -2.863544 [1026048/1237259]
loss: -2.854375 [1230848/1237259]
Eval results: 
recall@20: 0.0692  
ndcg@20: 0.0574  
diversity: 0.2051  


Epoch 126
-------------------------------
loss: -2.863259 [ 2048/1237259]
loss: -2.873254 [206848/1237259]
loss: -2.880392 [411648/1237259]
loss: -2.862284 [616448/1237259]
loss: -2.854577 [821248/1237259]
loss: -2.850422 [1026048/1237259]
loss: -2.851748 [1230848/1237259]
Epoch 127
-------------------------------
loss: -2.870802 [ 2048/1237259]
loss: -2.871801 [206848/1237259]
loss: -2.882797 [411648/1237259]
loss: -2.879528 [616448/1237259]
loss: -2.857768 [821248/1237259]
loss: -2.841020 [1026048/1237259]
loss: -2.853521 [1230848/1237259]
Epoch 128
-------------------------------
loss: -2.857305 [ 2048/1237259]
loss: -2.880357 [206848/1237259]
loss: -2.858554 [411648/1237259]
loss: -2.867506 [616448/1237259]
loss: -2.863151 [821248/1237259]
loss: -2.856601 [1026048/1237259]
loss: -2.853497 [1230848/1237259]
Epoch 129
-------------------------------
loss: -2.883689 [ 2048/1237259]
loss: -2.865887 [206848/1237259]
loss: -2.868797 [411648/1237259]
loss: -2.849616 [616448/1237259]
loss: -2.871521 [821248/1237259]
loss: -2.861311 [1026048/1237259]
loss: -2.855115 [1230848/1237259]
Epoch 130
-------------------------------
loss: -2.881643 [ 2048/1237259]
loss: -2.879087 [206848/1237259]
loss: -2.844476 [411648/1237259]
loss: -2.861698 [616448/1237259]
loss: -2.874395 [821248/1237259]
loss: -2.856451 [1026048/1237259]
loss: -2.861571 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0692  
ndcg@20: 0.0575  
diversity: 0.2050  


Epoch 131
-------------------------------
loss: -2.874226 [ 2048/1237259]
loss: -2.880043 [206848/1237259]
loss: -2.854164 [411648/1237259]
loss: -2.838665 [616448/1237259]
loss: -2.870717 [821248/1237259]
loss: -2.861095 [1026048/1237259]
loss: -2.874921 [1230848/1237259]
Epoch 132
-------------------------------
loss: -2.872240 [ 2048/1237259]
loss: -2.874498 [206848/1237259]
loss: -2.873524 [411648/1237259]
loss: -2.853655 [616448/1237259]
loss: -2.847948 [821248/1237259]
loss: -2.869049 [1026048/1237259]
loss: -2.855076 [1230848/1237259]
Epoch 133
-------------------------------
loss: -2.892930 [ 2048/1237259]
loss: -2.856125 [206848/1237259]
loss: -2.870637 [411648/1237259]
loss: -2.861311 [616448/1237259]
loss: -2.847841 [821248/1237259]
loss: -2.857938 [1026048/1237259]
loss: -2.859602 [1230848/1237259]
Epoch 134
-------------------------------
loss: -2.871101 [ 2048/1237259]
loss: -2.883048 [206848/1237259]
loss: -2.866256 [411648/1237259]
loss: -2.872506 [616448/1237259]
loss: -2.856866 [821248/1237259]
loss: -2.850774 [1026048/1237259]
loss: -2.852434 [1230848/1237259]
Epoch 135
-------------------------------
loss: -2.881720 [ 2048/1237259]
loss: -2.863515 [206848/1237259]
loss: -2.856747 [411648/1237259]
loss: -2.857560 [616448/1237259]
loss: -2.859746 [821248/1237259]
loss: -2.862483 [1026048/1237259]
loss: -2.863896 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0692  
ndcg@20: 0.0575  
diversity: 0.2051  


Epoch 136
-------------------------------
loss: -2.873683 [ 2048/1237259]
loss: -2.876736 [206848/1237259]
loss: -2.868590 [411648/1237259]
loss: -2.863080 [616448/1237259]
loss: -2.868145 [821248/1237259]
loss: -2.861709 [1026048/1237259]
loss: -2.853980 [1230848/1237259]
Epoch 137
-------------------------------
loss: -2.877996 [ 2048/1237259]
loss: -2.873933 [206848/1237259]
loss: -2.874210 [411648/1237259]
loss: -2.871036 [616448/1237259]
loss: -2.865517 [821248/1237259]
loss: -2.845057 [1026048/1237259]
loss: -2.866840 [1230848/1237259]
Epoch 138
-------------------------------
loss: -2.883461 [ 2048/1237259]
loss: -2.871309 [206848/1237259]
loss: -2.869528 [411648/1237259]
loss: -2.878291 [616448/1237259]
loss: -2.866461 [821248/1237259]
loss: -2.857526 [1026048/1237259]
loss: -2.841147 [1230848/1237259]
Epoch 139
-------------------------------
loss: -2.881500 [ 2048/1237259]
loss: -2.854009 [206848/1237259]
loss: -2.881037 [411648/1237259]
loss: -2.861017 [616448/1237259]
loss: -2.849139 [821248/1237259]
loss: -2.853087 [1026048/1237259]
loss: -2.864558 [1230848/1237259]
Epoch 140
-------------------------------
loss: -2.873184 [ 2048/1237259]
loss: -2.861794 [206848/1237259]
loss: -2.865412 [411648/1237259]
loss: -2.855230 [616448/1237259]
loss: -2.870247 [821248/1237259]
loss: -2.852697 [1026048/1237259]
loss: -2.854601 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0693  
ndcg@20: 0.0575  
diversity: 0.2051  


Epoch 141
-------------------------------
loss: -2.867590 [ 2048/1237259]
loss: -2.875236 [206848/1237259]
loss: -2.849592 [411648/1237259]
loss: -2.865235 [616448/1237259]
loss: -2.859620 [821248/1237259]
loss: -2.865714 [1026048/1237259]
loss: -2.870067 [1230848/1237259]
Epoch 142
-------------------------------
loss: -2.891726 [ 2048/1237259]
loss: -2.879203 [206848/1237259]
loss: -2.859657 [411648/1237259]
loss: -2.873067 [616448/1237259]
loss: -2.856939 [821248/1237259]
loss: -2.853528 [1026048/1237259]
loss: -2.865531 [1230848/1237259]
Epoch 143
-------------------------------
loss: -2.872554 [ 2048/1237259]
loss: -2.865433 [206848/1237259]
loss: -2.871766 [411648/1237259]
loss: -2.860714 [616448/1237259]
loss: -2.864187 [821248/1237259]
loss: -2.849763 [1026048/1237259]
loss: -2.868395 [1230848/1237259]
Epoch 144
-------------------------------
loss: -2.863537 [ 2048/1237259]
loss: -2.874815 [206848/1237259]
loss: -2.860655 [411648/1237259]
loss: -2.864863 [616448/1237259]
loss: -2.860316 [821248/1237259]
loss: -2.865992 [1026048/1237259]
loss: -2.872507 [1230848/1237259]
Epoch 145
-------------------------------
loss: -2.870724 [ 2048/1237259]
loss: -2.864996 [206848/1237259]
loss: -2.887695 [411648/1237259]
loss: -2.871234 [616448/1237259]
loss: -2.874263 [821248/1237259]
loss: -2.877439 [1026048/1237259]
loss: -2.853889 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0694  
ndcg@20: 0.0576  
diversity: 0.2051  


Epoch 146
-------------------------------
loss: -2.871617 [ 2048/1237259]
loss: -2.890081 [206848/1237259]
loss: -2.890284 [411648/1237259]
loss: -2.859525 [616448/1237259]
loss: -2.857442 [821248/1237259]
loss: -2.856293 [1026048/1237259]
loss: -2.856042 [1230848/1237259]
Epoch 147
-------------------------------
loss: -2.875557 [ 2048/1237259]
loss: -2.861276 [206848/1237259]
loss: -2.869865 [411648/1237259]
loss: -2.872553 [616448/1237259]
loss: -2.847813 [821248/1237259]
loss: -2.855479 [1026048/1237259]
loss: -2.857313 [1230848/1237259]
Epoch 148
-------------------------------
loss: -2.881068 [ 2048/1237259]
loss: -2.860482 [206848/1237259]
loss: -2.867135 [411648/1237259]
loss: -2.859955 [616448/1237259]
loss: -2.868864 [821248/1237259]
loss: -2.856926 [1026048/1237259]
loss: -2.862789 [1230848/1237259]
Epoch 149
-------------------------------
loss: -2.891601 [ 2048/1237259]
loss: -2.866710 [206848/1237259]
loss: -2.865921 [411648/1237259]
loss: -2.875177 [616448/1237259]
loss: -2.855097 [821248/1237259]
loss: -2.859760 [1026048/1237259]
loss: -2.856066 [1230848/1237259]
Epoch 150
-------------------------------
loss: -2.877709 [ 2048/1237259]
loss: -2.877713 [206848/1237259]
loss: -2.875464 [411648/1237259]
loss: -2.853381 [616448/1237259]
loss: -2.850593 [821248/1237259]
loss: -2.877520 [1026048/1237259]
loss: -2.875975 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0694  
ndcg@20: 0.0575  
diversity: 0.2050  


Epoch 151
-------------------------------
loss: -2.882560 [ 2048/1237259]
loss: -2.889585 [206848/1237259]
loss: -2.868864 [411648/1237259]
loss: -2.862218 [616448/1237259]
loss: -2.865611 [821248/1237259]
loss: -2.856424 [1026048/1237259]
loss: -2.854753 [1230848/1237259]
Epoch 152
-------------------------------
loss: -2.881562 [ 2048/1237259]
loss: -2.858243 [206848/1237259]
loss: -2.868297 [411648/1237259]
loss: -2.875256 [616448/1237259]
loss: -2.855273 [821248/1237259]
loss: -2.846285 [1026048/1237259]
loss: -2.856929 [1230848/1237259]
Epoch 153
-------------------------------
loss: -2.887566 [ 2048/1237259]
loss: -2.872986 [206848/1237259]
loss: -2.863372 [411648/1237259]
loss: -2.866247 [616448/1237259]
loss: -2.875046 [821248/1237259]
loss: -2.881682 [1026048/1237259]
loss: -2.851886 [1230848/1237259]
Epoch 154
-------------------------------
loss: -2.878141 [ 2048/1237259]
loss: -2.873155 [206848/1237259]
loss: -2.876569 [411648/1237259]
loss: -2.853485 [616448/1237259]
loss: -2.869833 [821248/1237259]
loss: -2.868472 [1026048/1237259]
loss: -2.851835 [1230848/1237259]
Epoch 155
-------------------------------
loss: -2.868581 [ 2048/1237259]
loss: -2.870186 [206848/1237259]
loss: -2.874779 [411648/1237259]
loss: -2.865417 [616448/1237259]
loss: -2.851490 [821248/1237259]
loss: -2.862349 [1026048/1237259]
loss: -2.864239 [1230848/1237259]
Eval results: 
recall@20: 0.0693  
ndcg@20: 0.0575  
diversity: 0.2051  


Epoch 156
-------------------------------
loss: -2.876937 [ 2048/1237259]
loss: -2.880731 [206848/1237259]
loss: -2.868532 [411648/1237259]
loss: -2.868017 [616448/1237259]
loss: -2.868821 [821248/1237259]
loss: -2.876333 [1026048/1237259]
loss: -2.840190 [1230848/1237259]
Epoch 157
-------------------------------
loss: -2.878431 [ 2048/1237259]
loss: -2.869011 [206848/1237259]
loss: -2.865563 [411648/1237259]
loss: -2.861718 [616448/1237259]
loss: -2.871770 [821248/1237259]
loss: -2.860116 [1026048/1237259]
loss: -2.851388 [1230848/1237259]
Epoch 158
-------------------------------
loss: -2.877590 [ 2048/1237259]
loss: -2.874545 [206848/1237259]
loss: -2.861547 [411648/1237259]
loss: -2.871828 [616448/1237259]
loss: -2.860794 [821248/1237259]
loss: -2.852386 [1026048/1237259]
loss: -2.851600 [1230848/1237259]
Epoch 159
-------------------------------
loss: -2.871646 [ 2048/1237259]
loss: -2.881326 [206848/1237259]
loss: -2.864120 [411648/1237259]
loss: -2.861639 [616448/1237259]
loss: -2.872882 [821248/1237259]
loss: -2.872843 [1026048/1237259]
loss: -2.856883 [1230848/1237259]
Epoch 160
-------------------------------
loss: -2.879067 [ 2048/1237259]
loss: -2.869113 [206848/1237259]
loss: -2.870704 [411648/1237259]
loss: -2.861442 [616448/1237259]
loss: -2.850120 [821248/1237259]
loss: -2.853077 [1026048/1237259]
loss: -2.846415 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0694  
ndcg@20: 0.0576  
diversity: 0.2050  


Epoch 161
-------------------------------
loss: -2.874908 [ 2048/1237259]
loss: -2.863550 [206848/1237259]
loss: -2.875272 [411648/1237259]
loss: -2.874383 [616448/1237259]
loss: -2.874295 [821248/1237259]
loss: -2.857084 [1026048/1237259]
loss: -2.867903 [1230848/1237259]
Epoch 162
-------------------------------
loss: -2.867449 [ 2048/1237259]
loss: -2.874286 [206848/1237259]
loss: -2.875829 [411648/1237259]
loss: -2.857978 [616448/1237259]
loss: -2.858972 [821248/1237259]
loss: -2.855087 [1026048/1237259]
loss: -2.865744 [1230848/1237259]
Epoch 163
-------------------------------
loss: -2.884133 [ 2048/1237259]
loss: -2.887282 [206848/1237259]
loss: -2.881192 [411648/1237259]
loss: -2.866641 [616448/1237259]
loss: -2.865030 [821248/1237259]
loss: -2.853118 [1026048/1237259]
loss: -2.849560 [1230848/1237259]
Epoch 164
-------------------------------
loss: -2.868669 [ 2048/1237259]
loss: -2.883385 [206848/1237259]
loss: -2.862762 [411648/1237259]
loss: -2.864340 [616448/1237259]
loss: -2.864948 [821248/1237259]
loss: -2.871573 [1026048/1237259]
loss: -2.854820 [1230848/1237259]
Epoch 165
-------------------------------
loss: -2.861875 [ 2048/1237259]
loss: -2.878498 [206848/1237259]
loss: -2.858417 [411648/1237259]
loss: -2.883138 [616448/1237259]
loss: -2.851741 [821248/1237259]
loss: -2.870042 [1026048/1237259]
loss: -2.872134 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0694  
ndcg@20: 0.0576  
diversity: 0.2050  


Epoch 166
-------------------------------
loss: -2.872645 [ 2048/1237259]
loss: -2.864259 [206848/1237259]
loss: -2.874380 [411648/1237259]
loss: -2.864174 [616448/1237259]
loss: -2.860126 [821248/1237259]
loss: -2.860719 [1026048/1237259]
loss: -2.856013 [1230848/1237259]
Epoch 167
-------------------------------
loss: -2.873224 [ 2048/1237259]
loss: -2.864775 [206848/1237259]
loss: -2.881387 [411648/1237259]
loss: -2.873125 [616448/1237259]
loss: -2.865809 [821248/1237259]
loss: -2.850878 [1026048/1237259]
loss: -2.845309 [1230848/1237259]
Epoch 168
-------------------------------
loss: -2.869340 [ 2048/1237259]
loss: -2.866257 [206848/1237259]
loss: -2.878909 [411648/1237259]
loss: -2.874547 [616448/1237259]
loss: -2.860796 [821248/1237259]
loss: -2.858662 [1026048/1237259]
loss: -2.869416 [1230848/1237259]
Epoch 169
-------------------------------
loss: -2.873163 [ 2048/1237259]
loss: -2.892852 [206848/1237259]
loss: -2.883247 [411648/1237259]
loss: -2.869667 [616448/1237259]
loss: -2.868015 [821248/1237259]
loss: -2.871611 [1026048/1237259]
loss: -2.851078 [1230848/1237259]
Epoch 170
-------------------------------
loss: -2.874598 [ 2048/1237259]
loss: -2.881475 [206848/1237259]
loss: -2.871019 [411648/1237259]
loss: -2.869171 [616448/1237259]
loss: -2.870180 [821248/1237259]
loss: -2.854061 [1026048/1237259]
loss: -2.855199 [1230848/1237259]
Eval results: 
recall@20: 0.0693  
ndcg@20: 0.0575  
diversity: 0.2049  


Epoch 171
-------------------------------
loss: -2.870439 [ 2048/1237259]
loss: -2.872458 [206848/1237259]
loss: -2.867434 [411648/1237259]
loss: -2.860087 [616448/1237259]
loss: -2.874128 [821248/1237259]
loss: -2.859778 [1026048/1237259]
loss: -2.864238 [1230848/1237259]
Epoch 172
-------------------------------
loss: -2.871816 [ 2048/1237259]
loss: -2.886321 [206848/1237259]
loss: -2.866673 [411648/1237259]
loss: -2.886036 [616448/1237259]
loss: -2.864542 [821248/1237259]
loss: -2.864621 [1026048/1237259]
loss: -2.846592 [1230848/1237259]
Epoch 173
-------------------------------
loss: -2.867724 [ 2048/1237259]
loss: -2.887229 [206848/1237259]
loss: -2.865932 [411648/1237259]
loss: -2.868428 [616448/1237259]
loss: -2.858107 [821248/1237259]
loss: -2.869457 [1026048/1237259]
loss: -2.867585 [1230848/1237259]
Epoch 174
-------------------------------
loss: -2.876092 [ 2048/1237259]
loss: -2.867848 [206848/1237259]
loss: -2.869102 [411648/1237259]
loss: -2.885985 [616448/1237259]
loss: -2.859993 [821248/1237259]
loss: -2.859826 [1026048/1237259]
loss: -2.859448 [1230848/1237259]
Epoch 175
-------------------------------
loss: -2.871351 [ 2048/1237259]
loss: -2.889134 [206848/1237259]
loss: -2.862828 [411648/1237259]
loss: -2.853106 [616448/1237259]
loss: -2.871667 [821248/1237259]
loss: -2.852232 [1026048/1237259]
loss: -2.854185 [1230848/1237259]
Eval results: 
recall@20: 0.0693  
ndcg@20: 0.0575  
diversity: 0.2049  


Epoch 176
-------------------------------
loss: -2.869991 [ 2048/1237259]
loss: -2.863910 [206848/1237259]
loss: -2.873802 [411648/1237259]
loss: -2.853784 [616448/1237259]
loss: -2.853470 [821248/1237259]
loss: -2.853229 [1026048/1237259]
loss: -2.861868 [1230848/1237259]
Epoch 177
-------------------------------
loss: -2.876315 [ 2048/1237259]
loss: -2.874153 [206848/1237259]
loss: -2.879884 [411648/1237259]
loss: -2.856849 [616448/1237259]
loss: -2.876617 [821248/1237259]
loss: -2.867082 [1026048/1237259]
loss: -2.872134 [1230848/1237259]
Epoch 178
-------------------------------
loss: -2.862298 [ 2048/1237259]
loss: -2.884284 [206848/1237259]
loss: -2.877510 [411648/1237259]
loss: -2.869597 [616448/1237259]
loss: -2.857317 [821248/1237259]
loss: -2.853251 [1026048/1237259]
loss: -2.861355 [1230848/1237259]
Epoch 179
-------------------------------
loss: -2.860823 [ 2048/1237259]
loss: -2.871755 [206848/1237259]
loss: -2.864932 [411648/1237259]
loss: -2.872780 [616448/1237259]
loss: -2.865110 [821248/1237259]
loss: -2.854175 [1026048/1237259]
loss: -2.839730 [1230848/1237259]
Epoch 180
-------------------------------
loss: -2.887522 [ 2048/1237259]
loss: -2.870682 [206848/1237259]
loss: -2.856041 [411648/1237259]
loss: -2.863841 [616448/1237259]
loss: -2.863509 [821248/1237259]
loss: -2.854046 [1026048/1237259]
loss: -2.875856 [1230848/1237259]
Eval results: 
recall@20: 0.0693  
ndcg@20: 0.0575  
diversity: 0.2050  


Epoch 181
-------------------------------
loss: -2.881095 [ 2048/1237259]
loss: -2.870870 [206848/1237259]
loss: -2.878453 [411648/1237259]
loss: -2.876553 [616448/1237259]
loss: -2.862109 [821248/1237259]
loss: -2.860942 [1026048/1237259]
loss: -2.856789 [1230848/1237259]
Epoch 182
-------------------------------
loss: -2.890862 [ 2048/1237259]
loss: -2.860535 [206848/1237259]
loss: -2.881817 [411648/1237259]
loss: -2.877768 [616448/1237259]
loss: -2.851967 [821248/1237259]
loss: -2.864241 [1026048/1237259]
loss: -2.854430 [1230848/1237259]
Epoch 183
-------------------------------
loss: -2.851321 [ 2048/1237259]
loss: -2.870289 [206848/1237259]
loss: -2.865365 [411648/1237259]
loss: -2.872080 [616448/1237259]
loss: -2.870852 [821248/1237259]
loss: -2.850655 [1026048/1237259]
loss: -2.876893 [1230848/1237259]
Epoch 184
-------------------------------
loss: -2.857787 [ 2048/1237259]
loss: -2.867608 [206848/1237259]
loss: -2.874395 [411648/1237259]
loss: -2.870790 [616448/1237259]
loss: -2.844947 [821248/1237259]
loss: -2.868338 [1026048/1237259]
loss: -2.860845 [1230848/1237259]
Epoch 185
-------------------------------
loss: -2.873508 [ 2048/1237259]
loss: -2.891734 [206848/1237259]
loss: -2.864505 [411648/1237259]
loss: -2.871053 [616448/1237259]
loss: -2.878750 [821248/1237259]
loss: -2.860137 [1026048/1237259]
loss: -2.870329 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0695  
ndcg@20: 0.0576  
diversity: 0.2049  


Epoch 186
-------------------------------
loss: -2.868293 [ 2048/1237259]
loss: -2.865040 [206848/1237259]
loss: -2.872413 [411648/1237259]
loss: -2.876463 [616448/1237259]
loss: -2.861633 [821248/1237259]
loss: -2.860375 [1026048/1237259]
loss: -2.860016 [1230848/1237259]
Epoch 187
-------------------------------
loss: -2.871631 [ 2048/1237259]
loss: -2.883713 [206848/1237259]
loss: -2.884586 [411648/1237259]
loss: -2.861164 [616448/1237259]
loss: -2.858870 [821248/1237259]
loss: -2.862484 [1026048/1237259]
loss: -2.851826 [1230848/1237259]
Epoch 188
-------------------------------
loss: -2.882842 [ 2048/1237259]
loss: -2.879636 [206848/1237259]
loss: -2.860349 [411648/1237259]
loss: -2.849884 [616448/1237259]
loss: -2.853332 [821248/1237259]
loss: -2.866625 [1026048/1237259]
loss: -2.866434 [1230848/1237259]
Epoch 189
-------------------------------
loss: -2.877000 [ 2048/1237259]
loss: -2.877075 [206848/1237259]
loss: -2.870124 [411648/1237259]
loss: -2.864450 [616448/1237259]
loss: -2.862490 [821248/1237259]
loss: -2.868782 [1026048/1237259]
loss: -2.856553 [1230848/1237259]
Epoch 190
-------------------------------
loss: -2.871288 [ 2048/1237259]
loss: -2.870785 [206848/1237259]
loss: -2.869834 [411648/1237259]
loss: -2.875789 [616448/1237259]
loss: -2.846386 [821248/1237259]
loss: -2.860963 [1026048/1237259]
loss: -2.863438 [1230848/1237259]
Eval results: 
recall@20: 0.0694  
ndcg@20: 0.0576  
diversity: 0.2049  


Epoch 191
-------------------------------
loss: -2.875537 [ 2048/1237259]
loss: -2.860857 [206848/1237259]
loss: -2.882348 [411648/1237259]
loss: -2.868245 [616448/1237259]
loss: -2.868712 [821248/1237259]
loss: -2.864114 [1026048/1237259]
loss: -2.863906 [1230848/1237259]
Epoch 192
-------------------------------
loss: -2.883906 [ 2048/1237259]
loss: -2.861382 [206848/1237259]
loss: -2.869649 [411648/1237259]
loss: -2.872157 [616448/1237259]
loss: -2.853988 [821248/1237259]
loss: -2.857976 [1026048/1237259]
loss: -2.846392 [1230848/1237259]
Epoch 193
-------------------------------
loss: -2.872408 [ 2048/1237259]
loss: -2.866698 [206848/1237259]
loss: -2.864649 [411648/1237259]
loss: -2.864418 [616448/1237259]
loss: -2.858883 [821248/1237259]
loss: -2.855686 [1026048/1237259]
loss: -2.855513 [1230848/1237259]
Epoch 194
-------------------------------
loss: -2.866428 [ 2048/1237259]
loss: -2.854437 [206848/1237259]
loss: -2.882804 [411648/1237259]
loss: -2.874369 [616448/1237259]
loss: -2.863541 [821248/1237259]
loss: -2.872323 [1026048/1237259]
loss: -2.850492 [1230848/1237259]
Epoch 195
-------------------------------
loss: -2.873953 [ 2048/1237259]
loss: -2.854833 [206848/1237259]
loss: -2.864937 [411648/1237259]
loss: -2.870000 [616448/1237259]
loss: -2.856301 [821248/1237259]
loss: -2.863493 [1026048/1237259]
loss: -2.861152 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0695  
ndcg@20: 0.0576  
diversity: 0.2049  


Epoch 196
-------------------------------
loss: -2.871617 [ 2048/1237259]
loss: -2.870425 [206848/1237259]
loss: -2.871336 [411648/1237259]
loss: -2.862260 [616448/1237259]
loss: -2.865729 [821248/1237259]
loss: -2.864275 [1026048/1237259]
loss: -2.862064 [1230848/1237259]
Epoch 197
-------------------------------
loss: -2.875347 [ 2048/1237259]
loss: -2.866305 [206848/1237259]
loss: -2.864606 [411648/1237259]
loss: -2.862127 [616448/1237259]
loss: -2.865833 [821248/1237259]
loss: -2.837451 [1026048/1237259]
loss: -2.864377 [1230848/1237259]
Epoch 198
-------------------------------
loss: -2.867243 [ 2048/1237259]
loss: -2.873748 [206848/1237259]
loss: -2.866374 [411648/1237259]
loss: -2.870760 [616448/1237259]
loss: -2.864079 [821248/1237259]
loss: -2.869218 [1026048/1237259]
loss: -2.855327 [1230848/1237259]
Epoch 199
-------------------------------
loss: -2.868746 [ 2048/1237259]
loss: -2.871728 [206848/1237259]
loss: -2.879531 [411648/1237259]
loss: -2.873940 [616448/1237259]
loss: -2.882904 [821248/1237259]
loss: -2.858758 [1026048/1237259]
loss: -2.845457 [1230848/1237259]
