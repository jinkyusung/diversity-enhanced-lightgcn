Folder created at: ./bpr2/best_model
Your Device: cuda
Yelp2018

#user = 31668
#item = 38048

#interactions
    (train) 1237259
    (test)  324147
    (total) 1561406

Sparsity = 0.0012958757851778645

Epoch 1
-------------------------------
loss: 0.693143 0.000001 [ 2048/1237259]
loss: 0.454275 0.000075 [206848/1237259]
loss: 0.260477 0.000224 [411648/1237259]
loss: 0.228655 0.000328 [616448/1237259]
loss: 0.188992 0.000392 [821248/1237259]
loss: 0.164395 0.000444 [1026048/1237259]
loss: 0.175284 0.000474 [1230848/1237259]
Epoch 2
-------------------------------
loss: 0.201341 0.000471 [ 2048/1237259]
loss: 0.151921 0.000495 [206848/1237259]
loss: 0.159083 0.000514 [411648/1237259]
loss: 0.151434 0.000527 [616448/1237259]
loss: 0.161650 0.000539 [821248/1237259]
loss: 0.160174 0.000551 [1026048/1237259]
loss: 0.149633 0.000555 [1230848/1237259]
Epoch 3
-------------------------------
loss: 0.158850 0.000553 [ 2048/1237259]
loss: 0.159009 0.000564 [206848/1237259]
loss: 0.165358 0.000570 [411648/1237259]
loss: 0.151793 0.000582 [616448/1237259]
loss: 0.167059 0.000585 [821248/1237259]
loss: 0.155976 0.000588 [1026048/1237259]
loss: 0.165106 0.000590 [1230848/1237259]
Epoch 4
-------------------------------
loss: 0.141252 0.000600 [ 2048/1237259]
loss: 0.133802 0.000594 [206848/1237259]
loss: 0.138237 0.000600 [411648/1237259]
loss: 0.139164 0.000615 [616448/1237259]
loss: 0.138503 0.000618 [821248/1237259]
loss: 0.159012 0.000620 [1026048/1237259]
loss: 0.153132 0.000628 [1230848/1237259]
Epoch 5
-------------------------------
loss: 0.148512 0.000627 [ 2048/1237259]
loss: 0.140827 0.000633 [206848/1237259]
loss: 0.126755 0.000632 [411648/1237259]
loss: 0.135884 0.000643 [616448/1237259]
loss: 0.157868 0.000645 [821248/1237259]
loss: 0.145186 0.000650 [1026048/1237259]
loss: 0.143802 0.000660 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0350  
ndcg@20: 0.0284  
diversity: 0.1624  


Epoch 6
-------------------------------
loss: 0.136274 0.000653 [ 2048/1237259]
loss: 0.136374 0.000674 [206848/1237259]
loss: 0.147758 0.000675 [411648/1237259]
loss: 0.152690 0.000684 [616448/1237259]
loss: 0.157118 0.000692 [821248/1237259]
loss: 0.148199 0.000696 [1026048/1237259]
loss: 0.135280 0.000704 [1230848/1237259]
Epoch 7
-------------------------------
loss: 0.139540 0.000699 [ 2048/1237259]
loss: 0.144430 0.000699 [206848/1237259]
loss: 0.136370 0.000713 [411648/1237259]
loss: 0.158527 0.000716 [616448/1237259]
loss: 0.139524 0.000740 [821248/1237259]
loss: 0.130347 0.000733 [1026048/1237259]
loss: 0.121155 0.000754 [1230848/1237259]
Epoch 8
-------------------------------
loss: 0.131785 0.000754 [ 2048/1237259]
loss: 0.143651 0.000747 [206848/1237259]
loss: 0.130456 0.000752 [411648/1237259]
loss: 0.135050 0.000777 [616448/1237259]
loss: 0.133143 0.000789 [821248/1237259]
loss: 0.137658 0.000791 [1026048/1237259]
loss: 0.127810 0.000795 [1230848/1237259]
Epoch 9
-------------------------------
loss: 0.124658 0.000806 [ 2048/1237259]
loss: 0.130463 0.000817 [206848/1237259]
loss: 0.144162 0.000819 [411648/1237259]
loss: 0.133028 0.000824 [616448/1237259]
loss: 0.128479 0.000832 [821248/1237259]
loss: 0.117824 0.000851 [1026048/1237259]
loss: 0.128732 0.000862 [1230848/1237259]
Epoch 10
-------------------------------
loss: 0.114878 0.000856 [ 2048/1237259]
loss: 0.124984 0.000873 [206848/1237259]
loss: 0.139499 0.000872 [411648/1237259]
loss: 0.128994 0.000891 [616448/1237259]
loss: 0.129710 0.000896 [821248/1237259]
loss: 0.134485 0.000913 [1026048/1237259]
loss: 0.123598 0.000909 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0385  
ndcg@20: 0.0310  
diversity: 0.1641  


Epoch 11
-------------------------------
loss: 0.107420 0.000921 [ 2048/1237259]
loss: 0.115482 0.000935 [206848/1237259]
loss: 0.102566 0.000948 [411648/1237259]
loss: 0.124426 0.000959 [616448/1237259]
loss: 0.118186 0.000984 [821248/1237259]
loss: 0.126941 0.000996 [1026048/1237259]
loss: 0.116563 0.000992 [1230848/1237259]
Epoch 12
-------------------------------
loss: 0.142716 0.000982 [ 2048/1237259]
loss: 0.114045 0.001002 [206848/1237259]
loss: 0.120119 0.001036 [411648/1237259]
loss: 0.127204 0.001037 [616448/1237259]
loss: 0.121837 0.001036 [821248/1237259]
loss: 0.111935 0.001035 [1026048/1237259]
loss: 0.129098 0.001079 [1230848/1237259]
Epoch 13
-------------------------------
loss: 0.118267 0.001072 [ 2048/1237259]
loss: 0.111103 0.001084 [206848/1237259]
loss: 0.098780 0.001076 [411648/1237259]
loss: 0.112661 0.001091 [616448/1237259]
loss: 0.117324 0.001099 [821248/1237259]
loss: 0.117506 0.001119 [1026048/1237259]
loss: 0.094804 0.001132 [1230848/1237259]
Epoch 14
-------------------------------
loss: 0.107144 0.001139 [ 2048/1237259]
loss: 0.123024 0.001159 [206848/1237259]
loss: 0.105878 0.001158 [411648/1237259]
loss: 0.121617 0.001165 [616448/1237259]
loss: 0.102176 0.001166 [821248/1237259]
loss: 0.108539 0.001202 [1026048/1237259]
loss: 0.101365 0.001184 [1230848/1237259]
Epoch 15
-------------------------------
loss: 0.105667 0.001199 [ 2048/1237259]
loss: 0.103869 0.001200 [206848/1237259]
loss: 0.111781 0.001212 [411648/1237259]
loss: 0.101009 0.001237 [616448/1237259]
loss: 0.103820 0.001245 [821248/1237259]
loss: 0.112502 0.001246 [1026048/1237259]
loss: 0.095873 0.001248 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0408  
ndcg@20: 0.0328  
diversity: 0.1663  


Epoch 16
-------------------------------
loss: 0.101313 0.001248 [ 2048/1237259]
loss: 0.114784 0.001299 [206848/1237259]
loss: 0.103677 0.001272 [411648/1237259]
loss: 0.105855 0.001270 [616448/1237259]
loss: 0.109055 0.001304 [821248/1237259]
loss: 0.109542 0.001308 [1026048/1237259]
loss: 0.112576 0.001310 [1230848/1237259]
Epoch 17
-------------------------------
loss: 0.096832 0.001346 [ 2048/1237259]
loss: 0.094809 0.001327 [206848/1237259]
loss: 0.117735 0.001343 [411648/1237259]
loss: 0.096796 0.001368 [616448/1237259]
loss: 0.091612 0.001354 [821248/1237259]
loss: 0.103224 0.001362 [1026048/1237259]
loss: 0.087091 0.001362 [1230848/1237259]
Epoch 18
-------------------------------
loss: 0.089006 0.001387 [ 2048/1237259]
loss: 0.111233 0.001383 [206848/1237259]
loss: 0.088984 0.001375 [411648/1237259]
loss: 0.106577 0.001412 [616448/1237259]
loss: 0.088945 0.001419 [821248/1237259]
loss: 0.101939 0.001428 [1026048/1237259]
loss: 0.109253 0.001419 [1230848/1237259]
Epoch 19
-------------------------------
loss: 0.100124 0.001424 [ 2048/1237259]
loss: 0.108862 0.001421 [206848/1237259]
loss: 0.101021 0.001456 [411648/1237259]
loss: 0.116275 0.001471 [616448/1237259]
loss: 0.106325 0.001482 [821248/1237259]
loss: 0.094178 0.001460 [1026048/1237259]
loss: 0.109609 0.001510 [1230848/1237259]
Epoch 20
-------------------------------
loss: 0.093597 0.001527 [ 2048/1237259]
loss: 0.101683 0.001539 [206848/1237259]
loss: 0.095214 0.001506 [411648/1237259]
loss: 0.100608 0.001507 [616448/1237259]
loss: 0.094734 0.001531 [821248/1237259]
loss: 0.100737 0.001515 [1026048/1237259]
loss: 0.100885 0.001557 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0416  
ndcg@20: 0.0334  
diversity: 0.1673  


Epoch 21
-------------------------------
loss: 0.100391 0.001568 [ 2048/1237259]
loss: 0.086614 0.001564 [206848/1237259]
loss: 0.086526 0.001552 [411648/1237259]
loss: 0.096836 0.001563 [616448/1237259]
loss: 0.091455 0.001567 [821248/1237259]
loss: 0.095769 0.001598 [1026048/1237259]
loss: 0.104187 0.001599 [1230848/1237259]
Epoch 22
-------------------------------
loss: 0.103267 0.001598 [ 2048/1237259]
loss: 0.095458 0.001617 [206848/1237259]
loss: 0.100085 0.001660 [411648/1237259]
loss: 0.087231 0.001624 [616448/1237259]
loss: 0.089836 0.001663 [821248/1237259]
loss: 0.095971 0.001645 [1026048/1237259]
loss: 0.083751 0.001693 [1230848/1237259]
Epoch 23
-------------------------------
loss: 0.086630 0.001675 [ 2048/1237259]
loss: 0.087306 0.001667 [206848/1237259]
loss: 0.090364 0.001666 [411648/1237259]
loss: 0.099699 0.001681 [616448/1237259]
loss: 0.112095 0.001703 [821248/1237259]
loss: 0.105015 0.001701 [1026048/1237259]
loss: 0.098885 0.001701 [1230848/1237259]
Epoch 24
-------------------------------
loss: 0.099739 0.001698 [ 2048/1237259]
loss: 0.085883 0.001725 [206848/1237259]
loss: 0.090066 0.001709 [411648/1237259]
loss: 0.105864 0.001781 [616448/1237259]
loss: 0.105308 0.001756 [821248/1237259]
loss: 0.106692 0.001784 [1026048/1237259]
loss: 0.085531 0.001773 [1230848/1237259]
Epoch 25
-------------------------------
loss: 0.096631 0.001774 [ 2048/1237259]
loss: 0.098626 0.001772 [206848/1237259]
loss: 0.096085 0.001780 [411648/1237259]
loss: 0.096811 0.001835 [616448/1237259]
loss: 0.080857 0.001792 [821248/1237259]
loss: 0.088795 0.001803 [1026048/1237259]
loss: 0.093955 0.001845 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0431  
ndcg@20: 0.0347  
diversity: 0.1682  


Epoch 26
-------------------------------
loss: 0.095540 0.001820 [ 2048/1237259]
loss: 0.095408 0.001841 [206848/1237259]
loss: 0.083515 0.001872 [411648/1237259]
loss: 0.086819 0.001852 [616448/1237259]
loss: 0.092028 0.001889 [821248/1237259]
loss: 0.088326 0.001872 [1026048/1237259]
loss: 0.081931 0.001880 [1230848/1237259]
Epoch 27
-------------------------------
loss: 0.094352 0.001861 [ 2048/1237259]
loss: 0.076569 0.001904 [206848/1237259]
loss: 0.085940 0.001927 [411648/1237259]
loss: 0.094082 0.001959 [616448/1237259]
loss: 0.098926 0.001922 [821248/1237259]
loss: 0.081818 0.001912 [1026048/1237259]
loss: 0.076486 0.001957 [1230848/1237259]
Epoch 28
-------------------------------
loss: 0.086186 0.001937 [ 2048/1237259]
loss: 0.084309 0.001982 [206848/1237259]
loss: 0.085010 0.001958 [411648/1237259]
loss: 0.086854 0.001980 [616448/1237259]
loss: 0.089225 0.002013 [821248/1237259]
loss: 0.083555 0.002007 [1026048/1237259]
loss: 0.094760 0.002051 [1230848/1237259]
Epoch 29
-------------------------------
loss: 0.096786 0.002003 [ 2048/1237259]
loss: 0.102680 0.002010 [206848/1237259]
loss: 0.072475 0.001979 [411648/1237259]
loss: 0.080280 0.002023 [616448/1237259]
loss: 0.080521 0.002061 [821248/1237259]
loss: 0.073799 0.002054 [1026048/1237259]
loss: 0.074736 0.002105 [1230848/1237259]
Epoch 30
-------------------------------
loss: 0.084254 0.002091 [ 2048/1237259]
loss: 0.091649 0.002090 [206848/1237259]
loss: 0.073241 0.002100 [411648/1237259]
loss: 0.081064 0.002099 [616448/1237259]
loss: 0.083326 0.002107 [821248/1237259]
loss: 0.080070 0.002120 [1026048/1237259]
loss: 0.092052 0.002129 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0441  
ndcg@20: 0.0357  
diversity: 0.1695  


Epoch 31
-------------------------------
loss: 0.077754 0.002179 [ 2048/1237259]
loss: 0.079860 0.002146 [206848/1237259]
loss: 0.077833 0.002158 [411648/1237259]
loss: 0.079409 0.002138 [616448/1237259]
loss: 0.081997 0.002171 [821248/1237259]
loss: 0.082368 0.002165 [1026048/1237259]
loss: 0.092605 0.002145 [1230848/1237259]
Epoch 32
-------------------------------
loss: 0.094184 0.002214 [ 2048/1237259]
loss: 0.087656 0.002180 [206848/1237259]
loss: 0.071460 0.002196 [411648/1237259]
loss: 0.089953 0.002193 [616448/1237259]
loss: 0.072493 0.002265 [821248/1237259]
loss: 0.081281 0.002256 [1026048/1237259]
loss: 0.096636 0.002243 [1230848/1237259]
Epoch 33
-------------------------------
loss: 0.079939 0.002205 [ 2048/1237259]
loss: 0.080367 0.002238 [206848/1237259]
loss: 0.077653 0.002299 [411648/1237259]
loss: 0.082013 0.002276 [616448/1237259]
loss: 0.065075 0.002271 [821248/1237259]
loss: 0.088104 0.002308 [1026048/1237259]
loss: 0.076629 0.002296 [1230848/1237259]
Epoch 34
-------------------------------
loss: 0.076563 0.002327 [ 2048/1237259]
loss: 0.083795 0.002328 [206848/1237259]
loss: 0.080859 0.002331 [411648/1237259]
loss: 0.079343 0.002339 [616448/1237259]
loss: 0.063786 0.002372 [821248/1237259]
loss: 0.081989 0.002361 [1026048/1237259]
loss: 0.075864 0.002374 [1230848/1237259]
Epoch 35
-------------------------------
loss: 0.075892 0.002339 [ 2048/1237259]
loss: 0.078832 0.002382 [206848/1237259]
loss: 0.071646 0.002394 [411648/1237259]
loss: 0.070341 0.002370 [616448/1237259]
loss: 0.069611 0.002371 [821248/1237259]
loss: 0.080460 0.002419 [1026048/1237259]
loss: 0.071016 0.002440 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0451  
ndcg@20: 0.0366  
diversity: 0.1707  


Epoch 36
-------------------------------
loss: 0.075224 0.002356 [ 2048/1237259]
loss: 0.080146 0.002469 [206848/1237259]
loss: 0.069970 0.002423 [411648/1237259]
loss: 0.086267 0.002423 [616448/1237259]
loss: 0.073334 0.002414 [821248/1237259]
loss: 0.078966 0.002506 [1026048/1237259]
loss: 0.073064 0.002476 [1230848/1237259]
Epoch 37
-------------------------------
loss: 0.087284 0.002477 [ 2048/1237259]
loss: 0.079234 0.002486 [206848/1237259]
loss: 0.070439 0.002469 [411648/1237259]
loss: 0.075747 0.002473 [616448/1237259]
loss: 0.081310 0.002515 [821248/1237259]
loss: 0.081770 0.002479 [1026048/1237259]
loss: 0.081390 0.002504 [1230848/1237259]
Epoch 38
-------------------------------
loss: 0.068383 0.002544 [ 2048/1237259]
loss: 0.081954 0.002524 [206848/1237259]
loss: 0.075077 0.002543 [411648/1237259]
loss: 0.071624 0.002530 [616448/1237259]
loss: 0.082912 0.002586 [821248/1237259]
loss: 0.078995 0.002563 [1026048/1237259]
loss: 0.079057 0.002519 [1230848/1237259]
Epoch 39
-------------------------------
loss: 0.068989 0.002594 [ 2048/1237259]
loss: 0.072871 0.002573 [206848/1237259]
loss: 0.067092 0.002596 [411648/1237259]
loss: 0.068905 0.002606 [616448/1237259]
loss: 0.072344 0.002611 [821248/1237259]
loss: 0.071658 0.002643 [1026048/1237259]
loss: 0.075144 0.002684 [1230848/1237259]
Epoch 40
-------------------------------
loss: 0.074513 0.002666 [ 2048/1237259]
loss: 0.068286 0.002668 [206848/1237259]
loss: 0.076622 0.002650 [411648/1237259]
loss: 0.079326 0.002631 [616448/1237259]
loss: 0.068662 0.002627 [821248/1237259]
loss: 0.069575 0.002694 [1026048/1237259]
loss: 0.096573 0.002686 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0457  
ndcg@20: 0.0371  
diversity: 0.1716  


Epoch 41
-------------------------------
loss: 0.078909 0.002688 [ 2048/1237259]
loss: 0.077254 0.002679 [206848/1237259]
loss: 0.079684 0.002672 [411648/1237259]
loss: 0.072186 0.002702 [616448/1237259]
loss: 0.072724 0.002759 [821248/1237259]
loss: 0.071705 0.002734 [1026048/1237259]
loss: 0.077082 0.002741 [1230848/1237259]
Epoch 42
-------------------------------
loss: 0.073948 0.002721 [ 2048/1237259]
loss: 0.061049 0.002733 [206848/1237259]
loss: 0.062481 0.002731 [411648/1237259]
loss: 0.076518 0.002781 [616448/1237259]
loss: 0.077077 0.002795 [821248/1237259]
loss: 0.081580 0.002753 [1026048/1237259]
loss: 0.064196 0.002772 [1230848/1237259]
Epoch 43
-------------------------------
loss: 0.079779 0.002779 [ 2048/1237259]
loss: 0.067166 0.002792 [206848/1237259]
loss: 0.086662 0.002773 [411648/1237259]
loss: 0.066954 0.002855 [616448/1237259]
loss: 0.071029 0.002781 [821248/1237259]
loss: 0.067587 0.002823 [1026048/1237259]
loss: 0.066991 0.002836 [1230848/1237259]
Epoch 44
-------------------------------
loss: 0.067476 0.002858 [ 2048/1237259]
loss: 0.069247 0.002836 [206848/1237259]
loss: 0.060363 0.002841 [411648/1237259]
loss: 0.069820 0.002851 [616448/1237259]
loss: 0.059532 0.002848 [821248/1237259]
loss: 0.079815 0.002869 [1026048/1237259]
loss: 0.066332 0.002846 [1230848/1237259]
Epoch 45
-------------------------------
loss: 0.064313 0.002846 [ 2048/1237259]
loss: 0.070685 0.002846 [206848/1237259]
loss: 0.059285 0.002878 [411648/1237259]
loss: 0.070361 0.002902 [616448/1237259]
loss: 0.065881 0.002883 [821248/1237259]
loss: 0.071493 0.002931 [1026048/1237259]
loss: 0.069194 0.002908 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0467  
ndcg@20: 0.0378  
diversity: 0.1726  


Epoch 46
-------------------------------
loss: 0.073836 0.002965 [ 2048/1237259]
loss: 0.078025 0.002933 [206848/1237259]
loss: 0.070774 0.002959 [411648/1237259]
loss: 0.067223 0.002939 [616448/1237259]
loss: 0.064662 0.002988 [821248/1237259]
loss: 0.058456 0.002947 [1026048/1237259]
loss: 0.076376 0.002973 [1230848/1237259]
Epoch 47
-------------------------------
loss: 0.069289 0.002949 [ 2048/1237259]
loss: 0.076616 0.002954 [206848/1237259]
loss: 0.075706 0.003006 [411648/1237259]
loss: 0.067059 0.003013 [616448/1237259]
loss: 0.066886 0.003097 [821248/1237259]
loss: 0.077830 0.002987 [1026048/1237259]
loss: 0.060219 0.003027 [1230848/1237259]
Epoch 48
-------------------------------
loss: 0.065672 0.003051 [ 2048/1237259]
loss: 0.066957 0.003074 [206848/1237259]
loss: 0.068773 0.003030 [411648/1237259]
loss: 0.064775 0.003063 [616448/1237259]
loss: 0.067042 0.003018 [821248/1237259]
loss: 0.056476 0.003021 [1026048/1237259]
loss: 0.075180 0.003097 [1230848/1237259]
Epoch 49
-------------------------------
loss: 0.054804 0.003096 [ 2048/1237259]
loss: 0.063835 0.003087 [206848/1237259]
loss: 0.068538 0.003092 [411648/1237259]
loss: 0.064299 0.003069 [616448/1237259]
loss: 0.060549 0.003128 [821248/1237259]
loss: 0.065401 0.003156 [1026048/1237259]
loss: 0.074848 0.003150 [1230848/1237259]
Epoch 50
-------------------------------
loss: 0.075942 0.003068 [ 2048/1237259]
loss: 0.064188 0.003181 [206848/1237259]
loss: 0.070015 0.003102 [411648/1237259]
loss: 0.075598 0.003162 [616448/1237259]
loss: 0.060454 0.003186 [821248/1237259]
loss: 0.061703 0.003190 [1026048/1237259]
loss: 0.061149 0.003164 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0476  
ndcg@20: 0.0386  
diversity: 0.1735  


Epoch 51
-------------------------------
loss: 0.063454 0.003139 [ 2048/1237259]
loss: 0.063028 0.003181 [206848/1237259]
loss: 0.065983 0.003160 [411648/1237259]
loss: 0.071914 0.003187 [616448/1237259]
loss: 0.058986 0.003154 [821248/1237259]
loss: 0.067528 0.003229 [1026048/1237259]
loss: 0.063121 0.003155 [1230848/1237259]
Epoch 52
-------------------------------
loss: 0.068060 0.003229 [ 2048/1237259]
loss: 0.071946 0.003215 [206848/1237259]
loss: 0.063771 0.003222 [411648/1237259]
loss: 0.063359 0.003206 [616448/1237259]
loss: 0.056405 0.003273 [821248/1237259]
loss: 0.076312 0.003220 [1026048/1237259]
loss: 0.067475 0.003251 [1230848/1237259]
Epoch 53
-------------------------------
loss: 0.067075 0.003286 [ 2048/1237259]
loss: 0.062203 0.003228 [206848/1237259]
loss: 0.074127 0.003308 [411648/1237259]
loss: 0.073871 0.003295 [616448/1237259]
loss: 0.068040 0.003273 [821248/1237259]
loss: 0.069940 0.003284 [1026048/1237259]
loss: 0.062677 0.003292 [1230848/1237259]
Epoch 54
-------------------------------
loss: 0.060016 0.003352 [ 2048/1237259]
loss: 0.055817 0.003347 [206848/1237259]
loss: 0.059088 0.003235 [411648/1237259]
loss: 0.046988 0.003314 [616448/1237259]
loss: 0.067575 0.003314 [821248/1237259]
loss: 0.062135 0.003290 [1026048/1237259]
loss: 0.063994 0.003345 [1230848/1237259]
Epoch 55
-------------------------------
loss: 0.060494 0.003344 [ 2048/1237259]
loss: 0.066736 0.003296 [206848/1237259]
loss: 0.064269 0.003382 [411648/1237259]
loss: 0.071305 0.003378 [616448/1237259]
loss: 0.071032 0.003354 [821248/1237259]
loss: 0.083471 0.003392 [1026048/1237259]
loss: 0.061052 0.003353 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0487  
ndcg@20: 0.0394  
diversity: 0.1742  


Epoch 56
-------------------------------
loss: 0.064242 0.003391 [ 2048/1237259]
loss: 0.066681 0.003408 [206848/1237259]
loss: 0.064578 0.003466 [411648/1237259]
loss: 0.057284 0.003396 [616448/1237259]
loss: 0.062607 0.003372 [821248/1237259]
loss: 0.072721 0.003434 [1026048/1237259]
loss: 0.060674 0.003443 [1230848/1237259]
Epoch 57
-------------------------------
loss: 0.063422 0.003400 [ 2048/1237259]
loss: 0.066082 0.003467 [206848/1237259]
loss: 0.060984 0.003453 [411648/1237259]
loss: 0.060573 0.003448 [616448/1237259]
loss: 0.069226 0.003418 [821248/1237259]
loss: 0.069743 0.003461 [1026048/1237259]
loss: 0.054778 0.003447 [1230848/1237259]
Epoch 58
-------------------------------
loss: 0.060864 0.003509 [ 2048/1237259]
loss: 0.063748 0.003524 [206848/1237259]
loss: 0.059842 0.003532 [411648/1237259]
loss: 0.066276 0.003464 [616448/1237259]
loss: 0.051217 0.003471 [821248/1237259]
loss: 0.057861 0.003504 [1026048/1237259]
loss: 0.057342 0.003542 [1230848/1237259]
Epoch 59
-------------------------------
loss: 0.055639 0.003506 [ 2048/1237259]
loss: 0.057151 0.003505 [206848/1237259]
loss: 0.060302 0.003506 [411648/1237259]
loss: 0.064232 0.003493 [616448/1237259]
loss: 0.054829 0.003514 [821248/1237259]
loss: 0.059736 0.003548 [1026048/1237259]
loss: 0.053546 0.003544 [1230848/1237259]
Epoch 60
-------------------------------
loss: 0.064687 0.003587 [ 2048/1237259]
loss: 0.060001 0.003570 [206848/1237259]
loss: 0.068148 0.003542 [411648/1237259]
loss: 0.046418 0.003584 [616448/1237259]
loss: 0.062288 0.003563 [821248/1237259]
loss: 0.065165 0.003554 [1026048/1237259]
loss: 0.045369 0.003602 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0496  
ndcg@20: 0.0402  
diversity: 0.1751  


Epoch 61
-------------------------------
loss: 0.061044 0.003559 [ 2048/1237259]
loss: 0.062434 0.003649 [206848/1237259]
loss: 0.060342 0.003559 [411648/1237259]
loss: 0.056034 0.003570 [616448/1237259]
loss: 0.076842 0.003624 [821248/1237259]
loss: 0.059822 0.003680 [1026048/1237259]
loss: 0.058659 0.003578 [1230848/1237259]
Epoch 62
-------------------------------
loss: 0.063860 0.003617 [ 2048/1237259]
loss: 0.050001 0.003665 [206848/1237259]
loss: 0.055296 0.003620 [411648/1237259]
loss: 0.057585 0.003671 [616448/1237259]
loss: 0.056338 0.003688 [821248/1237259]
loss: 0.069264 0.003632 [1026048/1237259]
loss: 0.069105 0.003671 [1230848/1237259]
Epoch 63
-------------------------------
loss: 0.065598 0.003670 [ 2048/1237259]
loss: 0.058830 0.003704 [206848/1237259]
loss: 0.046941 0.003711 [411648/1237259]
loss: 0.062693 0.003708 [616448/1237259]
loss: 0.054710 0.003736 [821248/1237259]
loss: 0.053715 0.003714 [1026048/1237259]
loss: 0.074873 0.003773 [1230848/1237259]
Epoch 64
-------------------------------
loss: 0.059153 0.003739 [ 2048/1237259]
loss: 0.058327 0.003706 [206848/1237259]
loss: 0.054762 0.003781 [411648/1237259]
loss: 0.067115 0.003714 [616448/1237259]
loss: 0.070266 0.003697 [821248/1237259]
loss: 0.060064 0.003763 [1026048/1237259]
loss: 0.055404 0.003754 [1230848/1237259]
Epoch 65
-------------------------------
loss: 0.054703 0.003764 [ 2048/1237259]
loss: 0.055088 0.003808 [206848/1237259]
loss: 0.062778 0.003770 [411648/1237259]
loss: 0.055572 0.003767 [616448/1237259]
loss: 0.069964 0.003736 [821248/1237259]
loss: 0.055527 0.003812 [1026048/1237259]
loss: 0.070734 0.003758 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0505  
ndcg@20: 0.0409  
diversity: 0.1759  


Epoch 66
-------------------------------
loss: 0.072275 0.003796 [ 2048/1237259]
loss: 0.059671 0.003773 [206848/1237259]
loss: 0.052650 0.003807 [411648/1237259]
loss: 0.065334 0.003782 [616448/1237259]
loss: 0.064131 0.003811 [821248/1237259]
loss: 0.067913 0.003833 [1026048/1237259]
loss: 0.065797 0.003845 [1230848/1237259]
Epoch 67
-------------------------------
loss: 0.069351 0.003861 [ 2048/1237259]
loss: 0.058736 0.003879 [206848/1237259]
loss: 0.050378 0.003826 [411648/1237259]
loss: 0.054860 0.003830 [616448/1237259]
loss: 0.054681 0.003863 [821248/1237259]
loss: 0.065483 0.003845 [1026048/1237259]
loss: 0.054424 0.003877 [1230848/1237259]
Epoch 68
-------------------------------
loss: 0.060934 0.003885 [ 2048/1237259]
loss: 0.066609 0.003870 [206848/1237259]
loss: 0.055609 0.003894 [411648/1237259]
loss: 0.057826 0.003973 [616448/1237259]
loss: 0.052833 0.003969 [821248/1237259]
loss: 0.062443 0.003858 [1026048/1237259]
loss: 0.055391 0.003915 [1230848/1237259]
Epoch 69
-------------------------------
loss: 0.053962 0.003940 [ 2048/1237259]
loss: 0.053829 0.003885 [206848/1237259]
loss: 0.061071 0.003932 [411648/1237259]
loss: 0.059348 0.003929 [616448/1237259]
loss: 0.059785 0.003960 [821248/1237259]
loss: 0.063194 0.003919 [1026048/1237259]
loss: 0.067149 0.003930 [1230848/1237259]
Epoch 70
-------------------------------
loss: 0.069826 0.003926 [ 2048/1237259]
loss: 0.048106 0.004002 [206848/1237259]
loss: 0.066057 0.004005 [411648/1237259]
loss: 0.067478 0.003994 [616448/1237259]
loss: 0.059759 0.003965 [821248/1237259]
loss: 0.058852 0.003914 [1026048/1237259]
loss: 0.053560 0.004002 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0515  
ndcg@20: 0.0417  
diversity: 0.1766  


Epoch 71
-------------------------------
loss: 0.060404 0.003985 [ 2048/1237259]
loss: 0.053682 0.003981 [206848/1237259]
loss: 0.055270 0.003977 [411648/1237259]
loss: 0.063284 0.004046 [616448/1237259]
loss: 0.058963 0.003969 [821248/1237259]
loss: 0.054250 0.004054 [1026048/1237259]
loss: 0.060528 0.003980 [1230848/1237259]
Epoch 72
-------------------------------
loss: 0.068360 0.004050 [ 2048/1237259]
loss: 0.062594 0.004059 [206848/1237259]
loss: 0.056842 0.004015 [411648/1237259]
loss: 0.058522 0.004004 [616448/1237259]
loss: 0.055493 0.003994 [821248/1237259]
loss: 0.050154 0.003948 [1026048/1237259]
loss: 0.057315 0.004040 [1230848/1237259]
Epoch 73
-------------------------------
loss: 0.058980 0.004140 [ 2048/1237259]
loss: 0.055996 0.004061 [206848/1237259]
loss: 0.053848 0.004156 [411648/1237259]
loss: 0.062482 0.004097 [616448/1237259]
loss: 0.046351 0.004112 [821248/1237259]
loss: 0.055404 0.004072 [1026048/1237259]
loss: 0.061044 0.004071 [1230848/1237259]
Epoch 74
-------------------------------
loss: 0.058164 0.004078 [ 2048/1237259]
loss: 0.052309 0.004115 [206848/1237259]
loss: 0.058910 0.004098 [411648/1237259]
loss: 0.059421 0.004126 [616448/1237259]
loss: 0.045646 0.004137 [821248/1237259]
loss: 0.057848 0.004137 [1026048/1237259]
loss: 0.053604 0.004111 [1230848/1237259]
Epoch 75
-------------------------------
loss: 0.051577 0.004143 [ 2048/1237259]
loss: 0.057281 0.004060 [206848/1237259]
loss: 0.046753 0.004123 [411648/1237259]
loss: 0.056482 0.004156 [616448/1237259]
loss: 0.057408 0.004203 [821248/1237259]
loss: 0.060269 0.004135 [1026048/1237259]
loss: 0.048817 0.004161 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0521  
ndcg@20: 0.0422  
diversity: 0.1774  


Epoch 76
-------------------------------
loss: 0.052493 0.004096 [ 2048/1237259]
loss: 0.060097 0.004144 [206848/1237259]
loss: 0.066508 0.004183 [411648/1237259]
loss: 0.059730 0.004152 [616448/1237259]
loss: 0.057337 0.004203 [821248/1237259]
loss: 0.063611 0.004200 [1026048/1237259]
loss: 0.058637 0.004234 [1230848/1237259]
Epoch 77
-------------------------------
loss: 0.052152 0.004279 [ 2048/1237259]
loss: 0.053450 0.004184 [206848/1237259]
loss: 0.057031 0.004201 [411648/1237259]
loss: 0.054215 0.004241 [616448/1237259]
loss: 0.056025 0.004246 [821248/1237259]
loss: 0.068332 0.004265 [1026048/1237259]
loss: 0.060809 0.004208 [1230848/1237259]
Epoch 78
-------------------------------
loss: 0.050295 0.004274 [ 2048/1237259]
loss: 0.061046 0.004318 [206848/1237259]
loss: 0.058922 0.004276 [411648/1237259]
loss: 0.050245 0.004272 [616448/1237259]
loss: 0.056979 0.004253 [821248/1237259]
loss: 0.052153 0.004217 [1026048/1237259]
loss: 0.048891 0.004363 [1230848/1237259]
Epoch 79
-------------------------------
loss: 0.056389 0.004276 [ 2048/1237259]
loss: 0.055523 0.004300 [206848/1237259]
loss: 0.057281 0.004307 [411648/1237259]
loss: 0.058814 0.004273 [616448/1237259]
loss: 0.047300 0.004245 [821248/1237259]
loss: 0.060561 0.004310 [1026048/1237259]
loss: 0.059899 0.004309 [1230848/1237259]
Epoch 80
-------------------------------
loss: 0.057015 0.004321 [ 2048/1237259]
loss: 0.062125 0.004298 [206848/1237259]
loss: 0.051909 0.004224 [411648/1237259]
loss: 0.062154 0.004309 [616448/1237259]
loss: 0.053782 0.004337 [821248/1237259]
loss: 0.050604 0.004340 [1026048/1237259]
loss: 0.056814 0.004308 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0528  
ndcg@20: 0.0426  
diversity: 0.1780  


Epoch 81
-------------------------------
loss: 0.058708 0.004328 [ 2048/1237259]
loss: 0.056097 0.004365 [206848/1237259]
loss: 0.058098 0.004337 [411648/1237259]
loss: 0.055035 0.004397 [616448/1237259]
loss: 0.054241 0.004429 [821248/1237259]
loss: 0.050525 0.004414 [1026048/1237259]
loss: 0.060818 0.004406 [1230848/1237259]
Epoch 82
-------------------------------
loss: 0.063821 0.004453 [ 2048/1237259]
loss: 0.050638 0.004395 [206848/1237259]
loss: 0.053317 0.004361 [411648/1237259]
loss: 0.054558 0.004366 [616448/1237259]
loss: 0.045249 0.004344 [821248/1237259]
loss: 0.062855 0.004419 [1026048/1237259]
loss: 0.048589 0.004415 [1230848/1237259]
Epoch 83
-------------------------------
loss: 0.054551 0.004453 [ 2048/1237259]
loss: 0.045846 0.004384 [206848/1237259]
loss: 0.048203 0.004466 [411648/1237259]
loss: 0.056019 0.004400 [616448/1237259]
loss: 0.052481 0.004441 [821248/1237259]
loss: 0.047703 0.004430 [1026048/1237259]
loss: 0.058572 0.004416 [1230848/1237259]
Epoch 84
-------------------------------
loss: 0.048244 0.004531 [ 2048/1237259]
loss: 0.051280 0.004438 [206848/1237259]
loss: 0.050964 0.004487 [411648/1237259]
loss: 0.050563 0.004518 [616448/1237259]
loss: 0.056193 0.004456 [821248/1237259]
loss: 0.047228 0.004546 [1026048/1237259]
loss: 0.058903 0.004499 [1230848/1237259]
Epoch 85
-------------------------------
loss: 0.046665 0.004482 [ 2048/1237259]
loss: 0.053874 0.004509 [206848/1237259]
loss: 0.050594 0.004444 [411648/1237259]
loss: 0.043492 0.004472 [616448/1237259]
loss: 0.058185 0.004521 [821248/1237259]
loss: 0.051912 0.004485 [1026048/1237259]
loss: 0.050983 0.004562 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0535  
ndcg@20: 0.0432  
diversity: 0.1786  


Epoch 86
-------------------------------
loss: 0.041491 0.004539 [ 2048/1237259]
loss: 0.060528 0.004542 [206848/1237259]
loss: 0.060433 0.004455 [411648/1237259]
loss: 0.058423 0.004479 [616448/1237259]
loss: 0.054039 0.004561 [821248/1237259]
loss: 0.040537 0.004536 [1026048/1237259]
loss: 0.055833 0.004473 [1230848/1237259]
Epoch 87
-------------------------------
loss: 0.049555 0.004564 [ 2048/1237259]
loss: 0.054873 0.004568 [206848/1237259]
loss: 0.064322 0.004521 [411648/1237259]
loss: 0.044865 0.004568 [616448/1237259]
loss: 0.048805 0.004540 [821248/1237259]
loss: 0.051453 0.004645 [1026048/1237259]
loss: 0.053941 0.004514 [1230848/1237259]
Epoch 88
-------------------------------
loss: 0.051425 0.004509 [ 2048/1237259]
loss: 0.049606 0.004598 [206848/1237259]
loss: 0.040589 0.004606 [411648/1237259]
loss: 0.060895 0.004585 [616448/1237259]
loss: 0.050386 0.004545 [821248/1237259]
loss: 0.043078 0.004569 [1026048/1237259]
loss: 0.060934 0.004629 [1230848/1237259]
Epoch 89
-------------------------------
loss: 0.062702 0.004603 [ 2048/1237259]
loss: 0.052114 0.004683 [206848/1237259]
loss: 0.051770 0.004591 [411648/1237259]
loss: 0.057490 0.004654 [616448/1237259]
loss: 0.053688 0.004638 [821248/1237259]
loss: 0.056803 0.004543 [1026048/1237259]
loss: 0.040220 0.004640 [1230848/1237259]
Epoch 90
-------------------------------
loss: 0.049183 0.004612 [ 2048/1237259]
loss: 0.054927 0.004636 [206848/1237259]
loss: 0.046807 0.004693 [411648/1237259]
loss: 0.058457 0.004631 [616448/1237259]
loss: 0.045577 0.004606 [821248/1237259]
loss: 0.044729 0.004599 [1026048/1237259]
loss: 0.049811 0.004626 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0539  
ndcg@20: 0.0436  
diversity: 0.1792  


Epoch 91
-------------------------------
loss: 0.046857 0.004671 [ 2048/1237259]
loss: 0.053326 0.004735 [206848/1237259]
loss: 0.056395 0.004687 [411648/1237259]
loss: 0.056303 0.004633 [616448/1237259]
loss: 0.037112 0.004694 [821248/1237259]
loss: 0.045573 0.004669 [1026048/1237259]
loss: 0.053678 0.004728 [1230848/1237259]
Epoch 92
-------------------------------
loss: 0.052997 0.004724 [ 2048/1237259]
loss: 0.046615 0.004731 [206848/1237259]
loss: 0.052529 0.004707 [411648/1237259]
loss: 0.052268 0.004805 [616448/1237259]
loss: 0.043791 0.004695 [821248/1237259]
loss: 0.042361 0.004724 [1026048/1237259]
loss: 0.041957 0.004749 [1230848/1237259]
Epoch 93
-------------------------------
loss: 0.051089 0.004725 [ 2048/1237259]
loss: 0.067820 0.004765 [206848/1237259]
loss: 0.055351 0.004756 [411648/1237259]
loss: 0.055070 0.004779 [616448/1237259]
loss: 0.051358 0.004804 [821248/1237259]
loss: 0.043714 0.004827 [1026048/1237259]
loss: 0.054395 0.004747 [1230848/1237259]
Epoch 94
-------------------------------
loss: 0.046420 0.004793 [ 2048/1237259]
loss: 0.045815 0.004685 [206848/1237259]
loss: 0.052102 0.004805 [411648/1237259]
loss: 0.056311 0.004761 [616448/1237259]
loss: 0.055126 0.004842 [821248/1237259]
loss: 0.052509 0.004768 [1026048/1237259]
loss: 0.042483 0.004749 [1230848/1237259]
Epoch 95
-------------------------------
loss: 0.060477 0.004786 [ 2048/1237259]
loss: 0.040379 0.004870 [206848/1237259]
loss: 0.050291 0.004830 [411648/1237259]
loss: 0.045181 0.004964 [616448/1237259]
loss: 0.045237 0.004829 [821248/1237259]
loss: 0.054724 0.004806 [1026048/1237259]
loss: 0.063184 0.004877 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0542  
ndcg@20: 0.0439  
diversity: 0.1796  


Epoch 96
-------------------------------
loss: 0.043282 0.004838 [ 2048/1237259]
loss: 0.049077 0.004823 [206848/1237259]
loss: 0.051155 0.004828 [411648/1237259]
loss: 0.054454 0.004872 [616448/1237259]
loss: 0.042805 0.004865 [821248/1237259]
loss: 0.049666 0.004895 [1026048/1237259]
loss: 0.049581 0.004879 [1230848/1237259]
Epoch 97
-------------------------------
loss: 0.052254 0.004869 [ 2048/1237259]
loss: 0.062129 0.004914 [206848/1237259]
loss: 0.049240 0.004865 [411648/1237259]
loss: 0.054471 0.004878 [616448/1237259]
loss: 0.054881 0.004815 [821248/1237259]
loss: 0.045027 0.004878 [1026048/1237259]
loss: 0.051770 0.004897 [1230848/1237259]
Epoch 98
-------------------------------
loss: 0.041652 0.004909 [ 2048/1237259]
loss: 0.055596 0.004922 [206848/1237259]
loss: 0.053448 0.004887 [411648/1237259]
loss: 0.048531 0.004958 [616448/1237259]
loss: 0.046695 0.004957 [821248/1237259]
loss: 0.055686 0.004913 [1026048/1237259]
loss: 0.047984 0.004929 [1230848/1237259]
Epoch 99
-------------------------------
loss: 0.043367 0.004906 [ 2048/1237259]
loss: 0.042733 0.004900 [206848/1237259]
loss: 0.046475 0.004959 [411648/1237259]
loss: 0.041426 0.004943 [616448/1237259]
loss: 0.050316 0.004983 [821248/1237259]
loss: 0.053558 0.004936 [1026048/1237259]
loss: 0.048690 0.004934 [1230848/1237259]
Epoch 100
-------------------------------
loss: 0.054851 0.004903 [ 2048/1237259]
loss: 0.047686 0.004945 [206848/1237259]
loss: 0.041250 0.004869 [411648/1237259]
loss: 0.047003 0.004990 [616448/1237259]
loss: 0.052697 0.004977 [821248/1237259]
loss: 0.047991 0.004955 [1026048/1237259]
loss: 0.038692 0.004993 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0548  
ndcg@20: 0.0445  
diversity: 0.1804  


Epoch 101
-------------------------------
loss: 0.038783 0.005002 [ 2048/1237259]
loss: 0.048859 0.005013 [206848/1237259]
loss: 0.051746 0.005007 [411648/1237259]
loss: 0.043243 0.004953 [616448/1237259]
loss: 0.037753 0.005024 [821248/1237259]
loss: 0.045527 0.005032 [1026048/1237259]
loss: 0.050199 0.005010 [1230848/1237259]
Epoch 102
-------------------------------
loss: 0.049859 0.005037 [ 2048/1237259]
loss: 0.048577 0.004990 [206848/1237259]
loss: 0.052582 0.004939 [411648/1237259]
loss: 0.060043 0.005062 [616448/1237259]
loss: 0.051355 0.005082 [821248/1237259]
loss: 0.054337 0.005080 [1026048/1237259]
loss: 0.052019 0.005034 [1230848/1237259]
Epoch 103
-------------------------------
loss: 0.049726 0.004994 [ 2048/1237259]
loss: 0.045269 0.005043 [206848/1237259]
loss: 0.049682 0.005018 [411648/1237259]
loss: 0.040672 0.005073 [616448/1237259]
loss: 0.040507 0.005015 [821248/1237259]
loss: 0.041274 0.005080 [1026048/1237259]
loss: 0.050717 0.005039 [1230848/1237259]
Epoch 104
-------------------------------
loss: 0.046725 0.005063 [ 2048/1237259]
loss: 0.046161 0.005008 [206848/1237259]
loss: 0.046474 0.005072 [411648/1237259]
loss: 0.050259 0.005055 [616448/1237259]
loss: 0.045542 0.005012 [821248/1237259]
loss: 0.043157 0.005047 [1026048/1237259]
loss: 0.049258 0.005035 [1230848/1237259]
Epoch 105
-------------------------------
loss: 0.050126 0.005056 [ 2048/1237259]
loss: 0.050641 0.005101 [206848/1237259]
loss: 0.042093 0.005095 [411648/1237259]
loss: 0.044076 0.005039 [616448/1237259]
loss: 0.048018 0.005102 [821248/1237259]
loss: 0.042170 0.005082 [1026048/1237259]
loss: 0.060498 0.005068 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0553  
ndcg@20: 0.0449  
diversity: 0.1809  


Epoch 106
-------------------------------
loss: 0.048170 0.005210 [ 2048/1237259]
loss: 0.037825 0.005152 [206848/1237259]
loss: 0.040255 0.005101 [411648/1237259]
loss: 0.046171 0.005125 [616448/1237259]
loss: 0.053353 0.005121 [821248/1237259]
loss: 0.047126 0.005134 [1026048/1237259]
loss: 0.049051 0.005186 [1230848/1237259]
Epoch 107
-------------------------------
loss: 0.040566 0.005129 [ 2048/1237259]
loss: 0.044537 0.005127 [206848/1237259]
loss: 0.046091 0.005175 [411648/1237259]
loss: 0.057893 0.005188 [616448/1237259]
loss: 0.048379 0.005200 [821248/1237259]
loss: 0.054937 0.005113 [1026048/1237259]
loss: 0.039211 0.005112 [1230848/1237259]
Epoch 108
-------------------------------
loss: 0.039564 0.005159 [ 2048/1237259]
loss: 0.049423 0.005155 [206848/1237259]
loss: 0.056741 0.005321 [411648/1237259]
loss: 0.049922 0.005285 [616448/1237259]
loss: 0.046810 0.005169 [821248/1237259]
loss: 0.048858 0.005174 [1026048/1237259]
loss: 0.039362 0.005234 [1230848/1237259]
Epoch 109
-------------------------------
loss: 0.054115 0.005288 [ 2048/1237259]
loss: 0.052012 0.005169 [206848/1237259]
loss: 0.048120 0.005182 [411648/1237259]
loss: 0.055224 0.005222 [616448/1237259]
loss: 0.047674 0.005209 [821248/1237259]
loss: 0.048227 0.005189 [1026048/1237259]
loss: 0.048106 0.005202 [1230848/1237259]
Epoch 110
-------------------------------
loss: 0.051001 0.005206 [ 2048/1237259]
loss: 0.043234 0.005236 [206848/1237259]
loss: 0.050281 0.005188 [411648/1237259]
loss: 0.038481 0.005219 [616448/1237259]
loss: 0.052100 0.005257 [821248/1237259]
loss: 0.048737 0.005370 [1026048/1237259]
loss: 0.041157 0.005169 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0553  
ndcg@20: 0.0450  
diversity: 0.1813  


Epoch 111
-------------------------------
loss: 0.045344 0.005295 [ 2048/1237259]
loss: 0.048230 0.005292 [206848/1237259]
loss: 0.045342 0.005263 [411648/1237259]
loss: 0.046523 0.005302 [616448/1237259]
loss: 0.054545 0.005262 [821248/1237259]
loss: 0.056804 0.005359 [1026048/1237259]
loss: 0.053824 0.005249 [1230848/1237259]
Epoch 112
-------------------------------
loss: 0.046350 0.005225 [ 2048/1237259]
loss: 0.041865 0.005321 [206848/1237259]
loss: 0.046750 0.005290 [411648/1237259]
loss: 0.046181 0.005358 [616448/1237259]
loss: 0.041090 0.005208 [821248/1237259]
loss: 0.039224 0.005302 [1026048/1237259]
loss: 0.042048 0.005394 [1230848/1237259]
Epoch 113
-------------------------------
loss: 0.041083 0.005364 [ 2048/1237259]
loss: 0.036190 0.005355 [206848/1237259]
loss: 0.041073 0.005278 [411648/1237259]
loss: 0.042867 0.005344 [616448/1237259]
loss: 0.042973 0.005284 [821248/1237259]
loss: 0.046362 0.005325 [1026048/1237259]
loss: 0.039548 0.005269 [1230848/1237259]
Epoch 114
-------------------------------
loss: 0.051960 0.005303 [ 2048/1237259]
loss: 0.045267 0.005408 [206848/1237259]
loss: 0.045257 0.005359 [411648/1237259]
loss: 0.043036 0.005489 [616448/1237259]
loss: 0.049396 0.005365 [821248/1237259]
loss: 0.043827 0.005238 [1026048/1237259]
loss: 0.043802 0.005370 [1230848/1237259]
Epoch 115
-------------------------------
loss: 0.049822 0.005284 [ 2048/1237259]
loss: 0.041427 0.005321 [206848/1237259]
loss: 0.040487 0.005358 [411648/1237259]
loss: 0.051280 0.005305 [616448/1237259]
loss: 0.045657 0.005407 [821248/1237259]
loss: 0.043201 0.005348 [1026048/1237259]
loss: 0.051735 0.005409 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0561  
ndcg@20: 0.0454  
diversity: 0.1819  


Epoch 116
-------------------------------
loss: 0.038880 0.005404 [ 2048/1237259]
loss: 0.048392 0.005347 [206848/1237259]
loss: 0.042846 0.005338 [411648/1237259]
loss: 0.042661 0.005359 [616448/1237259]
loss: 0.045166 0.005353 [821248/1237259]
loss: 0.050156 0.005402 [1026048/1237259]
loss: 0.041479 0.005400 [1230848/1237259]
Epoch 117
-------------------------------
loss: 0.045958 0.005422 [ 2048/1237259]
loss: 0.049757 0.005411 [206848/1237259]
loss: 0.039877 0.005395 [411648/1237259]
loss: 0.049694 0.005460 [616448/1237259]
loss: 0.040558 0.005559 [821248/1237259]
loss: 0.037750 0.005497 [1026048/1237259]
loss: 0.061299 0.005370 [1230848/1237259]
Epoch 118
-------------------------------
loss: 0.053816 0.005495 [ 2048/1237259]
loss: 0.044253 0.005462 [206848/1237259]
loss: 0.048232 0.005444 [411648/1237259]
loss: 0.048540 0.005477 [616448/1237259]
loss: 0.046748 0.005498 [821248/1237259]
loss: 0.045290 0.005516 [1026048/1237259]
loss: 0.040929 0.005445 [1230848/1237259]
Epoch 119
-------------------------------
loss: 0.043203 0.005497 [ 2048/1237259]
loss: 0.041720 0.005519 [206848/1237259]
loss: 0.042465 0.005470 [411648/1237259]
loss: 0.043074 0.005537 [616448/1237259]
loss: 0.041325 0.005450 [821248/1237259]
loss: 0.048925 0.005468 [1026048/1237259]
loss: 0.054782 0.005551 [1230848/1237259]
Epoch 120
-------------------------------
loss: 0.054331 0.005491 [ 2048/1237259]
loss: 0.042418 0.005491 [206848/1237259]
loss: 0.040935 0.005520 [411648/1237259]
loss: 0.047634 0.005475 [616448/1237259]
loss: 0.044019 0.005543 [821248/1237259]
loss: 0.053408 0.005547 [1026048/1237259]
loss: 0.040054 0.005515 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0563  
ndcg@20: 0.0457  
diversity: 0.1823  


Epoch 121
-------------------------------
loss: 0.041584 0.005514 [ 2048/1237259]
loss: 0.052342 0.005463 [206848/1237259]
loss: 0.039724 0.005561 [411648/1237259]
loss: 0.047559 0.005630 [616448/1237259]
loss: 0.047789 0.005566 [821248/1237259]
loss: 0.033829 0.005620 [1026048/1237259]
loss: 0.035498 0.005523 [1230848/1237259]
Epoch 122
-------------------------------
loss: 0.046311 0.005506 [ 2048/1237259]
loss: 0.039238 0.005493 [206848/1237259]
loss: 0.045594 0.005529 [411648/1237259]
loss: 0.048255 0.005525 [616448/1237259]
loss: 0.046607 0.005629 [821248/1237259]
loss: 0.039328 0.005570 [1026048/1237259]
loss: 0.045935 0.005566 [1230848/1237259]
Epoch 123
-------------------------------
loss: 0.038595 0.005570 [ 2048/1237259]
loss: 0.049255 0.005567 [206848/1237259]
loss: 0.038822 0.005604 [411648/1237259]
loss: 0.040251 0.005647 [616448/1237259]
loss: 0.034855 0.005468 [821248/1237259]
loss: 0.048426 0.005622 [1026048/1237259]
loss: 0.048940 0.005600 [1230848/1237259]
Epoch 124
-------------------------------
loss: 0.036023 0.005589 [ 2048/1237259]
loss: 0.040291 0.005530 [206848/1237259]
loss: 0.055667 0.005642 [411648/1237259]
loss: 0.049053 0.005560 [616448/1237259]
loss: 0.034821 0.005625 [821248/1237259]
loss: 0.042870 0.005586 [1026048/1237259]
loss: 0.045314 0.005564 [1230848/1237259]
Epoch 125
-------------------------------
loss: 0.037232 0.005657 [ 2048/1237259]
loss: 0.043217 0.005573 [206848/1237259]
loss: 0.042556 0.005588 [411648/1237259]
loss: 0.042841 0.005622 [616448/1237259]
loss: 0.042664 0.005612 [821248/1237259]
loss: 0.040042 0.005622 [1026048/1237259]
loss: 0.039548 0.005573 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0567  
ndcg@20: 0.0461  
diversity: 0.1828  


Epoch 126
-------------------------------
loss: 0.050031 0.005680 [ 2048/1237259]
loss: 0.040807 0.005662 [206848/1237259]
loss: 0.037380 0.005680 [411648/1237259]
loss: 0.047319 0.005650 [616448/1237259]
loss: 0.037308 0.005630 [821248/1237259]
loss: 0.045430 0.005727 [1026048/1237259]
loss: 0.042766 0.005700 [1230848/1237259]
Epoch 127
-------------------------------
loss: 0.044098 0.005653 [ 2048/1237259]
loss: 0.046159 0.005714 [206848/1237259]
loss: 0.039552 0.005632 [411648/1237259]
loss: 0.033469 0.005772 [616448/1237259]
loss: 0.044813 0.005724 [821248/1237259]
loss: 0.049305 0.005687 [1026048/1237259]
loss: 0.041876 0.005735 [1230848/1237259]
Epoch 128
-------------------------------
loss: 0.042593 0.005694 [ 2048/1237259]
loss: 0.044126 0.005680 [206848/1237259]
loss: 0.034292 0.005670 [411648/1237259]
loss: 0.040110 0.005727 [616448/1237259]
loss: 0.047213 0.005686 [821248/1237259]
loss: 0.046312 0.005705 [1026048/1237259]
loss: 0.037549 0.005765 [1230848/1237259]
Epoch 129
-------------------------------
loss: 0.039191 0.005778 [ 2048/1237259]
loss: 0.040560 0.005682 [206848/1237259]
loss: 0.040217 0.005766 [411648/1237259]
loss: 0.035849 0.005729 [616448/1237259]
loss: 0.051150 0.005648 [821248/1237259]
loss: 0.050171 0.005788 [1026048/1237259]
loss: 0.048639 0.005687 [1230848/1237259]
Epoch 130
-------------------------------
loss: 0.049456 0.005844 [ 2048/1237259]
loss: 0.043975 0.005751 [206848/1237259]
loss: 0.044261 0.005603 [411648/1237259]
loss: 0.042461 0.005772 [616448/1237259]
loss: 0.055904 0.005753 [821248/1237259]
loss: 0.048660 0.005758 [1026048/1237259]
loss: 0.035475 0.005717 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0571  
ndcg@20: 0.0464  
diversity: 0.1832  


Epoch 131
-------------------------------
loss: 0.040882 0.005781 [ 2048/1237259]
loss: 0.041884 0.005714 [206848/1237259]
loss: 0.041241 0.005771 [411648/1237259]
loss: 0.053912 0.005816 [616448/1237259]
loss: 0.043448 0.005850 [821248/1237259]
loss: 0.035813 0.005725 [1026048/1237259]
loss: 0.043177 0.005706 [1230848/1237259]
Epoch 132
-------------------------------
loss: 0.040363 0.005723 [ 2048/1237259]
loss: 0.041681 0.005789 [206848/1237259]
loss: 0.047996 0.005746 [411648/1237259]
loss: 0.038585 0.005862 [616448/1237259]
loss: 0.047611 0.005917 [821248/1237259]
loss: 0.045163 0.005777 [1026048/1237259]
loss: 0.038321 0.005809 [1230848/1237259]
Epoch 133
-------------------------------
loss: 0.034073 0.005910 [ 2048/1237259]
loss: 0.038676 0.005872 [206848/1237259]
loss: 0.041865 0.005809 [411648/1237259]
loss: 0.033966 0.005860 [616448/1237259]
loss: 0.041686 0.005884 [821248/1237259]
loss: 0.043010 0.005882 [1026048/1237259]
loss: 0.047105 0.005845 [1230848/1237259]
Epoch 134
-------------------------------
loss: 0.046195 0.005836 [ 2048/1237259]
loss: 0.038886 0.005858 [206848/1237259]
loss: 0.037216 0.005898 [411648/1237259]
loss: 0.030814 0.005877 [616448/1237259]
loss: 0.037970 0.005821 [821248/1237259]
loss: 0.039181 0.005939 [1026048/1237259]
loss: 0.046454 0.005840 [1230848/1237259]
Epoch 135
-------------------------------
loss: 0.039989 0.005841 [ 2048/1237259]
loss: 0.049056 0.005908 [206848/1237259]
loss: 0.040123 0.005838 [411648/1237259]
loss: 0.042979 0.005860 [616448/1237259]
loss: 0.041901 0.005856 [821248/1237259]
loss: 0.039491 0.005900 [1026048/1237259]
loss: 0.040228 0.005923 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0574  
ndcg@20: 0.0468  
diversity: 0.1835  


Epoch 136
-------------------------------
loss: 0.047121 0.005841 [ 2048/1237259]
loss: 0.036616 0.005942 [206848/1237259]
loss: 0.038686 0.005938 [411648/1237259]
loss: 0.035700 0.005852 [616448/1237259]
loss: 0.039326 0.005965 [821248/1237259]
loss: 0.035933 0.005948 [1026048/1237259]
loss: 0.038035 0.005898 [1230848/1237259]
Epoch 137
-------------------------------
loss: 0.034021 0.005915 [ 2048/1237259]
loss: 0.043258 0.005951 [206848/1237259]
loss: 0.041912 0.005940 [411648/1237259]
loss: 0.044188 0.005888 [616448/1237259]
loss: 0.046249 0.005958 [821248/1237259]
loss: 0.038729 0.005970 [1026048/1237259]
loss: 0.036850 0.005914 [1230848/1237259]
Epoch 138
-------------------------------
loss: 0.042046 0.006014 [ 2048/1237259]
loss: 0.038114 0.006017 [206848/1237259]
loss: 0.031594 0.005955 [411648/1237259]
loss: 0.039543 0.005950 [616448/1237259]
loss: 0.041944 0.006039 [821248/1237259]
loss: 0.041890 0.005941 [1026048/1237259]
loss: 0.046046 0.005936 [1230848/1237259]
Epoch 139
-------------------------------
loss: 0.038888 0.005901 [ 2048/1237259]
loss: 0.046873 0.005895 [206848/1237259]
loss: 0.035484 0.005954 [411648/1237259]
loss: 0.037095 0.006046 [616448/1237259]
loss: 0.041160 0.006030 [821248/1237259]
loss: 0.045859 0.005958 [1026048/1237259]
loss: 0.041153 0.006060 [1230848/1237259]
Epoch 140
-------------------------------
loss: 0.046165 0.006029 [ 2048/1237259]
loss: 0.048114 0.005885 [206848/1237259]
loss: 0.037626 0.006035 [411648/1237259]
loss: 0.054924 0.006022 [616448/1237259]
loss: 0.042714 0.006046 [821248/1237259]
loss: 0.037541 0.005986 [1026048/1237259]
loss: 0.035710 0.005997 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0579  
ndcg@20: 0.0470  
diversity: 0.1840  


Epoch 141
-------------------------------
loss: 0.042131 0.006066 [ 2048/1237259]
loss: 0.047076 0.006020 [206848/1237259]
loss: 0.034549 0.006078 [411648/1237259]
loss: 0.048955 0.005985 [616448/1237259]
loss: 0.041405 0.006034 [821248/1237259]
loss: 0.037557 0.006080 [1026048/1237259]
loss: 0.032978 0.006129 [1230848/1237259]
Epoch 142
-------------------------------
loss: 0.037113 0.006011 [ 2048/1237259]
loss: 0.042749 0.005989 [206848/1237259]
loss: 0.035843 0.006015 [411648/1237259]
loss: 0.045029 0.006025 [616448/1237259]
loss: 0.039603 0.006105 [821248/1237259]
loss: 0.047706 0.006035 [1026048/1237259]
loss: 0.039671 0.006071 [1230848/1237259]
Epoch 143
-------------------------------
loss: 0.041140 0.006043 [ 2048/1237259]
loss: 0.032515 0.006030 [206848/1237259]
loss: 0.047224 0.006091 [411648/1237259]
loss: 0.048799 0.006090 [616448/1237259]
loss: 0.043030 0.005996 [821248/1237259]
loss: 0.043069 0.006080 [1026048/1237259]
loss: 0.047554 0.006111 [1230848/1237259]
Epoch 144
-------------------------------
loss: 0.047498 0.005984 [ 2048/1237259]
loss: 0.039196 0.006110 [206848/1237259]
loss: 0.038285 0.006178 [411648/1237259]
loss: 0.041504 0.006069 [616448/1237259]
loss: 0.029919 0.006127 [821248/1237259]
loss: 0.035564 0.006120 [1026048/1237259]
loss: 0.037844 0.006149 [1230848/1237259]
Epoch 145
-------------------------------
loss: 0.039564 0.006057 [ 2048/1237259]
loss: 0.042003 0.006196 [206848/1237259]
loss: 0.039150 0.006185 [411648/1237259]
loss: 0.036809 0.006071 [616448/1237259]
loss: 0.043603 0.006123 [821248/1237259]
loss: 0.037784 0.006042 [1026048/1237259]
loss: 0.045087 0.006152 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0576  
ndcg@20: 0.0469  
diversity: 0.1842  


Epoch 146
-------------------------------
loss: 0.041928 0.006049 [ 2048/1237259]
loss: 0.039412 0.006127 [206848/1237259]
loss: 0.038400 0.006111 [411648/1237259]
loss: 0.044485 0.006196 [616448/1237259]
loss: 0.036345 0.006199 [821248/1237259]
loss: 0.032087 0.006223 [1026048/1237259]
loss: 0.052163 0.006192 [1230848/1237259]
Epoch 147
-------------------------------
loss: 0.033584 0.006196 [ 2048/1237259]
loss: 0.039013 0.006179 [206848/1237259]
loss: 0.038207 0.006188 [411648/1237259]
loss: 0.032258 0.006172 [616448/1237259]
loss: 0.038435 0.006225 [821248/1237259]
loss: 0.041987 0.006147 [1026048/1237259]
loss: 0.040009 0.006236 [1230848/1237259]
Epoch 148
-------------------------------
loss: 0.034617 0.006131 [ 2048/1237259]
loss: 0.041910 0.006165 [206848/1237259]
loss: 0.035537 0.006245 [411648/1237259]
loss: 0.039097 0.006300 [616448/1237259]
loss: 0.037009 0.006222 [821248/1237259]
loss: 0.039797 0.006274 [1026048/1237259]
loss: 0.044730 0.006233 [1230848/1237259]
Epoch 149
-------------------------------
loss: 0.033520 0.006143 [ 2048/1237259]
loss: 0.038766 0.006234 [206848/1237259]
loss: 0.037296 0.006186 [411648/1237259]
loss: 0.041618 0.006169 [616448/1237259]
loss: 0.035289 0.006211 [821248/1237259]
loss: 0.052677 0.006221 [1026048/1237259]
loss: 0.039084 0.006261 [1230848/1237259]
Epoch 150
-------------------------------
loss: 0.038585 0.006128 [ 2048/1237259]
loss: 0.050244 0.006262 [206848/1237259]
loss: 0.032853 0.006165 [411648/1237259]
loss: 0.038040 0.006259 [616448/1237259]
loss: 0.041331 0.006261 [821248/1237259]
loss: 0.033598 0.006290 [1026048/1237259]
loss: 0.047914 0.006326 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0579  
ndcg@20: 0.0471  
diversity: 0.1847  


Epoch 151
-------------------------------
loss: 0.041625 0.006238 [ 2048/1237259]
loss: 0.035434 0.006207 [206848/1237259]
loss: 0.038838 0.006246 [411648/1237259]
loss: 0.042961 0.006248 [616448/1237259]
loss: 0.038053 0.006296 [821248/1237259]
loss: 0.039489 0.006276 [1026048/1237259]
loss: 0.040084 0.006267 [1230848/1237259]
Epoch 152
-------------------------------
loss: 0.037557 0.006165 [ 2048/1237259]
loss: 0.035811 0.006345 [206848/1237259]
loss: 0.042052 0.006322 [411648/1237259]
loss: 0.037351 0.006310 [616448/1237259]
loss: 0.044116 0.006367 [821248/1237259]
loss: 0.040973 0.006316 [1026048/1237259]
loss: 0.036181 0.006168 [1230848/1237259]
Epoch 153
-------------------------------
loss: 0.032285 0.006340 [ 2048/1237259]
loss: 0.024602 0.006279 [206848/1237259]
loss: 0.040231 0.006277 [411648/1237259]
loss: 0.040430 0.006319 [616448/1237259]
loss: 0.046086 0.006334 [821248/1237259]
loss: 0.044110 0.006328 [1026048/1237259]
loss: 0.046999 0.006360 [1230848/1237259]
Epoch 154
-------------------------------
loss: 0.038646 0.006383 [ 2048/1237259]
loss: 0.033434 0.006318 [206848/1237259]
loss: 0.041360 0.006306 [411648/1237259]
loss: 0.034267 0.006233 [616448/1237259]
loss: 0.045848 0.006334 [821248/1237259]
loss: 0.046475 0.006269 [1026048/1237259]
loss: 0.041202 0.006464 [1230848/1237259]
Epoch 155
-------------------------------
loss: 0.041528 0.006311 [ 2048/1237259]
loss: 0.039272 0.006345 [206848/1237259]
loss: 0.036734 0.006356 [411648/1237259]
loss: 0.038211 0.006390 [616448/1237259]
loss: 0.039591 0.006343 [821248/1237259]
loss: 0.032932 0.006376 [1026048/1237259]
loss: 0.038080 0.006368 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0583  
ndcg@20: 0.0475  
diversity: 0.1851  


Epoch 156
-------------------------------
loss: 0.033730 0.006309 [ 2048/1237259]
loss: 0.043218 0.006378 [206848/1237259]
loss: 0.037020 0.006403 [411648/1237259]
loss: 0.033603 0.006466 [616448/1237259]
loss: 0.043875 0.006403 [821248/1237259]
loss: 0.030867 0.006346 [1026048/1237259]
loss: 0.039124 0.006389 [1230848/1237259]
Epoch 157
-------------------------------
loss: 0.046046 0.006379 [ 2048/1237259]
loss: 0.042220 0.006314 [206848/1237259]
loss: 0.042587 0.006356 [411648/1237259]
loss: 0.033041 0.006355 [616448/1237259]
loss: 0.037806 0.006438 [821248/1237259]
loss: 0.035689 0.006364 [1026048/1237259]
loss: 0.034297 0.006311 [1230848/1237259]
Epoch 158
-------------------------------
loss: 0.034354 0.006438 [ 2048/1237259]
loss: 0.036557 0.006355 [206848/1237259]
loss: 0.030804 0.006413 [411648/1237259]
loss: 0.035184 0.006435 [616448/1237259]
loss: 0.040026 0.006458 [821248/1237259]
loss: 0.040520 0.006425 [1026048/1237259]
loss: 0.037806 0.006324 [1230848/1237259]
Epoch 159
-------------------------------
loss: 0.038914 0.006425 [ 2048/1237259]
loss: 0.034874 0.006390 [206848/1237259]
loss: 0.033858 0.006619 [411648/1237259]
loss: 0.045301 0.006492 [616448/1237259]
loss: 0.034757 0.006480 [821248/1237259]
loss: 0.042355 0.006598 [1026048/1237259]
loss: 0.052701 0.006483 [1230848/1237259]
Epoch 160
-------------------------------
loss: 0.048864 0.006427 [ 2048/1237259]
loss: 0.039017 0.006394 [206848/1237259]
loss: 0.026667 0.006422 [411648/1237259]
loss: 0.033084 0.006478 [616448/1237259]
loss: 0.036136 0.006421 [821248/1237259]
loss: 0.034335 0.006505 [1026048/1237259]
loss: 0.036755 0.006504 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0581  
ndcg@20: 0.0475  
diversity: 0.1853  


Epoch 161
-------------------------------
loss: 0.040259 0.006503 [ 2048/1237259]
loss: 0.034748 0.006461 [206848/1237259]
loss: 0.036255 0.006598 [411648/1237259]
loss: 0.034479 0.006595 [616448/1237259]
loss: 0.045087 0.006524 [821248/1237259]
loss: 0.043683 0.006479 [1026048/1237259]
loss: 0.038208 0.006529 [1230848/1237259]
Epoch 162
-------------------------------
loss: 0.042331 0.006445 [ 2048/1237259]
loss: 0.037109 0.006532 [206848/1237259]
loss: 0.036159 0.006556 [411648/1237259]
loss: 0.037062 0.006516 [616448/1237259]
loss: 0.038643 0.006556 [821248/1237259]
loss: 0.045549 0.006505 [1026048/1237259]
loss: 0.035330 0.006528 [1230848/1237259]
Epoch 163
-------------------------------
loss: 0.040161 0.006546 [ 2048/1237259]
loss: 0.036137 0.006569 [206848/1237259]
loss: 0.034457 0.006439 [411648/1237259]
loss: 0.031367 0.006490 [616448/1237259]
loss: 0.046262 0.006584 [821248/1237259]
loss: 0.033284 0.006496 [1026048/1237259]
loss: 0.029776 0.006493 [1230848/1237259]
Epoch 164
-------------------------------
loss: 0.038672 0.006495 [ 2048/1237259]
loss: 0.030685 0.006610 [206848/1237259]
loss: 0.040623 0.006534 [411648/1237259]
loss: 0.034811 0.006563 [616448/1237259]
loss: 0.034872 0.006583 [821248/1237259]
loss: 0.032607 0.006620 [1026048/1237259]
loss: 0.043600 0.006611 [1230848/1237259]
Epoch 165
-------------------------------
loss: 0.037001 0.006497 [ 2048/1237259]
loss: 0.030926 0.006628 [206848/1237259]
loss: 0.031617 0.006602 [411648/1237259]
loss: 0.035306 0.006569 [616448/1237259]
loss: 0.036414 0.006674 [821248/1237259]
loss: 0.035175 0.006581 [1026048/1237259]
loss: 0.042114 0.006536 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0587  
ndcg@20: 0.0478  
diversity: 0.1857  


Epoch 166
-------------------------------
loss: 0.028056 0.006631 [ 2048/1237259]
loss: 0.028000 0.006474 [206848/1237259]
loss: 0.032482 0.006619 [411648/1237259]
loss: 0.036939 0.006601 [616448/1237259]
loss: 0.042567 0.006619 [821248/1237259]
loss: 0.040227 0.006680 [1026048/1237259]
loss: 0.044849 0.006586 [1230848/1237259]
Epoch 167
-------------------------------
loss: 0.035853 0.006583 [ 2048/1237259]
loss: 0.033453 0.006640 [206848/1237259]
loss: 0.041803 0.006574 [411648/1237259]
loss: 0.029982 0.006630 [616448/1237259]
loss: 0.032992 0.006734 [821248/1237259]
loss: 0.037496 0.006616 [1026048/1237259]
loss: 0.030740 0.006629 [1230848/1237259]
Epoch 168
-------------------------------
loss: 0.032038 0.006589 [ 2048/1237259]
loss: 0.037328 0.006610 [206848/1237259]
loss: 0.046278 0.006645 [411648/1237259]
loss: 0.042852 0.006646 [616448/1237259]
loss: 0.035253 0.006665 [821248/1237259]
loss: 0.035875 0.006655 [1026048/1237259]
loss: 0.037432 0.006672 [1230848/1237259]
Epoch 169
-------------------------------
loss: 0.029075 0.006685 [ 2048/1237259]
loss: 0.032626 0.006624 [206848/1237259]
loss: 0.033383 0.006605 [411648/1237259]
loss: 0.031949 0.006680 [616448/1237259]
loss: 0.039172 0.006754 [821248/1237259]
loss: 0.036897 0.006618 [1026048/1237259]
loss: 0.038611 0.006769 [1230848/1237259]
Epoch 170
-------------------------------
loss: 0.035546 0.006631 [ 2048/1237259]
loss: 0.034865 0.006731 [206848/1237259]
loss: 0.038714 0.006707 [411648/1237259]
loss: 0.041129 0.006679 [616448/1237259]
loss: 0.032581 0.006629 [821248/1237259]
loss: 0.042213 0.006748 [1026048/1237259]
loss: 0.037667 0.006727 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0589  
ndcg@20: 0.0479  
diversity: 0.1859  


Epoch 171
-------------------------------
loss: 0.037107 0.006667 [ 2048/1237259]
loss: 0.035407 0.006685 [206848/1237259]
loss: 0.034458 0.006733 [411648/1237259]
loss: 0.038879 0.006745 [616448/1237259]
loss: 0.029849 0.006706 [821248/1237259]
loss: 0.041765 0.006743 [1026048/1237259]
loss: 0.024743 0.006721 [1230848/1237259]
Epoch 172
-------------------------------
loss: 0.034011 0.006726 [ 2048/1237259]
loss: 0.040368 0.006658 [206848/1237259]
loss: 0.036323 0.006700 [411648/1237259]
loss: 0.037559 0.006771 [616448/1237259]
loss: 0.030301 0.006653 [821248/1237259]
loss: 0.032300 0.006699 [1026048/1237259]
loss: 0.031669 0.006758 [1230848/1237259]
Epoch 173
-------------------------------
loss: 0.044488 0.006775 [ 2048/1237259]
loss: 0.038805 0.006784 [206848/1237259]
loss: 0.032537 0.006741 [411648/1237259]
loss: 0.042980 0.006727 [616448/1237259]
loss: 0.030650 0.006706 [821248/1237259]
loss: 0.032185 0.006815 [1026048/1237259]
loss: 0.032973 0.006821 [1230848/1237259]
Epoch 174
-------------------------------
loss: 0.039265 0.006703 [ 2048/1237259]
loss: 0.040627 0.006756 [206848/1237259]
loss: 0.044460 0.006818 [411648/1237259]
loss: 0.039912 0.006731 [616448/1237259]
loss: 0.032459 0.006765 [821248/1237259]
loss: 0.038747 0.006816 [1026048/1237259]
loss: 0.040319 0.006811 [1230848/1237259]
Epoch 175
-------------------------------
loss: 0.037231 0.006797 [ 2048/1237259]
loss: 0.043717 0.006781 [206848/1237259]
loss: 0.037801 0.006755 [411648/1237259]
loss: 0.033105 0.006827 [616448/1237259]
loss: 0.037303 0.006773 [821248/1237259]
loss: 0.033015 0.006804 [1026048/1237259]
loss: 0.035837 0.006817 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0593  
ndcg@20: 0.0482  
diversity: 0.1862  


Epoch 176
-------------------------------
loss: 0.032464 0.006809 [ 2048/1237259]
loss: 0.042006 0.006729 [206848/1237259]
loss: 0.035918 0.006801 [411648/1237259]
loss: 0.048280 0.006855 [616448/1237259]
loss: 0.037981 0.006845 [821248/1237259]
loss: 0.030488 0.006786 [1026048/1237259]
loss: 0.035985 0.006746 [1230848/1237259]
Epoch 177
-------------------------------
loss: 0.039991 0.006801 [ 2048/1237259]
loss: 0.033650 0.006879 [206848/1237259]
loss: 0.031437 0.006792 [411648/1237259]
loss: 0.038433 0.006811 [616448/1237259]
loss: 0.042577 0.006836 [821248/1237259]
loss: 0.037364 0.006801 [1026048/1237259]
loss: 0.036552 0.006790 [1230848/1237259]
Epoch 178
-------------------------------
loss: 0.033705 0.006833 [ 2048/1237259]
loss: 0.038875 0.006858 [206848/1237259]
loss: 0.032117 0.006911 [411648/1237259]
loss: 0.033098 0.006910 [616448/1237259]
loss: 0.033834 0.006874 [821248/1237259]
loss: 0.033867 0.006917 [1026048/1237259]
loss: 0.039289 0.006890 [1230848/1237259]
Epoch 179
-------------------------------
loss: 0.031459 0.006835 [ 2048/1237259]
loss: 0.035903 0.006868 [206848/1237259]
loss: 0.033003 0.006819 [411648/1237259]
loss: 0.039398 0.006861 [616448/1237259]
loss: 0.031304 0.006827 [821248/1237259]
loss: 0.036570 0.006823 [1026048/1237259]
loss: 0.034479 0.006974 [1230848/1237259]
Epoch 180
-------------------------------
loss: 0.024808 0.006923 [ 2048/1237259]
loss: 0.032800 0.006832 [206848/1237259]
loss: 0.035213 0.006744 [411648/1237259]
loss: 0.040335 0.006840 [616448/1237259]
loss: 0.036168 0.006877 [821248/1237259]
loss: 0.035860 0.006871 [1026048/1237259]
loss: 0.031938 0.006871 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0595  
ndcg@20: 0.0484  
diversity: 0.1868  


Epoch 181
-------------------------------
loss: 0.035152 0.006889 [ 2048/1237259]
loss: 0.026429 0.006919 [206848/1237259]
loss: 0.035987 0.006971 [411648/1237259]
loss: 0.042727 0.006859 [616448/1237259]
loss: 0.038802 0.006921 [821248/1237259]
loss: 0.040723 0.006947 [1026048/1237259]
loss: 0.029515 0.006868 [1230848/1237259]
Epoch 182
-------------------------------
loss: 0.038459 0.006886 [ 2048/1237259]
loss: 0.037524 0.006880 [206848/1237259]
loss: 0.029647 0.006934 [411648/1237259]
loss: 0.041193 0.006982 [616448/1237259]
loss: 0.031281 0.006865 [821248/1237259]
loss: 0.039587 0.006930 [1026048/1237259]
loss: 0.041700 0.007006 [1230848/1237259]
Epoch 183
-------------------------------
loss: 0.040350 0.006892 [ 2048/1237259]
loss: 0.037696 0.007010 [206848/1237259]
loss: 0.038877 0.006877 [411648/1237259]
loss: 0.035932 0.006938 [616448/1237259]
loss: 0.037712 0.006946 [821248/1237259]
loss: 0.035986 0.006946 [1026048/1237259]
loss: 0.042275 0.006875 [1230848/1237259]
Epoch 184
-------------------------------
loss: 0.035516 0.006995 [ 2048/1237259]
loss: 0.037040 0.006989 [206848/1237259]
loss: 0.042121 0.007062 [411648/1237259]
loss: 0.049904 0.006962 [616448/1237259]
loss: 0.034162 0.006976 [821248/1237259]
loss: 0.036109 0.006971 [1026048/1237259]
loss: 0.023580 0.006930 [1230848/1237259]
Epoch 185
-------------------------------
loss: 0.033247 0.006939 [ 2048/1237259]
loss: 0.041433 0.006991 [206848/1237259]
loss: 0.034001 0.006990 [411648/1237259]
loss: 0.029773 0.006900 [616448/1237259]
loss: 0.032898 0.006992 [821248/1237259]
loss: 0.037192 0.006987 [1026048/1237259]
loss: 0.037955 0.007048 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0595  
ndcg@20: 0.0485  
diversity: 0.1870  


Epoch 186
-------------------------------
loss: 0.030634 0.007050 [ 2048/1237259]
loss: 0.028953 0.007068 [206848/1237259]
loss: 0.037137 0.006947 [411648/1237259]
loss: 0.030072 0.007010 [616448/1237259]
loss: 0.035767 0.006986 [821248/1237259]
loss: 0.031326 0.006960 [1026048/1237259]
loss: 0.031522 0.007066 [1230848/1237259]
Epoch 187
-------------------------------
loss: 0.033349 0.007052 [ 2048/1237259]
loss: 0.037666 0.007061 [206848/1237259]
loss: 0.032634 0.007110 [411648/1237259]
loss: 0.031174 0.006982 [616448/1237259]
loss: 0.038102 0.007114 [821248/1237259]
loss: 0.035916 0.007064 [1026048/1237259]
loss: 0.033812 0.007032 [1230848/1237259]
Epoch 188
-------------------------------
loss: 0.032412 0.007055 [ 2048/1237259]
loss: 0.038995 0.007047 [206848/1237259]
loss: 0.034176 0.007043 [411648/1237259]
loss: 0.029532 0.007068 [616448/1237259]
loss: 0.027960 0.007110 [821248/1237259]
loss: 0.037189 0.007134 [1026048/1237259]
loss: 0.030549 0.007092 [1230848/1237259]
Epoch 189
-------------------------------
loss: 0.035516 0.007174 [ 2048/1237259]
loss: 0.025478 0.007096 [206848/1237259]
loss: 0.033030 0.007043 [411648/1237259]
loss: 0.027508 0.007046 [616448/1237259]
loss: 0.039441 0.006993 [821248/1237259]
loss: 0.032401 0.007028 [1026048/1237259]
loss: 0.032479 0.007087 [1230848/1237259]
Epoch 190
-------------------------------
loss: 0.029578 0.007110 [ 2048/1237259]
loss: 0.026779 0.007165 [206848/1237259]
loss: 0.033132 0.007113 [411648/1237259]
loss: 0.038124 0.007046 [616448/1237259]
loss: 0.039320 0.007116 [821248/1237259]
loss: 0.036843 0.007128 [1026048/1237259]
loss: 0.026312 0.007151 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0594  
ndcg@20: 0.0485  
diversity: 0.1875  


Epoch 191
-------------------------------
loss: 0.045650 0.007091 [ 2048/1237259]
loss: 0.040138 0.007079 [206848/1237259]
loss: 0.042299 0.007068 [411648/1237259]
loss: 0.030339 0.007041 [616448/1237259]
loss: 0.033942 0.007167 [821248/1237259]
loss: 0.037847 0.007206 [1026048/1237259]
loss: 0.027494 0.007056 [1230848/1237259]
Epoch 192
-------------------------------
loss: 0.032692 0.007118 [ 2048/1237259]
loss: 0.033988 0.007137 [206848/1237259]
loss: 0.035516 0.007045 [411648/1237259]
loss: 0.027667 0.007090 [616448/1237259]
loss: 0.032322 0.007153 [821248/1237259]
loss: 0.031276 0.007195 [1026048/1237259]
loss: 0.033344 0.007110 [1230848/1237259]
Epoch 193
-------------------------------
loss: 0.040022 0.007115 [ 2048/1237259]
loss: 0.037915 0.007095 [206848/1237259]
loss: 0.031749 0.007118 [411648/1237259]
loss: 0.033993 0.007149 [616448/1237259]
loss: 0.033643 0.007190 [821248/1237259]
loss: 0.039643 0.007067 [1026048/1237259]
loss: 0.031856 0.007244 [1230848/1237259]
Epoch 194
-------------------------------
loss: 0.029832 0.007274 [ 2048/1237259]
loss: 0.032470 0.007175 [206848/1237259]
loss: 0.032540 0.007255 [411648/1237259]
loss: 0.038230 0.007171 [616448/1237259]
loss: 0.029243 0.007184 [821248/1237259]
loss: 0.033282 0.007078 [1026048/1237259]
loss: 0.036730 0.007169 [1230848/1237259]
Epoch 195
-------------------------------
loss: 0.032955 0.007136 [ 2048/1237259]
loss: 0.033693 0.007099 [206848/1237259]
loss: 0.038743 0.007171 [411648/1237259]
loss: 0.034372 0.007294 [616448/1237259]
loss: 0.032026 0.007194 [821248/1237259]
loss: 0.041790 0.007258 [1026048/1237259]
loss: 0.026538 0.007230 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0594  
ndcg@20: 0.0486  
diversity: 0.1877  


Epoch 196
-------------------------------
loss: 0.035147 0.007291 [ 2048/1237259]
loss: 0.028667 0.007253 [206848/1237259]
loss: 0.031058 0.007283 [411648/1237259]
loss: 0.034910 0.007200 [616448/1237259]
loss: 0.027220 0.007143 [821248/1237259]
loss: 0.029170 0.007333 [1026048/1237259]
loss: 0.029978 0.007309 [1230848/1237259]
Epoch 197
-------------------------------
loss: 0.030919 0.007350 [ 2048/1237259]
loss: 0.029038 0.007161 [206848/1237259]
loss: 0.029660 0.007240 [411648/1237259]
loss: 0.036726 0.007230 [616448/1237259]
loss: 0.038676 0.007235 [821248/1237259]
loss: 0.033832 0.007237 [1026048/1237259]
loss: 0.028617 0.007279 [1230848/1237259]
Epoch 198
-------------------------------
loss: 0.036253 0.007290 [ 2048/1237259]
loss: 0.038822 0.007240 [206848/1237259]
loss: 0.027248 0.007245 [411648/1237259]
loss: 0.030022 0.007365 [616448/1237259]
loss: 0.032374 0.007237 [821248/1237259]
loss: 0.032500 0.007272 [1026048/1237259]
loss: 0.038340 0.007246 [1230848/1237259]
Epoch 199
-------------------------------
loss: 0.034326 0.007271 [ 2048/1237259]
loss: 0.044027 0.007301 [206848/1237259]
loss: 0.039052 0.007208 [411648/1237259]
loss: 0.026288 0.007243 [616448/1237259]
loss: 0.037630 0.007294 [821248/1237259]
loss: 0.024468 0.007229 [1026048/1237259]
loss: 0.038959 0.007319 [1230848/1237259]
Epoch 200
-------------------------------
loss: 0.033979 0.007286 [ 2048/1237259]
loss: 0.031095 0.007361 [206848/1237259]
loss: 0.030273 0.007318 [411648/1237259]
loss: 0.035795 0.007343 [616448/1237259]
loss: 0.036072 0.007340 [821248/1237259]
loss: 0.031311 0.007217 [1026048/1237259]
loss: 0.037034 0.007303 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0597  
ndcg@20: 0.0488  
diversity: 0.1879  


Epoch 201
-------------------------------
loss: 0.033113 0.007381 [ 2048/1237259]
loss: 0.037241 0.007303 [206848/1237259]
loss: 0.032414 0.007343 [411648/1237259]
loss: 0.029770 0.007322 [616448/1237259]
loss: 0.026350 0.007318 [821248/1237259]
loss: 0.025134 0.007365 [1026048/1237259]
loss: 0.036653 0.007348 [1230848/1237259]
Epoch 202
-------------------------------
loss: 0.027597 0.007414 [ 2048/1237259]
loss: 0.031257 0.007303 [206848/1237259]
loss: 0.042260 0.007397 [411648/1237259]
loss: 0.031098 0.007374 [616448/1237259]
loss: 0.035099 0.007322 [821248/1237259]
loss: 0.036211 0.007361 [1026048/1237259]
loss: 0.028232 0.007334 [1230848/1237259]
Epoch 203
-------------------------------
loss: 0.024586 0.007345 [ 2048/1237259]
loss: 0.035164 0.007479 [206848/1237259]
loss: 0.035864 0.007272 [411648/1237259]
loss: 0.034616 0.007365 [616448/1237259]
loss: 0.024836 0.007348 [821248/1237259]
loss: 0.039158 0.007360 [1026048/1237259]
loss: 0.041070 0.007352 [1230848/1237259]
Epoch 204
-------------------------------
loss: 0.031130 0.007479 [ 2048/1237259]
loss: 0.036644 0.007403 [206848/1237259]
loss: 0.032376 0.007401 [411648/1237259]
loss: 0.030639 0.007388 [616448/1237259]
loss: 0.030452 0.007409 [821248/1237259]
loss: 0.032663 0.007392 [1026048/1237259]
loss: 0.030897 0.007446 [1230848/1237259]
Epoch 205
-------------------------------
loss: 0.036977 0.007420 [ 2048/1237259]
loss: 0.037681 0.007470 [206848/1237259]
loss: 0.034634 0.007421 [411648/1237259]
loss: 0.026598 0.007374 [616448/1237259]
loss: 0.039805 0.007274 [821248/1237259]
loss: 0.033396 0.007414 [1026048/1237259]
loss: 0.041976 0.007396 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0599  
ndcg@20: 0.0491  
diversity: 0.1882  


Epoch 206
-------------------------------
loss: 0.028935 0.007369 [ 2048/1237259]
loss: 0.027567 0.007425 [206848/1237259]
loss: 0.034001 0.007418 [411648/1237259]
loss: 0.034963 0.007357 [616448/1237259]
loss: 0.036038 0.007458 [821248/1237259]
loss: 0.035042 0.007346 [1026048/1237259]
loss: 0.029363 0.007399 [1230848/1237259]
Epoch 207
-------------------------------
loss: 0.029994 0.007340 [ 2048/1237259]
loss: 0.033158 0.007462 [206848/1237259]
loss: 0.037030 0.007441 [411648/1237259]
loss: 0.036812 0.007410 [616448/1237259]
loss: 0.029181 0.007447 [821248/1237259]
loss: 0.031548 0.007450 [1026048/1237259]
loss: 0.031070 0.007479 [1230848/1237259]
Epoch 208
-------------------------------
loss: 0.026350 0.007516 [ 2048/1237259]
loss: 0.029267 0.007474 [206848/1237259]
loss: 0.033239 0.007335 [411648/1237259]
loss: 0.030480 0.007503 [616448/1237259]
loss: 0.040064 0.007441 [821248/1237259]
loss: 0.037820 0.007396 [1026048/1237259]
loss: 0.023533 0.007509 [1230848/1237259]
Epoch 209
-------------------------------
loss: 0.027416 0.007490 [ 2048/1237259]
loss: 0.026660 0.007475 [206848/1237259]
loss: 0.035047 0.007529 [411648/1237259]
loss: 0.037860 0.007490 [616448/1237259]
loss: 0.033407 0.007473 [821248/1237259]
loss: 0.036441 0.007459 [1026048/1237259]
loss: 0.037246 0.007439 [1230848/1237259]
Epoch 210
-------------------------------
loss: 0.032124 0.007543 [ 2048/1237259]
loss: 0.037569 0.007422 [206848/1237259]
loss: 0.030651 0.007502 [411648/1237259]
loss: 0.028027 0.007528 [616448/1237259]
loss: 0.021598 0.007555 [821248/1237259]
loss: 0.043302 0.007471 [1026048/1237259]
loss: 0.039904 0.007485 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0599  
ndcg@20: 0.0491  
diversity: 0.1887  


Epoch 211
-------------------------------
loss: 0.031844 0.007534 [ 2048/1237259]
loss: 0.031411 0.007507 [206848/1237259]
loss: 0.028625 0.007495 [411648/1237259]
loss: 0.032693 0.007593 [616448/1237259]
loss: 0.024721 0.007578 [821248/1237259]
loss: 0.027999 0.007610 [1026048/1237259]
loss: 0.031796 0.007494 [1230848/1237259]
Epoch 212
-------------------------------
loss: 0.037356 0.007501 [ 2048/1237259]
loss: 0.032837 0.007502 [206848/1237259]
loss: 0.038619 0.007540 [411648/1237259]
loss: 0.028600 0.007550 [616448/1237259]
loss: 0.032848 0.007569 [821248/1237259]
loss: 0.032369 0.007469 [1026048/1237259]
loss: 0.030095 0.007558 [1230848/1237259]
Epoch 213
-------------------------------
loss: 0.030733 0.007544 [ 2048/1237259]
loss: 0.029765 0.007519 [206848/1237259]
loss: 0.037308 0.007519 [411648/1237259]
loss: 0.035819 0.007562 [616448/1237259]
loss: 0.032992 0.007539 [821248/1237259]
loss: 0.030345 0.007589 [1026048/1237259]
loss: 0.031684 0.007488 [1230848/1237259]
Epoch 214
-------------------------------
loss: 0.026689 0.007507 [ 2048/1237259]
loss: 0.035774 0.007562 [206848/1237259]
loss: 0.033804 0.007650 [411648/1237259]
loss: 0.028157 0.007672 [616448/1237259]
loss: 0.029438 0.007516 [821248/1237259]
loss: 0.026512 0.007558 [1026048/1237259]
loss: 0.033159 0.007612 [1230848/1237259]
Epoch 215
-------------------------------
loss: 0.026196 0.007575 [ 2048/1237259]
loss: 0.026089 0.007517 [206848/1237259]
loss: 0.037299 0.007645 [411648/1237259]
loss: 0.025100 0.007581 [616448/1237259]
loss: 0.043345 0.007619 [821248/1237259]
loss: 0.024728 0.007655 [1026048/1237259]
loss: 0.032547 0.007659 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0602  
ndcg@20: 0.0492  
diversity: 0.1889  


Epoch 216
-------------------------------
loss: 0.035324 0.007633 [ 2048/1237259]
loss: 0.032888 0.007589 [206848/1237259]
loss: 0.033250 0.007662 [411648/1237259]
loss: 0.031824 0.007707 [616448/1237259]
loss: 0.028633 0.007606 [821248/1237259]
loss: 0.032448 0.007571 [1026048/1237259]
loss: 0.032073 0.007595 [1230848/1237259]
Epoch 217
-------------------------------
loss: 0.028150 0.007615 [ 2048/1237259]
loss: 0.034862 0.007693 [206848/1237259]
loss: 0.029906 0.007717 [411648/1237259]
loss: 0.037370 0.007636 [616448/1237259]
loss: 0.034056 0.007620 [821248/1237259]
loss: 0.025815 0.007634 [1026048/1237259]
loss: 0.026063 0.007609 [1230848/1237259]
Epoch 218
-------------------------------
loss: 0.038455 0.007638 [ 2048/1237259]
loss: 0.036629 0.007664 [206848/1237259]
loss: 0.034026 0.007766 [411648/1237259]
loss: 0.041166 0.007585 [616448/1237259]
loss: 0.028664 0.007637 [821248/1237259]
loss: 0.034180 0.007714 [1026048/1237259]
loss: 0.036405 0.007712 [1230848/1237259]
Epoch 219
-------------------------------
loss: 0.026826 0.007708 [ 2048/1237259]
loss: 0.025638 0.007725 [206848/1237259]
loss: 0.025685 0.007729 [411648/1237259]
loss: 0.027794 0.007656 [616448/1237259]
loss: 0.031082 0.007674 [821248/1237259]
loss: 0.033170 0.007624 [1026048/1237259]
loss: 0.032022 0.007723 [1230848/1237259]
Epoch 220
-------------------------------
loss: 0.029488 0.007650 [ 2048/1237259]
loss: 0.027883 0.007742 [206848/1237259]
loss: 0.025318 0.007715 [411648/1237259]
loss: 0.035318 0.007672 [616448/1237259]
loss: 0.029415 0.007655 [821248/1237259]
loss: 0.024076 0.007687 [1026048/1237259]
loss: 0.031401 0.007706 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0602  
ndcg@20: 0.0492  
diversity: 0.1890  


Epoch 221
-------------------------------
loss: 0.032120 0.007679 [ 2048/1237259]
loss: 0.024905 0.007735 [206848/1237259]
loss: 0.026659 0.007698 [411648/1237259]
loss: 0.032201 0.007647 [616448/1237259]
loss: 0.031472 0.007674 [821248/1237259]
loss: 0.026164 0.007764 [1026048/1237259]
loss: 0.028047 0.007660 [1230848/1237259]
Epoch 222
-------------------------------
loss: 0.027892 0.007770 [ 2048/1237259]
loss: 0.041904 0.007646 [206848/1237259]
loss: 0.029369 0.007754 [411648/1237259]
loss: 0.028078 0.007679 [616448/1237259]
loss: 0.029221 0.007693 [821248/1237259]
loss: 0.027476 0.007787 [1026048/1237259]
loss: 0.030634 0.007819 [1230848/1237259]
Epoch 223
-------------------------------
loss: 0.033560 0.007759 [ 2048/1237259]
loss: 0.035396 0.007773 [206848/1237259]
loss: 0.028656 0.007762 [411648/1237259]
loss: 0.029241 0.007804 [616448/1237259]
loss: 0.032308 0.007751 [821248/1237259]
loss: 0.034079 0.007728 [1026048/1237259]
loss: 0.034975 0.007787 [1230848/1237259]
Epoch 224
-------------------------------
loss: 0.028007 0.007758 [ 2048/1237259]
loss: 0.029859 0.007717 [206848/1237259]
loss: 0.027269 0.007751 [411648/1237259]
loss: 0.032041 0.007756 [616448/1237259]
loss: 0.023610 0.007640 [821248/1237259]
loss: 0.026517 0.007746 [1026048/1237259]
loss: 0.027944 0.007711 [1230848/1237259]
Epoch 225
-------------------------------
loss: 0.032976 0.007721 [ 2048/1237259]
loss: 0.027024 0.007713 [206848/1237259]
loss: 0.029449 0.007710 [411648/1237259]
loss: 0.030948 0.007816 [616448/1237259]
loss: 0.025987 0.007742 [821248/1237259]
loss: 0.034297 0.007826 [1026048/1237259]
loss: 0.030831 0.007767 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0605  
ndcg@20: 0.0494  
diversity: 0.1892  


Epoch 226
-------------------------------
loss: 0.027035 0.007858 [ 2048/1237259]
loss: 0.031200 0.007779 [206848/1237259]
loss: 0.028876 0.007734 [411648/1237259]
loss: 0.030366 0.007834 [616448/1237259]
loss: 0.034574 0.007772 [821248/1237259]
loss: 0.030911 0.007813 [1026048/1237259]
loss: 0.035241 0.007817 [1230848/1237259]
Epoch 227
-------------------------------
loss: 0.033178 0.007832 [ 2048/1237259]
loss: 0.031688 0.007885 [206848/1237259]
loss: 0.029520 0.007850 [411648/1237259]
loss: 0.027507 0.007899 [616448/1237259]
loss: 0.028787 0.007863 [821248/1237259]
loss: 0.032548 0.007908 [1026048/1237259]
loss: 0.030088 0.007938 [1230848/1237259]
Epoch 228
-------------------------------
loss: 0.032523 0.007795 [ 2048/1237259]
loss: 0.028328 0.007816 [206848/1237259]
loss: 0.025932 0.007829 [411648/1237259]
loss: 0.028386 0.007831 [616448/1237259]
loss: 0.028937 0.007794 [821248/1237259]
loss: 0.021678 0.007829 [1026048/1237259]
loss: 0.036000 0.007911 [1230848/1237259]
Epoch 229
-------------------------------
loss: 0.036774 0.007842 [ 2048/1237259]
loss: 0.031784 0.007874 [206848/1237259]
loss: 0.038335 0.007930 [411648/1237259]
loss: 0.026933 0.007791 [616448/1237259]
loss: 0.025009 0.007849 [821248/1237259]
loss: 0.037696 0.007894 [1026048/1237259]
loss: 0.026433 0.007911 [1230848/1237259]
Epoch 230
-------------------------------
loss: 0.027456 0.007881 [ 2048/1237259]
loss: 0.023503 0.007861 [206848/1237259]
loss: 0.026345 0.007937 [411648/1237259]
loss: 0.026720 0.007787 [616448/1237259]
loss: 0.033986 0.007949 [821248/1237259]
loss: 0.032097 0.007916 [1026048/1237259]
loss: 0.031194 0.007868 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0607  
ndcg@20: 0.0495  
diversity: 0.1896  


Epoch 231
-------------------------------
loss: 0.028292 0.007836 [ 2048/1237259]
loss: 0.032288 0.007928 [206848/1237259]
loss: 0.025854 0.007931 [411648/1237259]
loss: 0.034427 0.007850 [616448/1237259]
loss: 0.036295 0.007912 [821248/1237259]
loss: 0.035659 0.007863 [1026048/1237259]
loss: 0.029682 0.007978 [1230848/1237259]
Epoch 232
-------------------------------
loss: 0.027798 0.007850 [ 2048/1237259]
loss: 0.028523 0.007908 [206848/1237259]
loss: 0.031824 0.007855 [411648/1237259]
loss: 0.032552 0.007820 [616448/1237259]
loss: 0.029130 0.007947 [821248/1237259]
loss: 0.031365 0.007952 [1026048/1237259]
loss: 0.027102 0.008009 [1230848/1237259]
Epoch 233
-------------------------------
loss: 0.027313 0.007998 [ 2048/1237259]
loss: 0.031642 0.007983 [206848/1237259]
loss: 0.033459 0.007966 [411648/1237259]
loss: 0.025833 0.007932 [616448/1237259]
loss: 0.029134 0.007924 [821248/1237259]
loss: 0.026805 0.007898 [1026048/1237259]
loss: 0.030728 0.007926 [1230848/1237259]
Epoch 234
-------------------------------
loss: 0.028721 0.007919 [ 2048/1237259]
loss: 0.030703 0.007933 [206848/1237259]
loss: 0.023709 0.007890 [411648/1237259]
loss: 0.027714 0.007993 [616448/1237259]
loss: 0.027028 0.007950 [821248/1237259]
loss: 0.027324 0.008008 [1026048/1237259]
loss: 0.028482 0.007936 [1230848/1237259]
Epoch 235
-------------------------------
loss: 0.029868 0.007960 [ 2048/1237259]
loss: 0.037921 0.007988 [206848/1237259]
loss: 0.029531 0.007944 [411648/1237259]
loss: 0.030482 0.007906 [616448/1237259]
loss: 0.027176 0.008015 [821248/1237259]
loss: 0.024281 0.008019 [1026048/1237259]
loss: 0.031514 0.007986 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0497  
diversity: 0.1899  


Epoch 236
-------------------------------
loss: 0.028075 0.008053 [ 2048/1237259]
loss: 0.025207 0.008023 [206848/1237259]
loss: 0.028743 0.007917 [411648/1237259]
loss: 0.027486 0.007941 [616448/1237259]
loss: 0.027000 0.007969 [821248/1237259]
loss: 0.031316 0.008071 [1026048/1237259]
loss: 0.022763 0.007961 [1230848/1237259]
Epoch 237
-------------------------------
loss: 0.021804 0.007937 [ 2048/1237259]
loss: 0.028122 0.008005 [206848/1237259]
loss: 0.033583 0.007923 [411648/1237259]
loss: 0.028922 0.008004 [616448/1237259]
loss: 0.037193 0.008046 [821248/1237259]
loss: 0.030684 0.008018 [1026048/1237259]
loss: 0.037964 0.007989 [1230848/1237259]
Epoch 238
-------------------------------
loss: 0.030510 0.007980 [ 2048/1237259]
loss: 0.032484 0.008008 [206848/1237259]
loss: 0.035181 0.007993 [411648/1237259]
loss: 0.030181 0.008054 [616448/1237259]
loss: 0.034771 0.008041 [821248/1237259]
loss: 0.032377 0.008103 [1026048/1237259]
loss: 0.038217 0.007983 [1230848/1237259]
Epoch 239
-------------------------------
loss: 0.025047 0.008003 [ 2048/1237259]
loss: 0.034278 0.008050 [206848/1237259]
loss: 0.032329 0.008094 [411648/1237259]
loss: 0.032047 0.008030 [616448/1237259]
loss: 0.026978 0.008063 [821248/1237259]
loss: 0.030851 0.007986 [1026048/1237259]
loss: 0.027508 0.007988 [1230848/1237259]
Epoch 240
-------------------------------
loss: 0.035315 0.008034 [ 2048/1237259]
loss: 0.031426 0.008053 [206848/1237259]
loss: 0.032230 0.007922 [411648/1237259]
loss: 0.034893 0.008088 [616448/1237259]
loss: 0.031006 0.008040 [821248/1237259]
loss: 0.034038 0.008017 [1026048/1237259]
loss: 0.020189 0.007971 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0608  
ndcg@20: 0.0497  
diversity: 0.1901  


Epoch 241
-------------------------------
loss: 0.035113 0.008050 [ 2048/1237259]
loss: 0.027199 0.007999 [206848/1237259]
loss: 0.025705 0.008015 [411648/1237259]
loss: 0.029403 0.008030 [616448/1237259]
loss: 0.029409 0.008074 [821248/1237259]
loss: 0.024170 0.008057 [1026048/1237259]
loss: 0.029796 0.008129 [1230848/1237259]
Epoch 242
-------------------------------
loss: 0.027723 0.008077 [ 2048/1237259]
loss: 0.025708 0.008104 [206848/1237259]
loss: 0.026708 0.008143 [411648/1237259]
loss: 0.026191 0.008115 [616448/1237259]
loss: 0.026879 0.008035 [821248/1237259]
loss: 0.031027 0.008082 [1026048/1237259]
loss: 0.032977 0.008029 [1230848/1237259]
Epoch 243
-------------------------------
loss: 0.025116 0.008049 [ 2048/1237259]
loss: 0.032553 0.008150 [206848/1237259]
loss: 0.034166 0.008052 [411648/1237259]
loss: 0.027721 0.008099 [616448/1237259]
loss: 0.027072 0.008168 [821248/1237259]
loss: 0.027555 0.008124 [1026048/1237259]
loss: 0.031356 0.008240 [1230848/1237259]
Epoch 244
-------------------------------
loss: 0.037561 0.008064 [ 2048/1237259]
loss: 0.028535 0.008180 [206848/1237259]
loss: 0.030432 0.008136 [411648/1237259]
loss: 0.032046 0.008111 [616448/1237259]
loss: 0.026159 0.008187 [821248/1237259]
loss: 0.030697 0.008103 [1026048/1237259]
loss: 0.028368 0.008044 [1230848/1237259]
Epoch 245
-------------------------------
loss: 0.024162 0.008118 [ 2048/1237259]
loss: 0.029020 0.008184 [206848/1237259]
loss: 0.026552 0.008079 [411648/1237259]
loss: 0.028810 0.008183 [616448/1237259]
loss: 0.027417 0.008140 [821248/1237259]
loss: 0.023663 0.008126 [1026048/1237259]
loss: 0.030524 0.008213 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0498  
diversity: 0.1902  


Epoch 246
-------------------------------
loss: 0.025840 0.008051 [ 2048/1237259]
loss: 0.024829 0.008179 [206848/1237259]
loss: 0.026419 0.008152 [411648/1237259]
loss: 0.029981 0.008193 [616448/1237259]
loss: 0.023887 0.008054 [821248/1237259]
loss: 0.028264 0.008065 [1026048/1237259]
loss: 0.028720 0.008325 [1230848/1237259]
Epoch 247
-------------------------------
loss: 0.033804 0.008181 [ 2048/1237259]
loss: 0.026238 0.008283 [206848/1237259]
loss: 0.026209 0.008217 [411648/1237259]
loss: 0.028326 0.008137 [616448/1237259]
loss: 0.030520 0.008150 [821248/1237259]
loss: 0.032670 0.008133 [1026048/1237259]
loss: 0.024814 0.008148 [1230848/1237259]
Epoch 248
-------------------------------
loss: 0.032744 0.008139 [ 2048/1237259]
loss: 0.028093 0.008206 [206848/1237259]
loss: 0.021718 0.008187 [411648/1237259]
loss: 0.029386 0.008182 [616448/1237259]
loss: 0.025446 0.008203 [821248/1237259]
loss: 0.023879 0.008273 [1026048/1237259]
loss: 0.025675 0.008181 [1230848/1237259]
Epoch 249
-------------------------------
loss: 0.025111 0.008249 [ 2048/1237259]
loss: 0.032628 0.008200 [206848/1237259]
loss: 0.028532 0.008135 [411648/1237259]
loss: 0.027926 0.008198 [616448/1237259]
loss: 0.026929 0.008120 [821248/1237259]
loss: 0.033469 0.008162 [1026048/1237259]
loss: 0.026654 0.008300 [1230848/1237259]
Epoch 250
-------------------------------
loss: 0.026039 0.008346 [ 2048/1237259]
loss: 0.027314 0.008258 [206848/1237259]
loss: 0.034959 0.008259 [411648/1237259]
loss: 0.028311 0.008332 [616448/1237259]
loss: 0.030073 0.008232 [821248/1237259]
loss: 0.029108 0.008321 [1026048/1237259]
loss: 0.024236 0.008255 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0610  
ndcg@20: 0.0498  
diversity: 0.1906  


Epoch 251
-------------------------------
loss: 0.028976 0.008182 [ 2048/1237259]
loss: 0.025584 0.008207 [206848/1237259]
loss: 0.027051 0.008330 [411648/1237259]
loss: 0.029846 0.008225 [616448/1237259]
loss: 0.031041 0.008315 [821248/1237259]
loss: 0.025950 0.008291 [1026048/1237259]
loss: 0.022005 0.008175 [1230848/1237259]
Epoch 252
-------------------------------
loss: 0.026875 0.008366 [ 2048/1237259]
loss: 0.030727 0.008342 [206848/1237259]
loss: 0.024604 0.008246 [411648/1237259]
loss: 0.031376 0.008246 [616448/1237259]
loss: 0.027064 0.008289 [821248/1237259]
loss: 0.024923 0.008333 [1026048/1237259]
loss: 0.030564 0.008277 [1230848/1237259]
Epoch 253
-------------------------------
loss: 0.030637 0.008313 [ 2048/1237259]
loss: 0.035040 0.008336 [206848/1237259]
loss: 0.026582 0.008263 [411648/1237259]
loss: 0.030408 0.008316 [616448/1237259]
loss: 0.029324 0.008259 [821248/1237259]
loss: 0.023393 0.008295 [1026048/1237259]
loss: 0.027116 0.008272 [1230848/1237259]
Epoch 254
-------------------------------
loss: 0.025943 0.008295 [ 2048/1237259]
loss: 0.026131 0.008345 [206848/1237259]
loss: 0.033129 0.008294 [411648/1237259]
loss: 0.031673 0.008253 [616448/1237259]
loss: 0.035284 0.008240 [821248/1237259]
loss: 0.028324 0.008266 [1026048/1237259]
loss: 0.031072 0.008308 [1230848/1237259]
Epoch 255
-------------------------------
loss: 0.026967 0.008346 [ 2048/1237259]
loss: 0.037726 0.008324 [206848/1237259]
loss: 0.020491 0.008248 [411648/1237259]
loss: 0.035078 0.008366 [616448/1237259]
loss: 0.030968 0.008352 [821248/1237259]
loss: 0.026419 0.008451 [1026048/1237259]
loss: 0.026940 0.008259 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0609  
ndcg@20: 0.0499  
diversity: 0.1907  


Epoch 256
-------------------------------
loss: 0.027094 0.008301 [ 2048/1237259]
loss: 0.030406 0.008271 [206848/1237259]
loss: 0.028017 0.008345 [411648/1237259]
loss: 0.023754 0.008237 [616448/1237259]
loss: 0.032703 0.008310 [821248/1237259]
loss: 0.025259 0.008260 [1026048/1237259]
loss: 0.030366 0.008354 [1230848/1237259]
Epoch 257
-------------------------------
loss: 0.027666 0.008268 [ 2048/1237259]
loss: 0.029766 0.008397 [206848/1237259]
loss: 0.035184 0.008325 [411648/1237259]
loss: 0.027238 0.008348 [616448/1237259]
loss: 0.032126 0.008362 [821248/1237259]
loss: 0.030517 0.008409 [1026048/1237259]
loss: 0.034528 0.008350 [1230848/1237259]
Epoch 258
-------------------------------
loss: 0.030214 0.008359 [ 2048/1237259]
loss: 0.029393 0.008342 [206848/1237259]
loss: 0.029265 0.008422 [411648/1237259]
loss: 0.035169 0.008354 [616448/1237259]
loss: 0.028245 0.008482 [821248/1237259]
loss: 0.025327 0.008435 [1026048/1237259]
loss: 0.023553 0.008473 [1230848/1237259]
Epoch 259
-------------------------------
loss: 0.024763 0.008324 [ 2048/1237259]
loss: 0.027282 0.008387 [206848/1237259]
loss: 0.024223 0.008398 [411648/1237259]
loss: 0.028257 0.008442 [616448/1237259]
loss: 0.035685 0.008369 [821248/1237259]
loss: 0.019681 0.008412 [1026048/1237259]
loss: 0.031409 0.008390 [1230848/1237259]
Epoch 260
-------------------------------
loss: 0.024069 0.008364 [ 2048/1237259]
loss: 0.023624 0.008427 [206848/1237259]
loss: 0.030625 0.008341 [411648/1237259]
loss: 0.028143 0.008448 [616448/1237259]
loss: 0.023848 0.008441 [821248/1237259]
loss: 0.021496 0.008336 [1026048/1237259]
loss: 0.023847 0.008442 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0615  
ndcg@20: 0.0500  
diversity: 0.1908  


Epoch 261
-------------------------------
loss: 0.024083 0.008299 [ 2048/1237259]
loss: 0.029082 0.008375 [206848/1237259]
loss: 0.023670 0.008473 [411648/1237259]
loss: 0.025758 0.008382 [616448/1237259]
loss: 0.027748 0.008442 [821248/1237259]
loss: 0.030391 0.008454 [1026048/1237259]
loss: 0.023165 0.008452 [1230848/1237259]
Epoch 262
-------------------------------
loss: 0.034136 0.008449 [ 2048/1237259]
loss: 0.026659 0.008368 [206848/1237259]
loss: 0.027355 0.008440 [411648/1237259]
loss: 0.029375 0.008419 [616448/1237259]
loss: 0.022752 0.008447 [821248/1237259]
loss: 0.027273 0.008505 [1026048/1237259]
loss: 0.024060 0.008460 [1230848/1237259]
Epoch 263
-------------------------------
loss: 0.028345 0.008418 [ 2048/1237259]
loss: 0.024765 0.008462 [206848/1237259]
loss: 0.024982 0.008471 [411648/1237259]
loss: 0.024677 0.008444 [616448/1237259]
loss: 0.027735 0.008551 [821248/1237259]
loss: 0.029158 0.008475 [1026048/1237259]
loss: 0.036824 0.008496 [1230848/1237259]
Epoch 264
-------------------------------
loss: 0.024441 0.008553 [ 2048/1237259]
loss: 0.023819 0.008454 [206848/1237259]
loss: 0.026559 0.008481 [411648/1237259]
loss: 0.026460 0.008516 [616448/1237259]
loss: 0.024787 0.008490 [821248/1237259]
loss: 0.032074 0.008530 [1026048/1237259]
loss: 0.027365 0.008476 [1230848/1237259]
Epoch 265
-------------------------------
loss: 0.025867 0.008506 [ 2048/1237259]
loss: 0.027469 0.008455 [206848/1237259]
loss: 0.024901 0.008451 [411648/1237259]
loss: 0.029366 0.008549 [616448/1237259]
loss: 0.023512 0.008548 [821248/1237259]
loss: 0.027660 0.008463 [1026048/1237259]
loss: 0.022150 0.008514 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0500  
diversity: 0.1908  


Epoch 266
-------------------------------
loss: 0.026451 0.008502 [ 2048/1237259]
loss: 0.026927 0.008490 [206848/1237259]
loss: 0.028284 0.008430 [411648/1237259]
loss: 0.023893 0.008582 [616448/1237259]
loss: 0.026411 0.008496 [821248/1237259]
loss: 0.029244 0.008538 [1026048/1237259]
loss: 0.021779 0.008530 [1230848/1237259]
Epoch 267
-------------------------------
loss: 0.028481 0.008420 [ 2048/1237259]
loss: 0.025262 0.008616 [206848/1237259]
loss: 0.030464 0.008601 [411648/1237259]
loss: 0.030444 0.008469 [616448/1237259]
loss: 0.024648 0.008530 [821248/1237259]
loss: 0.024793 0.008451 [1026048/1237259]
loss: 0.022263 0.008526 [1230848/1237259]
Epoch 268
-------------------------------
loss: 0.023421 0.008563 [ 2048/1237259]
loss: 0.027996 0.008608 [206848/1237259]
loss: 0.024403 0.008443 [411648/1237259]
loss: 0.026525 0.008653 [616448/1237259]
loss: 0.026085 0.008522 [821248/1237259]
loss: 0.027506 0.008544 [1026048/1237259]
loss: 0.032458 0.008502 [1230848/1237259]
Epoch 269
-------------------------------
loss: 0.033635 0.008609 [ 2048/1237259]
loss: 0.027373 0.008497 [206848/1237259]
loss: 0.027216 0.008576 [411648/1237259]
loss: 0.027516 0.008550 [616448/1237259]
loss: 0.031771 0.008564 [821248/1237259]
loss: 0.023914 0.008602 [1026048/1237259]
loss: 0.025980 0.008498 [1230848/1237259]
Epoch 270
-------------------------------
loss: 0.026494 0.008622 [ 2048/1237259]
loss: 0.030812 0.008555 [206848/1237259]
loss: 0.027327 0.008463 [411648/1237259]
loss: 0.021515 0.008640 [616448/1237259]
loss: 0.024009 0.008540 [821248/1237259]
loss: 0.024713 0.008590 [1026048/1237259]
loss: 0.029505 0.008527 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0503  
diversity: 0.1914  


Epoch 271
-------------------------------
loss: 0.023378 0.008560 [ 2048/1237259]
loss: 0.023853 0.008546 [206848/1237259]
loss: 0.027148 0.008616 [411648/1237259]
loss: 0.023386 0.008653 [616448/1237259]
loss: 0.020384 0.008684 [821248/1237259]
loss: 0.026630 0.008510 [1026048/1237259]
loss: 0.031997 0.008678 [1230848/1237259]
Epoch 272
-------------------------------
loss: 0.023521 0.008699 [ 2048/1237259]
loss: 0.019559 0.008570 [206848/1237259]
loss: 0.027003 0.008631 [411648/1237259]
loss: 0.032591 0.008629 [616448/1237259]
loss: 0.029821 0.008612 [821248/1237259]
loss: 0.027764 0.008617 [1026048/1237259]
loss: 0.032020 0.008642 [1230848/1237259]
Epoch 273
-------------------------------
loss: 0.020321 0.008678 [ 2048/1237259]
loss: 0.029040 0.008629 [206848/1237259]
loss: 0.021555 0.008599 [411648/1237259]
loss: 0.027624 0.008548 [616448/1237259]
loss: 0.024837 0.008668 [821248/1237259]
loss: 0.029521 0.008550 [1026048/1237259]
loss: 0.032862 0.008636 [1230848/1237259]
Epoch 274
-------------------------------
loss: 0.034492 0.008652 [ 2048/1237259]
loss: 0.027627 0.008545 [206848/1237259]
loss: 0.021326 0.008649 [411648/1237259]
loss: 0.025628 0.008707 [616448/1237259]
loss: 0.021609 0.008724 [821248/1237259]
loss: 0.026454 0.008607 [1026048/1237259]
loss: 0.024161 0.008677 [1230848/1237259]
Epoch 275
-------------------------------
loss: 0.024490 0.008670 [ 2048/1237259]
loss: 0.026026 0.008602 [206848/1237259]
loss: 0.026376 0.008621 [411648/1237259]
loss: 0.029821 0.008664 [616448/1237259]
loss: 0.023248 0.008618 [821248/1237259]
loss: 0.031843 0.008705 [1026048/1237259]
loss: 0.027119 0.008610 [1230848/1237259]
Eval results: 
recall@20: 0.0613  
ndcg@20: 0.0502  
diversity: 0.1913  


Epoch 276
-------------------------------
loss: 0.024432 0.008668 [ 2048/1237259]
loss: 0.031047 0.008601 [206848/1237259]
loss: 0.025222 0.008704 [411648/1237259]
loss: 0.016841 0.008636 [616448/1237259]
loss: 0.022952 0.008696 [821248/1237259]
loss: 0.021965 0.008659 [1026048/1237259]
loss: 0.027164 0.008692 [1230848/1237259]
Epoch 277
-------------------------------
loss: 0.029960 0.008660 [ 2048/1237259]
loss: 0.029814 0.008702 [206848/1237259]
loss: 0.026276 0.008722 [411648/1237259]
loss: 0.027232 0.008655 [616448/1237259]
loss: 0.028965 0.008616 [821248/1237259]
loss: 0.028755 0.008721 [1026048/1237259]
loss: 0.024868 0.008566 [1230848/1237259]
Epoch 278
-------------------------------
loss: 0.030403 0.008624 [ 2048/1237259]
loss: 0.023361 0.008687 [206848/1237259]
loss: 0.029221 0.008787 [411648/1237259]
loss: 0.020298 0.008662 [616448/1237259]
loss: 0.031044 0.008684 [821248/1237259]
loss: 0.029716 0.008711 [1026048/1237259]
loss: 0.018158 0.008722 [1230848/1237259]
Epoch 279
-------------------------------
loss: 0.023142 0.008724 [ 2048/1237259]
loss: 0.032758 0.008679 [206848/1237259]
loss: 0.023883 0.008787 [411648/1237259]
loss: 0.030362 0.008767 [616448/1237259]
loss: 0.027668 0.008797 [821248/1237259]
loss: 0.023611 0.008741 [1026048/1237259]
loss: 0.021798 0.008741 [1230848/1237259]
Epoch 280
-------------------------------
loss: 0.020224 0.008763 [ 2048/1237259]
loss: 0.021641 0.008769 [206848/1237259]
loss: 0.030235 0.008764 [411648/1237259]
loss: 0.019397 0.008760 [616448/1237259]
loss: 0.023395 0.008791 [821248/1237259]
loss: 0.023265 0.008736 [1026048/1237259]
loss: 0.021136 0.008694 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0614  
ndcg@20: 0.0502  
diversity: 0.1915  


Epoch 281
-------------------------------
loss: 0.025980 0.008767 [ 2048/1237259]
loss: 0.023759 0.008678 [206848/1237259]
loss: 0.031921 0.008716 [411648/1237259]
loss: 0.032738 0.008790 [616448/1237259]
loss: 0.026501 0.008740 [821248/1237259]
loss: 0.028143 0.008727 [1026048/1237259]
loss: 0.028405 0.008820 [1230848/1237259]
Epoch 282
-------------------------------
loss: 0.020314 0.008717 [ 2048/1237259]
loss: 0.026687 0.008762 [206848/1237259]
loss: 0.021351 0.008783 [411648/1237259]
loss: 0.025702 0.008863 [616448/1237259]
loss: 0.025307 0.008787 [821248/1237259]
loss: 0.022545 0.008802 [1026048/1237259]
loss: 0.024250 0.008788 [1230848/1237259]
Epoch 283
-------------------------------
loss: 0.027792 0.008858 [ 2048/1237259]
loss: 0.025588 0.008813 [206848/1237259]
loss: 0.017045 0.008797 [411648/1237259]
loss: 0.024114 0.008735 [616448/1237259]
loss: 0.021874 0.008733 [821248/1237259]
loss: 0.018903 0.008731 [1026048/1237259]
loss: 0.029616 0.008816 [1230848/1237259]
Epoch 284
-------------------------------
loss: 0.023646 0.008855 [ 2048/1237259]
loss: 0.028342 0.008696 [206848/1237259]
loss: 0.025232 0.008755 [411648/1237259]
loss: 0.024246 0.008645 [616448/1237259]
loss: 0.028031 0.008790 [821248/1237259]
loss: 0.022035 0.008885 [1026048/1237259]
loss: 0.024895 0.008712 [1230848/1237259]
Epoch 285
-------------------------------
loss: 0.024764 0.008948 [ 2048/1237259]
loss: 0.025640 0.008862 [206848/1237259]
loss: 0.029430 0.008895 [411648/1237259]
loss: 0.025126 0.008811 [616448/1237259]
loss: 0.028996 0.008727 [821248/1237259]
loss: 0.021009 0.008747 [1026048/1237259]
loss: 0.022135 0.008803 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0618  
ndcg@20: 0.0507  
diversity: 0.1921  


Epoch 286
-------------------------------
loss: 0.020659 0.008835 [ 2048/1237259]
loss: 0.023311 0.008744 [206848/1237259]
loss: 0.024655 0.008897 [411648/1237259]
loss: 0.018867 0.008842 [616448/1237259]
loss: 0.025555 0.008819 [821248/1237259]
loss: 0.024450 0.008865 [1026048/1237259]
loss: 0.021613 0.008805 [1230848/1237259]
Epoch 287
-------------------------------
loss: 0.024527 0.008876 [ 2048/1237259]
loss: 0.024828 0.008780 [206848/1237259]
loss: 0.026548 0.008930 [411648/1237259]
loss: 0.028677 0.008789 [616448/1237259]
loss: 0.027329 0.008910 [821248/1237259]
loss: 0.022550 0.008808 [1026048/1237259]
loss: 0.032292 0.008852 [1230848/1237259]
Epoch 288
-------------------------------
loss: 0.024182 0.008852 [ 2048/1237259]
loss: 0.022473 0.008864 [206848/1237259]
loss: 0.026857 0.008777 [411648/1237259]
loss: 0.025168 0.008884 [616448/1237259]
loss: 0.034890 0.008899 [821248/1237259]
loss: 0.020724 0.008923 [1026048/1237259]
loss: 0.026619 0.008858 [1230848/1237259]
Epoch 289
-------------------------------
loss: 0.022224 0.008867 [ 2048/1237259]
loss: 0.023460 0.008887 [206848/1237259]
loss: 0.027789 0.008788 [411648/1237259]
loss: 0.026822 0.008880 [616448/1237259]
loss: 0.024925 0.008879 [821248/1237259]
loss: 0.021980 0.008875 [1026048/1237259]
loss: 0.022308 0.008904 [1230848/1237259]
Epoch 290
-------------------------------
loss: 0.027351 0.008842 [ 2048/1237259]
loss: 0.034086 0.008870 [206848/1237259]
loss: 0.026960 0.008869 [411648/1237259]
loss: 0.024615 0.008865 [616448/1237259]
loss: 0.018992 0.008847 [821248/1237259]
loss: 0.021262 0.008884 [1026048/1237259]
loss: 0.036949 0.008919 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0618  
ndcg@20: 0.0506  
diversity: 0.1921  


Epoch 291
-------------------------------
loss: 0.018554 0.008817 [ 2048/1237259]
loss: 0.023825 0.008865 [206848/1237259]
loss: 0.022121 0.008897 [411648/1237259]
loss: 0.028272 0.008899 [616448/1237259]
loss: 0.024627 0.008936 [821248/1237259]
loss: 0.026914 0.008960 [1026048/1237259]
loss: 0.021173 0.008922 [1230848/1237259]
Epoch 292
-------------------------------
loss: 0.028040 0.008953 [ 2048/1237259]
loss: 0.022704 0.008779 [206848/1237259]
loss: 0.019811 0.008944 [411648/1237259]
loss: 0.023345 0.009008 [616448/1237259]
loss: 0.019919 0.008971 [821248/1237259]
loss: 0.021476 0.008823 [1026048/1237259]
loss: 0.022609 0.008882 [1230848/1237259]
Epoch 293
-------------------------------
loss: 0.023335 0.008962 [ 2048/1237259]
loss: 0.032704 0.008922 [206848/1237259]
loss: 0.022105 0.008913 [411648/1237259]
loss: 0.025672 0.009066 [616448/1237259]
loss: 0.026682 0.008986 [821248/1237259]
loss: 0.023280 0.008967 [1026048/1237259]
loss: 0.022569 0.008967 [1230848/1237259]
Epoch 294
-------------------------------
loss: 0.028264 0.008957 [ 2048/1237259]
loss: 0.031261 0.008919 [206848/1237259]
loss: 0.021682 0.009048 [411648/1237259]
loss: 0.026869 0.009022 [616448/1237259]
loss: 0.027294 0.008942 [821248/1237259]
loss: 0.020894 0.008979 [1026048/1237259]
loss: 0.023546 0.008922 [1230848/1237259]
Epoch 295
-------------------------------
loss: 0.022299 0.008886 [ 2048/1237259]
loss: 0.031650 0.008988 [206848/1237259]
loss: 0.023444 0.008843 [411648/1237259]
loss: 0.021979 0.009079 [616448/1237259]
loss: 0.027824 0.008982 [821248/1237259]
loss: 0.024203 0.008975 [1026048/1237259]
loss: 0.017530 0.008889 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0623  
ndcg@20: 0.0508  
diversity: 0.1920  


Epoch 296
-------------------------------
loss: 0.018762 0.008980 [ 2048/1237259]
loss: 0.027420 0.008925 [206848/1237259]
loss: 0.028043 0.008938 [411648/1237259]
loss: 0.024792 0.009080 [616448/1237259]
loss: 0.027983 0.009046 [821248/1237259]
loss: 0.017879 0.008968 [1026048/1237259]
loss: 0.021650 0.009013 [1230848/1237259]
Epoch 297
-------------------------------
loss: 0.023655 0.009105 [ 2048/1237259]
loss: 0.025378 0.008990 [206848/1237259]
loss: 0.022373 0.008954 [411648/1237259]
loss: 0.019745 0.008995 [616448/1237259]
loss: 0.021621 0.009134 [821248/1237259]
loss: 0.028675 0.009006 [1026048/1237259]
loss: 0.030484 0.008984 [1230848/1237259]
Epoch 298
-------------------------------
loss: 0.033239 0.009009 [ 2048/1237259]
loss: 0.029988 0.009025 [206848/1237259]
loss: 0.024540 0.009005 [411648/1237259]
loss: 0.028067 0.009054 [616448/1237259]
loss: 0.025928 0.009045 [821248/1237259]
loss: 0.028600 0.009040 [1026048/1237259]
loss: 0.029810 0.009035 [1230848/1237259]
Epoch 299
-------------------------------
loss: 0.027090 0.008984 [ 2048/1237259]
loss: 0.020275 0.008978 [206848/1237259]
loss: 0.023484 0.009020 [411648/1237259]
loss: 0.020688 0.008947 [616448/1237259]
loss: 0.022957 0.009010 [821248/1237259]
loss: 0.024188 0.009027 [1026048/1237259]
loss: 0.026641 0.008991 [1230848/1237259]
Epoch 300
-------------------------------
loss: 0.022297 0.008940 [ 2048/1237259]
loss: 0.031385 0.009123 [206848/1237259]
loss: 0.020258 0.009051 [411648/1237259]
loss: 0.028867 0.008974 [616448/1237259]
loss: 0.024401 0.008952 [821248/1237259]
loss: 0.033536 0.009069 [1026048/1237259]
loss: 0.022615 0.009015 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0619  
ndcg@20: 0.0506  
diversity: 0.1924  


Epoch 301
-------------------------------
loss: 0.022764 0.009087 [ 2048/1237259]
loss: 0.035176 0.009129 [206848/1237259]
loss: 0.026969 0.009078 [411648/1237259]
loss: 0.018059 0.009129 [616448/1237259]
loss: 0.026129 0.009125 [821248/1237259]
loss: 0.021790 0.009086 [1026048/1237259]
loss: 0.025337 0.009039 [1230848/1237259]
Epoch 302
-------------------------------
loss: 0.026667 0.008985 [ 2048/1237259]
loss: 0.021725 0.009106 [206848/1237259]
loss: 0.023026 0.009082 [411648/1237259]
loss: 0.021074 0.009057 [616448/1237259]
loss: 0.026794 0.009099 [821248/1237259]
loss: 0.034519 0.009059 [1026048/1237259]
loss: 0.023359 0.009000 [1230848/1237259]
Epoch 303
-------------------------------
loss: 0.027766 0.009099 [ 2048/1237259]
loss: 0.025626 0.009217 [206848/1237259]
loss: 0.026059 0.009040 [411648/1237259]
loss: 0.018857 0.009126 [616448/1237259]
loss: 0.023941 0.009019 [821248/1237259]
loss: 0.020264 0.009200 [1026048/1237259]
loss: 0.021887 0.009090 [1230848/1237259]
Epoch 304
-------------------------------
loss: 0.017651 0.009095 [ 2048/1237259]
loss: 0.029087 0.009110 [206848/1237259]
loss: 0.028004 0.009172 [411648/1237259]
loss: 0.026186 0.009098 [616448/1237259]
loss: 0.023868 0.009091 [821248/1237259]
loss: 0.026701 0.009122 [1026048/1237259]
loss: 0.025779 0.009101 [1230848/1237259]
Epoch 305
-------------------------------
loss: 0.021760 0.009169 [ 2048/1237259]
loss: 0.020915 0.009131 [206848/1237259]
loss: 0.024375 0.009037 [411648/1237259]
loss: 0.021381 0.009178 [616448/1237259]
loss: 0.023752 0.009149 [821248/1237259]
loss: 0.029365 0.009081 [1026048/1237259]
loss: 0.022879 0.009109 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0622  
ndcg@20: 0.0507  
diversity: 0.1924  


Epoch 306
-------------------------------
loss: 0.028246 0.009088 [ 2048/1237259]
loss: 0.022689 0.009187 [206848/1237259]
loss: 0.022426 0.009102 [411648/1237259]
loss: 0.019589 0.009032 [616448/1237259]
loss: 0.027103 0.009142 [821248/1237259]
loss: 0.027514 0.009163 [1026048/1237259]
loss: 0.019984 0.009121 [1230848/1237259]
Epoch 307
-------------------------------
loss: 0.020986 0.009092 [ 2048/1237259]
loss: 0.030541 0.009219 [206848/1237259]
loss: 0.022873 0.009231 [411648/1237259]
loss: 0.020541 0.009093 [616448/1237259]
loss: 0.024142 0.009134 [821248/1237259]
loss: 0.018956 0.009085 [1026048/1237259]
loss: 0.017657 0.009175 [1230848/1237259]
Epoch 308
-------------------------------
loss: 0.022889 0.009218 [ 2048/1237259]
loss: 0.023156 0.009182 [206848/1237259]
loss: 0.021885 0.009243 [411648/1237259]
loss: 0.029829 0.009176 [616448/1237259]
loss: 0.024005 0.009091 [821248/1237259]
loss: 0.025319 0.009138 [1026048/1237259]
loss: 0.024846 0.009193 [1230848/1237259]
Epoch 309
-------------------------------
loss: 0.023266 0.009134 [ 2048/1237259]
loss: 0.021521 0.009172 [206848/1237259]
loss: 0.028236 0.009327 [411648/1237259]
loss: 0.029882 0.009191 [616448/1237259]
loss: 0.019493 0.009213 [821248/1237259]
loss: 0.026798 0.009257 [1026048/1237259]
loss: 0.023336 0.009293 [1230848/1237259]
Epoch 310
-------------------------------
loss: 0.027359 0.009252 [ 2048/1237259]
loss: 0.017050 0.009257 [206848/1237259]
loss: 0.026872 0.009110 [411648/1237259]
loss: 0.020879 0.009204 [616448/1237259]
loss: 0.026702 0.009111 [821248/1237259]
loss: 0.025171 0.009288 [1026048/1237259]
loss: 0.022382 0.009131 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0623  
ndcg@20: 0.0508  
diversity: 0.1927  


Epoch 311
-------------------------------
loss: 0.022081 0.009220 [ 2048/1237259]
loss: 0.031470 0.009131 [206848/1237259]
loss: 0.025095 0.009343 [411648/1237259]
loss: 0.017589 0.009231 [616448/1237259]
loss: 0.025065 0.009287 [821248/1237259]
loss: 0.021965 0.009170 [1026048/1237259]
loss: 0.023071 0.009214 [1230848/1237259]
Epoch 312
-------------------------------
loss: 0.020759 0.009377 [ 2048/1237259]
loss: 0.021805 0.009248 [206848/1237259]
loss: 0.021016 0.009260 [411648/1237259]
loss: 0.023473 0.009194 [616448/1237259]
loss: 0.027109 0.009208 [821248/1237259]
loss: 0.023940 0.009309 [1026048/1237259]
loss: 0.023008 0.009328 [1230848/1237259]
Epoch 313
-------------------------------
loss: 0.034754 0.009307 [ 2048/1237259]
loss: 0.025447 0.009157 [206848/1237259]
loss: 0.023106 0.009137 [411648/1237259]
loss: 0.021823 0.009239 [616448/1237259]
loss: 0.023823 0.009221 [821248/1237259]
loss: 0.021117 0.009290 [1026048/1237259]
loss: 0.020760 0.009227 [1230848/1237259]
Epoch 314
-------------------------------
loss: 0.025230 0.009224 [ 2048/1237259]
loss: 0.023221 0.009289 [206848/1237259]
loss: 0.021650 0.009219 [411648/1237259]
loss: 0.020806 0.009219 [616448/1237259]
loss: 0.023388 0.009292 [821248/1237259]
loss: 0.029050 0.009245 [1026048/1237259]
loss: 0.025665 0.009180 [1230848/1237259]
Epoch 315
-------------------------------
loss: 0.025809 0.009224 [ 2048/1237259]
loss: 0.021080 0.009236 [206848/1237259]
loss: 0.023536 0.009339 [411648/1237259]
loss: 0.018900 0.009335 [616448/1237259]
loss: 0.026255 0.009286 [821248/1237259]
loss: 0.024082 0.009238 [1026048/1237259]
loss: 0.028905 0.009383 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0624  
ndcg@20: 0.0509  
diversity: 0.1928  


Epoch 316
-------------------------------
loss: 0.023763 0.009286 [ 2048/1237259]
loss: 0.024023 0.009397 [206848/1237259]
loss: 0.028076 0.009322 [411648/1237259]
loss: 0.021576 0.009315 [616448/1237259]
loss: 0.024004 0.009278 [821248/1237259]
loss: 0.020196 0.009297 [1026048/1237259]
loss: 0.023294 0.009321 [1230848/1237259]
Epoch 317
-------------------------------
loss: 0.024538 0.009257 [ 2048/1237259]
loss: 0.024491 0.009290 [206848/1237259]
loss: 0.028602 0.009332 [411648/1237259]
loss: 0.024632 0.009366 [616448/1237259]
loss: 0.018266 0.009342 [821248/1237259]
loss: 0.025954 0.009340 [1026048/1237259]
loss: 0.016804 0.009231 [1230848/1237259]
Epoch 318
-------------------------------
loss: 0.019198 0.009268 [ 2048/1237259]
loss: 0.019486 0.009262 [206848/1237259]
loss: 0.024328 0.009441 [411648/1237259]
loss: 0.031819 0.009289 [616448/1237259]
loss: 0.023883 0.009274 [821248/1237259]
loss: 0.020056 0.009306 [1026048/1237259]
loss: 0.027604 0.009313 [1230848/1237259]
Epoch 319
-------------------------------
loss: 0.023666 0.009397 [ 2048/1237259]
loss: 0.029029 0.009283 [206848/1237259]
loss: 0.023189 0.009342 [411648/1237259]
loss: 0.025630 0.009300 [616448/1237259]
loss: 0.022008 0.009250 [821248/1237259]
loss: 0.032229 0.009392 [1026048/1237259]
loss: 0.026224 0.009359 [1230848/1237259]
Epoch 320
-------------------------------
loss: 0.022453 0.009253 [ 2048/1237259]
loss: 0.025221 0.009358 [206848/1237259]
loss: 0.025101 0.009385 [411648/1237259]
loss: 0.023726 0.009417 [616448/1237259]
loss: 0.020186 0.009308 [821248/1237259]
loss: 0.025569 0.009353 [1026048/1237259]
loss: 0.022377 0.009346 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0623  
ndcg@20: 0.0510  
diversity: 0.1929  


Epoch 321
-------------------------------
loss: 0.021252 0.009377 [ 2048/1237259]
loss: 0.025435 0.009276 [206848/1237259]
loss: 0.023323 0.009311 [411648/1237259]
loss: 0.029108 0.009440 [616448/1237259]
loss: 0.018299 0.009275 [821248/1237259]
loss: 0.020140 0.009319 [1026048/1237259]
loss: 0.023749 0.009437 [1230848/1237259]
Epoch 322
-------------------------------
loss: 0.023164 0.009403 [ 2048/1237259]
loss: 0.018823 0.009359 [206848/1237259]
loss: 0.024304 0.009387 [411648/1237259]
loss: 0.018371 0.009406 [616448/1237259]
loss: 0.021894 0.009345 [821248/1237259]
loss: 0.025269 0.009377 [1026048/1237259]
loss: 0.022645 0.009397 [1230848/1237259]
Epoch 323
-------------------------------
loss: 0.024321 0.009388 [ 2048/1237259]
loss: 0.023441 0.009339 [206848/1237259]
loss: 0.019583 0.009304 [411648/1237259]
loss: 0.023350 0.009412 [616448/1237259]
loss: 0.019317 0.009342 [821248/1237259]
loss: 0.024205 0.009380 [1026048/1237259]
loss: 0.025844 0.009397 [1230848/1237259]
Epoch 324
-------------------------------
loss: 0.021107 0.009389 [ 2048/1237259]
loss: 0.025140 0.009313 [206848/1237259]
loss: 0.020784 0.009320 [411648/1237259]
loss: 0.025351 0.009311 [616448/1237259]
loss: 0.016123 0.009293 [821248/1237259]
loss: 0.023951 0.009491 [1026048/1237259]
loss: 0.028896 0.009384 [1230848/1237259]
Epoch 325
-------------------------------
loss: 0.022318 0.009418 [ 2048/1237259]
loss: 0.023777 0.009482 [206848/1237259]
loss: 0.019355 0.009425 [411648/1237259]
loss: 0.023633 0.009384 [616448/1237259]
loss: 0.024214 0.009370 [821248/1237259]
loss: 0.020784 0.009631 [1026048/1237259]
loss: 0.020291 0.009405 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0624  
ndcg@20: 0.0510  
diversity: 0.1930  


Epoch 326
-------------------------------
loss: 0.020505 0.009418 [ 2048/1237259]
loss: 0.020255 0.009475 [206848/1237259]
loss: 0.022162 0.009539 [411648/1237259]
loss: 0.022001 0.009412 [616448/1237259]
loss: 0.021472 0.009457 [821248/1237259]
loss: 0.024782 0.009500 [1026048/1237259]
loss: 0.017912 0.009415 [1230848/1237259]
Epoch 327
-------------------------------
loss: 0.019078 0.009431 [ 2048/1237259]
loss: 0.024565 0.009390 [206848/1237259]
loss: 0.026465 0.009480 [411648/1237259]
loss: 0.023494 0.009438 [616448/1237259]
loss: 0.017955 0.009426 [821248/1237259]
loss: 0.031525 0.009540 [1026048/1237259]
loss: 0.026333 0.009429 [1230848/1237259]
Epoch 328
-------------------------------
loss: 0.019427 0.009456 [ 2048/1237259]
loss: 0.029224 0.009390 [206848/1237259]
loss: 0.026810 0.009383 [411648/1237259]
loss: 0.021173 0.009526 [616448/1237259]
loss: 0.025723 0.009542 [821248/1237259]
loss: 0.022439 0.009557 [1026048/1237259]
loss: 0.016866 0.009544 [1230848/1237259]
Epoch 329
-------------------------------
loss: 0.025053 0.009567 [ 2048/1237259]
loss: 0.019053 0.009457 [206848/1237259]
loss: 0.020455 0.009514 [411648/1237259]
loss: 0.024173 0.009546 [616448/1237259]
loss: 0.025708 0.009448 [821248/1237259]
loss: 0.020892 0.009458 [1026048/1237259]
loss: 0.025191 0.009419 [1230848/1237259]
Epoch 330
-------------------------------
loss: 0.017895 0.009553 [ 2048/1237259]
loss: 0.023321 0.009482 [206848/1237259]
loss: 0.020471 0.009506 [411648/1237259]
loss: 0.020795 0.009425 [616448/1237259]
loss: 0.022169 0.009481 [821248/1237259]
loss: 0.023120 0.009419 [1026048/1237259]
loss: 0.019216 0.009509 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0625  
ndcg@20: 0.0511  
diversity: 0.1934  


Epoch 331
-------------------------------
loss: 0.024578 0.009399 [ 2048/1237259]
loss: 0.025635 0.009433 [206848/1237259]
loss: 0.017901 0.009441 [411648/1237259]
loss: 0.025484 0.009467 [616448/1237259]
loss: 0.027050 0.009543 [821248/1237259]
loss: 0.020432 0.009486 [1026048/1237259]
loss: 0.016290 0.009566 [1230848/1237259]
Epoch 332
-------------------------------
loss: 0.021452 0.009439 [ 2048/1237259]
loss: 0.024093 0.009523 [206848/1237259]
loss: 0.028180 0.009599 [411648/1237259]
loss: 0.017277 0.009477 [616448/1237259]
loss: 0.029495 0.009508 [821248/1237259]
loss: 0.022725 0.009537 [1026048/1237259]
loss: 0.027728 0.009434 [1230848/1237259]
Epoch 333
-------------------------------
loss: 0.016330 0.009591 [ 2048/1237259]
loss: 0.025579 0.009522 [206848/1237259]
loss: 0.023005 0.009546 [411648/1237259]
loss: 0.028677 0.009518 [616448/1237259]
loss: 0.022236 0.009449 [821248/1237259]
loss: 0.028800 0.009622 [1026048/1237259]
loss: 0.024470 0.009577 [1230848/1237259]
Epoch 334
-------------------------------
loss: 0.021312 0.009439 [ 2048/1237259]
loss: 0.023944 0.009510 [206848/1237259]
loss: 0.022228 0.009566 [411648/1237259]
loss: 0.024338 0.009577 [616448/1237259]
loss: 0.023100 0.009432 [821248/1237259]
loss: 0.020693 0.009538 [1026048/1237259]
loss: 0.021480 0.009626 [1230848/1237259]
Epoch 335
-------------------------------
loss: 0.022241 0.009483 [ 2048/1237259]
loss: 0.021361 0.009466 [206848/1237259]
loss: 0.027885 0.009577 [411648/1237259]
loss: 0.018654 0.009548 [616448/1237259]
loss: 0.022234 0.009664 [821248/1237259]
loss: 0.025324 0.009492 [1026048/1237259]
loss: 0.016188 0.009549 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0625  
ndcg@20: 0.0512  
diversity: 0.1934  


Epoch 336
-------------------------------
loss: 0.023083 0.009590 [ 2048/1237259]
loss: 0.018793 0.009620 [206848/1237259]
loss: 0.021339 0.009519 [411648/1237259]
loss: 0.020060 0.009602 [616448/1237259]
loss: 0.017803 0.009572 [821248/1237259]
loss: 0.021347 0.009569 [1026048/1237259]
loss: 0.021822 0.009483 [1230848/1237259]
Epoch 337
-------------------------------
loss: 0.019712 0.009542 [ 2048/1237259]
loss: 0.023848 0.009652 [206848/1237259]
loss: 0.023500 0.009723 [411648/1237259]
loss: 0.021851 0.009640 [616448/1237259]
loss: 0.023717 0.009506 [821248/1237259]
loss: 0.025174 0.009580 [1026048/1237259]
loss: 0.022287 0.009601 [1230848/1237259]
Epoch 338
-------------------------------
loss: 0.026798 0.009621 [ 2048/1237259]
loss: 0.022887 0.009658 [206848/1237259]
loss: 0.028244 0.009561 [411648/1237259]
loss: 0.022815 0.009621 [616448/1237259]
loss: 0.024108 0.009655 [821248/1237259]
loss: 0.021861 0.009657 [1026048/1237259]
loss: 0.020715 0.009488 [1230848/1237259]
Epoch 339
-------------------------------
loss: 0.024522 0.009575 [ 2048/1237259]
loss: 0.022387 0.009561 [206848/1237259]
loss: 0.027721 0.009548 [411648/1237259]
loss: 0.015299 0.009670 [616448/1237259]
loss: 0.021888 0.009689 [821248/1237259]
loss: 0.026493 0.009689 [1026048/1237259]
loss: 0.019718 0.009630 [1230848/1237259]
Epoch 340
-------------------------------
loss: 0.030401 0.009695 [ 2048/1237259]
loss: 0.020977 0.009661 [206848/1237259]
loss: 0.023609 0.009595 [411648/1237259]
loss: 0.020837 0.009605 [616448/1237259]
loss: 0.020589 0.009587 [821248/1237259]
loss: 0.019351 0.009596 [1026048/1237259]
loss: 0.019872 0.009685 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0626  
ndcg@20: 0.0512  
diversity: 0.1935  


Epoch 341
-------------------------------
loss: 0.014509 0.009617 [ 2048/1237259]
loss: 0.023617 0.009701 [206848/1237259]
loss: 0.025292 0.009590 [411648/1237259]
loss: 0.024332 0.009579 [616448/1237259]
loss: 0.020337 0.009588 [821248/1237259]
loss: 0.016463 0.009601 [1026048/1237259]
loss: 0.020838 0.009687 [1230848/1237259]
Epoch 342
-------------------------------
loss: 0.020412 0.009740 [ 2048/1237259]
loss: 0.022023 0.009669 [206848/1237259]
loss: 0.025919 0.009637 [411648/1237259]
loss: 0.016062 0.009654 [616448/1237259]
loss: 0.019688 0.009631 [821248/1237259]
loss: 0.017171 0.009569 [1026048/1237259]
loss: 0.020544 0.009727 [1230848/1237259]
Epoch 343
-------------------------------
loss: 0.018755 0.009718 [ 2048/1237259]
loss: 0.021255 0.009684 [206848/1237259]
loss: 0.025726 0.009611 [411648/1237259]
loss: 0.022184 0.009641 [616448/1237259]
loss: 0.018422 0.009709 [821248/1237259]
loss: 0.025330 0.009684 [1026048/1237259]
loss: 0.027924 0.009707 [1230848/1237259]
Epoch 344
-------------------------------
loss: 0.022549 0.009641 [ 2048/1237259]
loss: 0.024509 0.009673 [206848/1237259]
loss: 0.021664 0.009723 [411648/1237259]
loss: 0.022121 0.009582 [616448/1237259]
loss: 0.022805 0.009598 [821248/1237259]
loss: 0.021641 0.009691 [1026048/1237259]
loss: 0.025289 0.009724 [1230848/1237259]
Epoch 345
-------------------------------
loss: 0.026355 0.009643 [ 2048/1237259]
loss: 0.022114 0.009721 [206848/1237259]
loss: 0.026804 0.009735 [411648/1237259]
loss: 0.018788 0.009704 [616448/1237259]
loss: 0.023795 0.009691 [821248/1237259]
loss: 0.022013 0.009639 [1026048/1237259]
loss: 0.018148 0.009676 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0628  
ndcg@20: 0.0514  
diversity: 0.1935  


Epoch 346
-------------------------------
loss: 0.020562 0.009757 [ 2048/1237259]
loss: 0.023283 0.009744 [206848/1237259]
loss: 0.020868 0.009690 [411648/1237259]
loss: 0.021677 0.009664 [616448/1237259]
loss: 0.019052 0.009703 [821248/1237259]
loss: 0.023358 0.009647 [1026048/1237259]
loss: 0.028886 0.009724 [1230848/1237259]
Epoch 347
-------------------------------
loss: 0.015355 0.009673 [ 2048/1237259]
loss: 0.018495 0.009729 [206848/1237259]
loss: 0.025411 0.009693 [411648/1237259]
loss: 0.024625 0.009730 [616448/1237259]
loss: 0.019674 0.009697 [821248/1237259]
loss: 0.024589 0.009729 [1026048/1237259]
loss: 0.021661 0.009678 [1230848/1237259]
Epoch 348
-------------------------------
loss: 0.021099 0.009691 [ 2048/1237259]
loss: 0.023015 0.009660 [206848/1237259]
loss: 0.020010 0.009724 [411648/1237259]
loss: 0.025509 0.009804 [616448/1237259]
loss: 0.017235 0.009769 [821248/1237259]
loss: 0.031172 0.009746 [1026048/1237259]
loss: 0.015863 0.009857 [1230848/1237259]
Epoch 349
-------------------------------
loss: 0.019968 0.009766 [ 2048/1237259]
loss: 0.021274 0.009700 [206848/1237259]
loss: 0.018930 0.009702 [411648/1237259]
loss: 0.014807 0.009779 [616448/1237259]
loss: 0.019117 0.009738 [821248/1237259]
loss: 0.022769 0.009771 [1026048/1237259]
loss: 0.023726 0.009762 [1230848/1237259]
Epoch 350
-------------------------------
loss: 0.015462 0.009833 [ 2048/1237259]
loss: 0.021728 0.009689 [206848/1237259]
loss: 0.014712 0.009718 [411648/1237259]
loss: 0.021131 0.009845 [616448/1237259]
loss: 0.020931 0.009751 [821248/1237259]
loss: 0.020550 0.009830 [1026048/1237259]
loss: 0.017742 0.009738 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0625  
ndcg@20: 0.0514  
diversity: 0.1937  


Epoch 351
-------------------------------
loss: 0.021582 0.009821 [ 2048/1237259]
loss: 0.026375 0.009727 [206848/1237259]
loss: 0.026083 0.009764 [411648/1237259]
loss: 0.021798 0.009872 [616448/1237259]
loss: 0.017924 0.009756 [821248/1237259]
loss: 0.019822 0.009776 [1026048/1237259]
loss: 0.025422 0.009692 [1230848/1237259]
Epoch 352
-------------------------------
loss: 0.019977 0.009840 [ 2048/1237259]
loss: 0.023807 0.009659 [206848/1237259]
loss: 0.019898 0.009864 [411648/1237259]
loss: 0.021375 0.009757 [616448/1237259]
loss: 0.015795 0.009770 [821248/1237259]
loss: 0.027555 0.009755 [1026048/1237259]
loss: 0.027858 0.009899 [1230848/1237259]
Epoch 353
-------------------------------
loss: 0.019121 0.009833 [ 2048/1237259]
loss: 0.015769 0.009869 [206848/1237259]
loss: 0.025540 0.009841 [411648/1237259]
loss: 0.020593 0.009713 [616448/1237259]
loss: 0.021153 0.009723 [821248/1237259]
loss: 0.025577 0.009780 [1026048/1237259]
loss: 0.019790 0.009858 [1230848/1237259]
Epoch 354
-------------------------------
loss: 0.016588 0.009774 [ 2048/1237259]
loss: 0.022959 0.009824 [206848/1237259]
loss: 0.017507 0.009839 [411648/1237259]
loss: 0.030392 0.009793 [616448/1237259]
loss: 0.019388 0.009739 [821248/1237259]
loss: 0.020768 0.009807 [1026048/1237259]
loss: 0.027876 0.009724 [1230848/1237259]
Epoch 355
-------------------------------
loss: 0.026806 0.009827 [ 2048/1237259]
loss: 0.023736 0.009780 [206848/1237259]
loss: 0.024965 0.009702 [411648/1237259]
loss: 0.020963 0.009783 [616448/1237259]
loss: 0.021656 0.009836 [821248/1237259]
loss: 0.017635 0.009828 [1026048/1237259]
loss: 0.022819 0.009898 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0627  
ndcg@20: 0.0515  
diversity: 0.1938  


Epoch 356
-------------------------------
loss: 0.017863 0.009833 [ 2048/1237259]
loss: 0.025581 0.009789 [206848/1237259]
loss: 0.026673 0.009808 [411648/1237259]
loss: 0.017368 0.009798 [616448/1237259]
loss: 0.021257 0.009749 [821248/1237259]
loss: 0.018388 0.009921 [1026048/1237259]
loss: 0.019370 0.009897 [1230848/1237259]
Epoch 357
-------------------------------
loss: 0.018958 0.009863 [ 2048/1237259]
loss: 0.024824 0.009900 [206848/1237259]
loss: 0.023610 0.009839 [411648/1237259]
loss: 0.022731 0.009935 [616448/1237259]
loss: 0.024550 0.009920 [821248/1237259]
loss: 0.020003 0.009881 [1026048/1237259]
loss: 0.023619 0.009671 [1230848/1237259]
Epoch 358
-------------------------------
loss: 0.018925 0.009789 [ 2048/1237259]
loss: 0.018206 0.009749 [206848/1237259]
loss: 0.020950 0.009776 [411648/1237259]
loss: 0.018594 0.009804 [616448/1237259]
loss: 0.029803 0.009737 [821248/1237259]
loss: 0.021433 0.009885 [1026048/1237259]
loss: 0.020095 0.009882 [1230848/1237259]
Epoch 359
-------------------------------
loss: 0.024509 0.009952 [ 2048/1237259]
loss: 0.022281 0.009860 [206848/1237259]
loss: 0.025614 0.009937 [411648/1237259]
loss: 0.024753 0.009832 [616448/1237259]
loss: 0.016006 0.009805 [821248/1237259]
loss: 0.016848 0.009816 [1026048/1237259]
loss: 0.025289 0.009847 [1230848/1237259]
Epoch 360
-------------------------------
loss: 0.023284 0.009878 [ 2048/1237259]
loss: 0.021621 0.009899 [206848/1237259]
loss: 0.016584 0.009799 [411648/1237259]
loss: 0.024046 0.009874 [616448/1237259]
loss: 0.022899 0.009947 [821248/1237259]
loss: 0.017491 0.009872 [1026048/1237259]
loss: 0.020551 0.009860 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0630  
ndcg@20: 0.0516  
diversity: 0.1940  


Epoch 361
-------------------------------
loss: 0.017833 0.009891 [ 2048/1237259]
loss: 0.026435 0.009893 [206848/1237259]
loss: 0.021892 0.009892 [411648/1237259]
loss: 0.015723 0.009867 [616448/1237259]
loss: 0.019752 0.009891 [821248/1237259]
loss: 0.025180 0.009855 [1026048/1237259]
loss: 0.020305 0.009924 [1230848/1237259]
Epoch 362
-------------------------------
loss: 0.018574 0.010020 [ 2048/1237259]
loss: 0.027248 0.009958 [206848/1237259]
loss: 0.024024 0.009925 [411648/1237259]
loss: 0.017210 0.009969 [616448/1237259]
loss: 0.017347 0.009964 [821248/1237259]
loss: 0.024761 0.009876 [1026048/1237259]
loss: 0.019019 0.009921 [1230848/1237259]
Epoch 363
-------------------------------
loss: 0.016907 0.009981 [ 2048/1237259]
loss: 0.022639 0.009914 [206848/1237259]
loss: 0.021148 0.009827 [411648/1237259]
loss: 0.020047 0.009971 [616448/1237259]
loss: 0.019434 0.009974 [821248/1237259]
loss: 0.029798 0.009953 [1026048/1237259]
loss: 0.018937 0.009878 [1230848/1237259]
Epoch 364
-------------------------------
loss: 0.023318 0.009915 [ 2048/1237259]
loss: 0.023518 0.009923 [206848/1237259]
loss: 0.023793 0.009904 [411648/1237259]
loss: 0.017723 0.010082 [616448/1237259]
loss: 0.023612 0.009968 [821248/1237259]
loss: 0.021337 0.009958 [1026048/1237259]
loss: 0.024308 0.009968 [1230848/1237259]
Epoch 365
-------------------------------
loss: 0.023900 0.010005 [ 2048/1237259]
loss: 0.019506 0.009907 [206848/1237259]
loss: 0.016895 0.009891 [411648/1237259]
loss: 0.022647 0.009988 [616448/1237259]
loss: 0.015827 0.009893 [821248/1237259]
loss: 0.019485 0.009954 [1026048/1237259]
loss: 0.018293 0.009949 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0518  
diversity: 0.1939  


Epoch 366
-------------------------------
loss: 0.027244 0.009960 [ 2048/1237259]
loss: 0.020788 0.009947 [206848/1237259]
loss: 0.027840 0.009927 [411648/1237259]
loss: 0.021943 0.010001 [616448/1237259]
loss: 0.018464 0.009925 [821248/1237259]
loss: 0.022250 0.009924 [1026048/1237259]
loss: 0.019971 0.009938 [1230848/1237259]
Epoch 367
-------------------------------
loss: 0.027042 0.009978 [ 2048/1237259]
loss: 0.022110 0.009850 [206848/1237259]
loss: 0.017867 0.010050 [411648/1237259]
loss: 0.025139 0.010008 [616448/1237259]
loss: 0.023820 0.009898 [821248/1237259]
loss: 0.019793 0.009985 [1026048/1237259]
loss: 0.022237 0.009953 [1230848/1237259]
Epoch 368
-------------------------------
loss: 0.016484 0.010031 [ 2048/1237259]
loss: 0.020347 0.009943 [206848/1237259]
loss: 0.022781 0.010000 [411648/1237259]
loss: 0.023562 0.010067 [616448/1237259]
loss: 0.023336 0.009967 [821248/1237259]
loss: 0.016886 0.010040 [1026048/1237259]
loss: 0.028793 0.009928 [1230848/1237259]
Epoch 369
-------------------------------
loss: 0.018394 0.010008 [ 2048/1237259]
loss: 0.022542 0.010102 [206848/1237259]
loss: 0.020258 0.009910 [411648/1237259]
loss: 0.019304 0.010017 [616448/1237259]
loss: 0.017333 0.010010 [821248/1237259]
loss: 0.017742 0.010083 [1026048/1237259]
loss: 0.023707 0.010022 [1230848/1237259]
Epoch 370
-------------------------------
loss: 0.024725 0.010067 [ 2048/1237259]
loss: 0.019560 0.010069 [206848/1237259]
loss: 0.013367 0.010035 [411648/1237259]
loss: 0.015492 0.009901 [616448/1237259]
loss: 0.014386 0.010045 [821248/1237259]
loss: 0.023123 0.010063 [1026048/1237259]
loss: 0.020787 0.010014 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0518  
diversity: 0.1940  


Epoch 371
-------------------------------
loss: 0.013889 0.010033 [ 2048/1237259]
loss: 0.020619 0.010022 [206848/1237259]
loss: 0.017704 0.010095 [411648/1237259]
loss: 0.021749 0.010026 [616448/1237259]
loss: 0.024420 0.009953 [821248/1237259]
loss: 0.027220 0.010096 [1026048/1237259]
loss: 0.021563 0.009996 [1230848/1237259]
Epoch 372
-------------------------------
loss: 0.020764 0.009980 [ 2048/1237259]
loss: 0.018588 0.010007 [206848/1237259]
loss: 0.016599 0.010060 [411648/1237259]
loss: 0.015238 0.010087 [616448/1237259]
loss: 0.017108 0.010038 [821248/1237259]
loss: 0.020369 0.009948 [1026048/1237259]
loss: 0.018260 0.010057 [1230848/1237259]
Epoch 373
-------------------------------
loss: 0.019304 0.010016 [ 2048/1237259]
loss: 0.027088 0.009975 [206848/1237259]
loss: 0.019235 0.010077 [411648/1237259]
loss: 0.020224 0.010092 [616448/1237259]
loss: 0.021725 0.010235 [821248/1237259]
loss: 0.024763 0.010055 [1026048/1237259]
loss: 0.022894 0.010104 [1230848/1237259]
Epoch 374
-------------------------------
loss: 0.019993 0.010003 [ 2048/1237259]
loss: 0.020047 0.009962 [206848/1237259]
loss: 0.022551 0.010037 [411648/1237259]
loss: 0.019789 0.010001 [616448/1237259]
loss: 0.018270 0.010060 [821248/1237259]
loss: 0.021043 0.010017 [1026048/1237259]
loss: 0.016294 0.010145 [1230848/1237259]
Epoch 375
-------------------------------
loss: 0.021668 0.010044 [ 2048/1237259]
loss: 0.015627 0.010148 [206848/1237259]
loss: 0.020816 0.010157 [411648/1237259]
loss: 0.022623 0.010047 [616448/1237259]
loss: 0.023988 0.010085 [821248/1237259]
loss: 0.019636 0.010171 [1026048/1237259]
loss: 0.029296 0.010149 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0517  
diversity: 0.1941  


Epoch 376
-------------------------------
loss: 0.016086 0.010074 [ 2048/1237259]
loss: 0.016653 0.010081 [206848/1237259]
loss: 0.022001 0.010060 [411648/1237259]
loss: 0.018205 0.010078 [616448/1237259]
loss: 0.014843 0.009965 [821248/1237259]
loss: 0.020814 0.010117 [1026048/1237259]
loss: 0.024376 0.010086 [1230848/1237259]
Epoch 377
-------------------------------
loss: 0.023088 0.009997 [ 2048/1237259]
loss: 0.017178 0.010063 [206848/1237259]
loss: 0.018897 0.010035 [411648/1237259]
loss: 0.018502 0.010124 [616448/1237259]
loss: 0.021185 0.010139 [821248/1237259]
loss: 0.021725 0.010021 [1026048/1237259]
loss: 0.016839 0.010139 [1230848/1237259]
Epoch 378
-------------------------------
loss: 0.019961 0.010114 [ 2048/1237259]
loss: 0.017733 0.010106 [206848/1237259]
loss: 0.023789 0.010223 [411648/1237259]
loss: 0.024520 0.010112 [616448/1237259]
loss: 0.021044 0.010219 [821248/1237259]
loss: 0.023500 0.010099 [1026048/1237259]
loss: 0.017075 0.010100 [1230848/1237259]
Epoch 379
-------------------------------
loss: 0.015404 0.010101 [ 2048/1237259]
loss: 0.020783 0.010159 [206848/1237259]
loss: 0.021431 0.010154 [411648/1237259]
loss: 0.022929 0.010212 [616448/1237259]
loss: 0.020735 0.010148 [821248/1237259]
loss: 0.020281 0.010188 [1026048/1237259]
loss: 0.020653 0.010158 [1230848/1237259]
Epoch 380
-------------------------------
loss: 0.017967 0.010091 [ 2048/1237259]
loss: 0.021189 0.010152 [206848/1237259]
loss: 0.019325 0.010187 [411648/1237259]
loss: 0.021029 0.010185 [616448/1237259]
loss: 0.020998 0.010130 [821248/1237259]
loss: 0.023392 0.010210 [1026048/1237259]
loss: 0.014099 0.010145 [1230848/1237259]
Eval results: 
recall@20: 0.0629  
ndcg@20: 0.0518  
diversity: 0.1940  


Epoch 381
-------------------------------
loss: 0.018312 0.010152 [ 2048/1237259]
loss: 0.020876 0.010195 [206848/1237259]
loss: 0.021496 0.010119 [411648/1237259]
loss: 0.015790 0.010128 [616448/1237259]
loss: 0.020397 0.010125 [821248/1237259]
loss: 0.024190 0.010144 [1026048/1237259]
loss: 0.021166 0.010071 [1230848/1237259]
Epoch 382
-------------------------------
loss: 0.017235 0.010134 [ 2048/1237259]
loss: 0.022632 0.010064 [206848/1237259]
loss: 0.016506 0.010175 [411648/1237259]
loss: 0.014759 0.010102 [616448/1237259]
loss: 0.017985 0.010111 [821248/1237259]
loss: 0.019669 0.010202 [1026048/1237259]
loss: 0.022589 0.010068 [1230848/1237259]
Epoch 383
-------------------------------
loss: 0.015381 0.010112 [ 2048/1237259]
loss: 0.025049 0.010272 [206848/1237259]
loss: 0.023098 0.010134 [411648/1237259]
loss: 0.022521 0.010184 [616448/1237259]
loss: 0.024043 0.010193 [821248/1237259]
loss: 0.020617 0.010178 [1026048/1237259]
loss: 0.022832 0.010170 [1230848/1237259]
Epoch 384
-------------------------------
loss: 0.018061 0.010123 [ 2048/1237259]
loss: 0.022773 0.010112 [206848/1237259]
loss: 0.025091 0.010245 [411648/1237259]
loss: 0.019901 0.010155 [616448/1237259]
loss: 0.017142 0.010114 [821248/1237259]
loss: 0.023141 0.010259 [1026048/1237259]
loss: 0.017724 0.010279 [1230848/1237259]
Epoch 385
-------------------------------
loss: 0.018720 0.010331 [ 2048/1237259]
loss: 0.021660 0.010176 [206848/1237259]
loss: 0.024374 0.010197 [411648/1237259]
loss: 0.019645 0.010156 [616448/1237259]
loss: 0.021119 0.010314 [821248/1237259]
loss: 0.019696 0.010155 [1026048/1237259]
loss: 0.024178 0.010249 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0632  
ndcg@20: 0.0520  
diversity: 0.1943  


Epoch 386
-------------------------------
loss: 0.023446 0.010066 [ 2048/1237259]
loss: 0.023651 0.010103 [206848/1237259]
loss: 0.020047 0.010190 [411648/1237259]
loss: 0.017313 0.010128 [616448/1237259]
loss: 0.027700 0.010142 [821248/1237259]
loss: 0.023336 0.010089 [1026048/1237259]
loss: 0.020770 0.010288 [1230848/1237259]
Epoch 387
-------------------------------
loss: 0.019297 0.010192 [ 2048/1237259]
loss: 0.017558 0.010185 [206848/1237259]
loss: 0.017660 0.010195 [411648/1237259]
loss: 0.021359 0.010269 [616448/1237259]
loss: 0.022629 0.010165 [821248/1237259]
loss: 0.018927 0.010297 [1026048/1237259]
loss: 0.020499 0.010259 [1230848/1237259]
Epoch 388
-------------------------------
loss: 0.021220 0.010143 [ 2048/1237259]
loss: 0.019914 0.010306 [206848/1237259]
loss: 0.023859 0.010222 [411648/1237259]
loss: 0.019747 0.010234 [616448/1237259]
loss: 0.025117 0.010191 [821248/1237259]
loss: 0.020342 0.010099 [1026048/1237259]
loss: 0.021531 0.010257 [1230848/1237259]
Epoch 389
-------------------------------
loss: 0.025226 0.010363 [ 2048/1237259]
loss: 0.016545 0.010378 [206848/1237259]
loss: 0.021637 0.010300 [411648/1237259]
loss: 0.029008 0.010261 [616448/1237259]
loss: 0.023721 0.010303 [821248/1237259]
loss: 0.022760 0.010077 [1026048/1237259]
loss: 0.020121 0.010190 [1230848/1237259]
Epoch 390
-------------------------------
loss: 0.016647 0.010273 [ 2048/1237259]
loss: 0.015987 0.010189 [206848/1237259]
loss: 0.018962 0.010262 [411648/1237259]
loss: 0.019395 0.010217 [616448/1237259]
loss: 0.017976 0.010258 [821248/1237259]
loss: 0.023445 0.010255 [1026048/1237259]
loss: 0.021343 0.010296 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0636  
ndcg@20: 0.0520  
diversity: 0.1944  


Epoch 391
-------------------------------
loss: 0.014544 0.010322 [ 2048/1237259]
loss: 0.019797 0.010206 [206848/1237259]
loss: 0.025400 0.010297 [411648/1237259]
loss: 0.023724 0.010152 [616448/1237259]
loss: 0.020115 0.010252 [821248/1237259]
loss: 0.015623 0.010191 [1026048/1237259]
loss: 0.019508 0.010240 [1230848/1237259]
Epoch 392
-------------------------------
loss: 0.022051 0.010248 [ 2048/1237259]
loss: 0.026204 0.010268 [206848/1237259]
loss: 0.024408 0.010329 [411648/1237259]
loss: 0.018737 0.010213 [616448/1237259]
loss: 0.017346 0.010311 [821248/1237259]
loss: 0.021949 0.010332 [1026048/1237259]
loss: 0.016818 0.010336 [1230848/1237259]
Epoch 393
-------------------------------
loss: 0.015725 0.010231 [ 2048/1237259]
loss: 0.020030 0.010414 [206848/1237259]
loss: 0.024225 0.010244 [411648/1237259]
loss: 0.019119 0.010172 [616448/1237259]
loss: 0.023481 0.010369 [821248/1237259]
loss: 0.015417 0.010277 [1026048/1237259]
loss: 0.016721 0.010176 [1230848/1237259]
Epoch 394
-------------------------------
loss: 0.019392 0.010290 [ 2048/1237259]
loss: 0.021239 0.010353 [206848/1237259]
loss: 0.023545 0.010236 [411648/1237259]
loss: 0.017137 0.010158 [616448/1237259]
loss: 0.022956 0.010320 [821248/1237259]
loss: 0.019581 0.010271 [1026048/1237259]
loss: 0.022645 0.010150 [1230848/1237259]
Epoch 395
-------------------------------
loss: 0.017113 0.010383 [ 2048/1237259]
loss: 0.017690 0.010232 [206848/1237259]
loss: 0.021067 0.010253 [411648/1237259]
loss: 0.017369 0.010288 [616448/1237259]
loss: 0.017663 0.010237 [821248/1237259]
loss: 0.013845 0.010233 [1026048/1237259]
loss: 0.015420 0.010171 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0635  
ndcg@20: 0.0519  
diversity: 0.1946  


Epoch 396
-------------------------------
loss: 0.019715 0.010358 [ 2048/1237259]
loss: 0.021535 0.010208 [206848/1237259]
loss: 0.021606 0.010280 [411648/1237259]
loss: 0.021520 0.010217 [616448/1237259]
loss: 0.019681 0.010261 [821248/1237259]
loss: 0.018328 0.010233 [1026048/1237259]
loss: 0.020762 0.010231 [1230848/1237259]
Epoch 397
-------------------------------
loss: 0.021561 0.010384 [ 2048/1237259]
loss: 0.024030 0.010388 [206848/1237259]
loss: 0.018155 0.010225 [411648/1237259]
loss: 0.022797 0.010389 [616448/1237259]
loss: 0.024373 0.010372 [821248/1237259]
loss: 0.016658 0.010202 [1026048/1237259]
loss: 0.019210 0.010332 [1230848/1237259]
Epoch 398
-------------------------------
loss: 0.023531 0.010429 [ 2048/1237259]
loss: 0.020465 0.010204 [206848/1237259]
loss: 0.014826 0.010311 [411648/1237259]
loss: 0.023329 0.010390 [616448/1237259]
loss: 0.026176 0.010455 [821248/1237259]
loss: 0.022238 0.010355 [1026048/1237259]
loss: 0.015093 0.010397 [1230848/1237259]
Epoch 399
-------------------------------
loss: 0.018680 0.010188 [ 2048/1237259]
loss: 0.020788 0.010336 [206848/1237259]
loss: 0.018495 0.010246 [411648/1237259]
loss: 0.019085 0.010315 [616448/1237259]
loss: 0.015392 0.010315 [821248/1237259]
loss: 0.023276 0.010313 [1026048/1237259]
loss: 0.022511 0.010447 [1230848/1237259]
Epoch 400
-------------------------------
loss: 0.021277 0.010448 [ 2048/1237259]
loss: 0.014219 0.010254 [206848/1237259]
loss: 0.020701 0.010362 [411648/1237259]
loss: 0.020832 0.010441 [616448/1237259]
loss: 0.019707 0.010339 [821248/1237259]
loss: 0.014938 0.010348 [1026048/1237259]
loss: 0.025101 0.010283 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0636  
ndcg@20: 0.0521  
diversity: 0.1943  


Epoch 401
-------------------------------
loss: 0.017782 0.010267 [ 2048/1237259]
loss: 0.023814 0.010335 [206848/1237259]
loss: 0.018813 0.010375 [411648/1237259]
loss: 0.016735 0.010334 [616448/1237259]
loss: 0.013550 0.010232 [821248/1237259]
loss: 0.019959 0.010418 [1026048/1237259]
loss: 0.017498 0.010330 [1230848/1237259]
Epoch 402
-------------------------------
loss: 0.018329 0.010315 [ 2048/1237259]
loss: 0.019845 0.010406 [206848/1237259]
loss: 0.016643 0.010311 [411648/1237259]
loss: 0.019814 0.010315 [616448/1237259]
loss: 0.016968 0.010339 [821248/1237259]
loss: 0.015469 0.010477 [1026048/1237259]
loss: 0.017643 0.010345 [1230848/1237259]
Epoch 403
-------------------------------
loss: 0.018528 0.010439 [ 2048/1237259]
loss: 0.017725 0.010457 [206848/1237259]
loss: 0.020871 0.010359 [411648/1237259]
loss: 0.023427 0.010443 [616448/1237259]
loss: 0.020214 0.010314 [821248/1237259]
loss: 0.016511 0.010351 [1026048/1237259]
loss: 0.018238 0.010465 [1230848/1237259]
Epoch 404
-------------------------------
loss: 0.013562 0.010400 [ 2048/1237259]
loss: 0.020253 0.010357 [206848/1237259]
loss: 0.019024 0.010416 [411648/1237259]
loss: 0.016821 0.010467 [616448/1237259]
loss: 0.022689 0.010416 [821248/1237259]
loss: 0.014106 0.010273 [1026048/1237259]
loss: 0.022161 0.010355 [1230848/1237259]
Epoch 405
-------------------------------
loss: 0.019071 0.010336 [ 2048/1237259]
loss: 0.016183 0.010364 [206848/1237259]
loss: 0.016838 0.010390 [411648/1237259]
loss: 0.023869 0.010515 [616448/1237259]
loss: 0.019148 0.010361 [821248/1237259]
loss: 0.017791 0.010444 [1026048/1237259]
loss: 0.020289 0.010437 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0638  
ndcg@20: 0.0521  
diversity: 0.1946  


Epoch 406
-------------------------------
loss: 0.020277 0.010407 [ 2048/1237259]
loss: 0.016622 0.010456 [206848/1237259]
loss: 0.017036 0.010485 [411648/1237259]
loss: 0.014506 0.010369 [616448/1237259]
loss: 0.019356 0.010443 [821248/1237259]
loss: 0.016153 0.010307 [1026048/1237259]
loss: 0.019798 0.010490 [1230848/1237259]
Epoch 407
-------------------------------
loss: 0.016919 0.010465 [ 2048/1237259]
loss: 0.024116 0.010414 [206848/1237259]
loss: 0.021097 0.010279 [411648/1237259]
loss: 0.025525 0.010316 [616448/1237259]
loss: 0.023057 0.010428 [821248/1237259]
loss: 0.018132 0.010442 [1026048/1237259]
loss: 0.019380 0.010428 [1230848/1237259]
Epoch 408
-------------------------------
loss: 0.017447 0.010495 [ 2048/1237259]
loss: 0.017078 0.010364 [206848/1237259]
loss: 0.015805 0.010429 [411648/1237259]
loss: 0.019255 0.010382 [616448/1237259]
loss: 0.017504 0.010541 [821248/1237259]
loss: 0.018396 0.010492 [1026048/1237259]
loss: 0.018653 0.010488 [1230848/1237259]
Epoch 409
-------------------------------
loss: 0.021074 0.010511 [ 2048/1237259]
loss: 0.023708 0.010460 [206848/1237259]
loss: 0.025568 0.010403 [411648/1237259]
loss: 0.023305 0.010473 [616448/1237259]
loss: 0.019482 0.010437 [821248/1237259]
loss: 0.019638 0.010394 [1026048/1237259]
loss: 0.017955 0.010533 [1230848/1237259]
Epoch 410
-------------------------------
loss: 0.017919 0.010503 [ 2048/1237259]
loss: 0.017035 0.010523 [206848/1237259]
loss: 0.018450 0.010519 [411648/1237259]
loss: 0.018426 0.010372 [616448/1237259]
loss: 0.020561 0.010477 [821248/1237259]
loss: 0.023272 0.010436 [1026048/1237259]
loss: 0.015733 0.010555 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0637  
ndcg@20: 0.0521  
diversity: 0.1946  


Epoch 411
-------------------------------
loss: 0.022700 0.010548 [ 2048/1237259]
loss: 0.019038 0.010454 [206848/1237259]
loss: 0.018138 0.010314 [411648/1237259]
loss: 0.023134 0.010481 [616448/1237259]
loss: 0.019190 0.010530 [821248/1237259]
loss: 0.020591 0.010463 [1026048/1237259]
loss: 0.017489 0.010413 [1230848/1237259]
Epoch 412
-------------------------------
loss: 0.020137 0.010413 [ 2048/1237259]
loss: 0.019989 0.010588 [206848/1237259]
loss: 0.019618 0.010607 [411648/1237259]
loss: 0.016197 0.010438 [616448/1237259]
loss: 0.020520 0.010430 [821248/1237259]
loss: 0.017397 0.010555 [1026048/1237259]
loss: 0.017828 0.010535 [1230848/1237259]
Epoch 413
-------------------------------
loss: 0.018916 0.010590 [ 2048/1237259]
loss: 0.019702 0.010495 [206848/1237259]
loss: 0.013569 0.010479 [411648/1237259]
loss: 0.020078 0.010619 [616448/1237259]
loss: 0.017041 0.010520 [821248/1237259]
loss: 0.019187 0.010643 [1026048/1237259]
loss: 0.015083 0.010355 [1230848/1237259]
Epoch 414
-------------------------------
loss: 0.017228 0.010474 [ 2048/1237259]
loss: 0.017836 0.010468 [206848/1237259]
loss: 0.016222 0.010573 [411648/1237259]
loss: 0.013191 0.010536 [616448/1237259]
loss: 0.020386 0.010545 [821248/1237259]
loss: 0.022272 0.010424 [1026048/1237259]
loss: 0.024711 0.010476 [1230848/1237259]
Epoch 415
-------------------------------
loss: 0.015534 0.010614 [ 2048/1237259]
loss: 0.018731 0.010491 [206848/1237259]
loss: 0.020754 0.010548 [411648/1237259]
loss: 0.019717 0.010423 [616448/1237259]
loss: 0.015654 0.010496 [821248/1237259]
loss: 0.024220 0.010575 [1026048/1237259]
loss: 0.015974 0.010479 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0634  
ndcg@20: 0.0520  
diversity: 0.1948  


Epoch 416
-------------------------------
loss: 0.014151 0.010533 [ 2048/1237259]
loss: 0.018470 0.010467 [206848/1237259]
loss: 0.014319 0.010602 [411648/1237259]
loss: 0.019991 0.010513 [616448/1237259]
loss: 0.023701 0.010605 [821248/1237259]
loss: 0.017258 0.010583 [1026048/1237259]
loss: 0.020500 0.010454 [1230848/1237259]
Epoch 417
-------------------------------
loss: 0.018861 0.010567 [ 2048/1237259]
loss: 0.018932 0.010540 [206848/1237259]
loss: 0.018679 0.010453 [411648/1237259]
loss: 0.016073 0.010445 [616448/1237259]
loss: 0.017245 0.010533 [821248/1237259]
loss: 0.017591 0.010514 [1026048/1237259]
loss: 0.021712 0.010591 [1230848/1237259]
Epoch 418
-------------------------------
loss: 0.019098 0.010494 [ 2048/1237259]
loss: 0.021763 0.010528 [206848/1237259]
loss: 0.021673 0.010604 [411648/1237259]
loss: 0.019131 0.010565 [616448/1237259]
loss: 0.018985 0.010491 [821248/1237259]
loss: 0.016470 0.010681 [1026048/1237259]
loss: 0.018168 0.010592 [1230848/1237259]
Epoch 419
-------------------------------
loss: 0.019633 0.010531 [ 2048/1237259]
loss: 0.015929 0.010600 [206848/1237259]
loss: 0.017885 0.010643 [411648/1237259]
loss: 0.015355 0.010521 [616448/1237259]
loss: 0.018225 0.010604 [821248/1237259]
loss: 0.023561 0.010564 [1026048/1237259]
loss: 0.019154 0.010468 [1230848/1237259]
Epoch 420
-------------------------------
loss: 0.020021 0.010610 [ 2048/1237259]
loss: 0.018873 0.010358 [206848/1237259]
loss: 0.016972 0.010585 [411648/1237259]
loss: 0.012259 0.010537 [616448/1237259]
loss: 0.018727 0.010610 [821248/1237259]
loss: 0.016620 0.010601 [1026048/1237259]
loss: 0.017012 0.010580 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0637  
ndcg@20: 0.0522  
diversity: 0.1950  


Epoch 421
-------------------------------
loss: 0.023916 0.010483 [ 2048/1237259]
loss: 0.019343 0.010609 [206848/1237259]
loss: 0.018317 0.010437 [411648/1237259]
loss: 0.023218 0.010644 [616448/1237259]
loss: 0.017649 0.010583 [821248/1237259]
loss: 0.017094 0.010627 [1026048/1237259]
loss: 0.016602 0.010637 [1230848/1237259]
Epoch 422
-------------------------------
loss: 0.018995 0.010587 [ 2048/1237259]
loss: 0.016087 0.010513 [206848/1237259]
loss: 0.014800 0.010604 [411648/1237259]
loss: 0.019243 0.010512 [616448/1237259]
loss: 0.018130 0.010564 [821248/1237259]
loss: 0.017793 0.010572 [1026048/1237259]
loss: 0.026735 0.010537 [1230848/1237259]
Epoch 423
-------------------------------
loss: 0.015881 0.010649 [ 2048/1237259]
loss: 0.019409 0.010432 [206848/1237259]
loss: 0.016016 0.010563 [411648/1237259]
loss: 0.016372 0.010672 [616448/1237259]
loss: 0.017844 0.010574 [821248/1237259]
loss: 0.022287 0.010601 [1026048/1237259]
loss: 0.020751 0.010506 [1230848/1237259]
Epoch 424
-------------------------------
loss: 0.016610 0.010478 [ 2048/1237259]
loss: 0.015515 0.010619 [206848/1237259]
loss: 0.023956 0.010637 [411648/1237259]
loss: 0.019225 0.010623 [616448/1237259]
loss: 0.017296 0.010714 [821248/1237259]
loss: 0.021152 0.010686 [1026048/1237259]
loss: 0.016105 0.010621 [1230848/1237259]
Epoch 425
-------------------------------
loss: 0.017846 0.010668 [ 2048/1237259]
loss: 0.018551 0.010646 [206848/1237259]
loss: 0.017770 0.010678 [411648/1237259]
loss: 0.023516 0.010620 [616448/1237259]
loss: 0.014225 0.010607 [821248/1237259]
loss: 0.023801 0.010628 [1026048/1237259]
loss: 0.015807 0.010616 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0638  
ndcg@20: 0.0523  
diversity: 0.1950  


Epoch 426
-------------------------------
loss: 0.017951 0.010760 [ 2048/1237259]
loss: 0.026099 0.010596 [206848/1237259]
loss: 0.016185 0.010536 [411648/1237259]
loss: 0.017485 0.010578 [616448/1237259]
loss: 0.020885 0.010686 [821248/1237259]
loss: 0.025726 0.010605 [1026048/1237259]
loss: 0.025649 0.010555 [1230848/1237259]
Epoch 427
-------------------------------
loss: 0.018312 0.010681 [ 2048/1237259]
loss: 0.015528 0.010581 [206848/1237259]
loss: 0.015003 0.010733 [411648/1237259]
loss: 0.021045 0.010468 [616448/1237259]
loss: 0.018942 0.010664 [821248/1237259]
loss: 0.021711 0.010603 [1026048/1237259]
loss: 0.018611 0.010731 [1230848/1237259]
Epoch 428
-------------------------------
loss: 0.018869 0.010539 [ 2048/1237259]
loss: 0.025333 0.010524 [206848/1237259]
loss: 0.017768 0.010663 [411648/1237259]
loss: 0.022331 0.010664 [616448/1237259]
loss: 0.022584 0.010493 [821248/1237259]
loss: 0.019237 0.010672 [1026048/1237259]
loss: 0.015317 0.010687 [1230848/1237259]
Epoch 429
-------------------------------
loss: 0.019188 0.010546 [ 2048/1237259]
loss: 0.017433 0.010575 [206848/1237259]
loss: 0.016159 0.010518 [411648/1237259]
loss: 0.018735 0.010565 [616448/1237259]
loss: 0.017663 0.010635 [821248/1237259]
loss: 0.018960 0.010590 [1026048/1237259]
loss: 0.016780 0.010623 [1230848/1237259]
Epoch 430
-------------------------------
loss: 0.016429 0.010731 [ 2048/1237259]
loss: 0.017891 0.010744 [206848/1237259]
loss: 0.018088 0.010576 [411648/1237259]
loss: 0.015714 0.010613 [616448/1237259]
loss: 0.016769 0.010637 [821248/1237259]
loss: 0.024254 0.010702 [1026048/1237259]
loss: 0.017781 0.010675 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0638  
ndcg@20: 0.0523  
diversity: 0.1949  


Epoch 431
-------------------------------
loss: 0.019840 0.010568 [ 2048/1237259]
loss: 0.018532 0.010749 [206848/1237259]
loss: 0.021508 0.010694 [411648/1237259]
loss: 0.021419 0.010580 [616448/1237259]
loss: 0.014580 0.010681 [821248/1237259]
loss: 0.019407 0.010763 [1026048/1237259]
loss: 0.016447 0.010662 [1230848/1237259]
Epoch 432
-------------------------------
loss: 0.017883 0.010713 [ 2048/1237259]
loss: 0.016744 0.010570 [206848/1237259]
loss: 0.018109 0.010669 [411648/1237259]
loss: 0.025258 0.010598 [616448/1237259]
loss: 0.020604 0.010674 [821248/1237259]
loss: 0.017769 0.010681 [1026048/1237259]
loss: 0.017906 0.010774 [1230848/1237259]
Epoch 433
-------------------------------
loss: 0.015053 0.010769 [ 2048/1237259]
loss: 0.030296 0.010749 [206848/1237259]
loss: 0.020480 0.010783 [411648/1237259]
loss: 0.013408 0.010672 [616448/1237259]
loss: 0.018185 0.010625 [821248/1237259]
loss: 0.018097 0.010661 [1026048/1237259]
loss: 0.019408 0.010621 [1230848/1237259]
Epoch 434
-------------------------------
loss: 0.012523 0.010714 [ 2048/1237259]
loss: 0.018860 0.010598 [206848/1237259]
loss: 0.017679 0.010864 [411648/1237259]
loss: 0.017953 0.010723 [616448/1237259]
loss: 0.018225 0.010634 [821248/1237259]
loss: 0.014947 0.010745 [1026048/1237259]
loss: 0.015659 0.010671 [1230848/1237259]
Epoch 435
-------------------------------
loss: 0.013442 0.010846 [ 2048/1237259]
loss: 0.023328 0.010715 [206848/1237259]
loss: 0.015668 0.010612 [411648/1237259]
loss: 0.016966 0.010767 [616448/1237259]
loss: 0.016235 0.010821 [821248/1237259]
loss: 0.014265 0.010737 [1026048/1237259]
loss: 0.027648 0.010734 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0639  
ndcg@20: 0.0523  
diversity: 0.1949  


Epoch 436
-------------------------------
loss: 0.019819 0.010766 [ 2048/1237259]
loss: 0.015369 0.010665 [206848/1237259]
loss: 0.018750 0.010688 [411648/1237259]
loss: 0.016945 0.010763 [616448/1237259]
loss: 0.020391 0.010791 [821248/1237259]
loss: 0.018593 0.010676 [1026048/1237259]
loss: 0.020266 0.010760 [1230848/1237259]
Epoch 437
-------------------------------
loss: 0.018354 0.010816 [ 2048/1237259]
loss: 0.010027 0.010706 [206848/1237259]
loss: 0.012938 0.010675 [411648/1237259]
loss: 0.013037 0.010774 [616448/1237259]
loss: 0.019593 0.010754 [821248/1237259]
loss: 0.017431 0.010683 [1026048/1237259]
loss: 0.020450 0.010730 [1230848/1237259]
Epoch 438
-------------------------------
loss: 0.021694 0.010811 [ 2048/1237259]
loss: 0.017130 0.010673 [206848/1237259]
loss: 0.018456 0.010756 [411648/1237259]
loss: 0.019998 0.010705 [616448/1237259]
loss: 0.014017 0.010799 [821248/1237259]
loss: 0.019397 0.010783 [1026048/1237259]
loss: 0.016867 0.010698 [1230848/1237259]
Epoch 439
-------------------------------
loss: 0.017629 0.010701 [ 2048/1237259]
loss: 0.018257 0.010640 [206848/1237259]
loss: 0.018730 0.010696 [411648/1237259]
loss: 0.011659 0.010661 [616448/1237259]
loss: 0.016434 0.010773 [821248/1237259]
loss: 0.022053 0.010827 [1026048/1237259]
loss: 0.015377 0.010665 [1230848/1237259]
Epoch 440
-------------------------------
loss: 0.016415 0.010734 [ 2048/1237259]
loss: 0.018415 0.010643 [206848/1237259]
loss: 0.014646 0.010806 [411648/1237259]
loss: 0.017007 0.010602 [616448/1237259]
loss: 0.020413 0.010764 [821248/1237259]
loss: 0.015061 0.010756 [1026048/1237259]
loss: 0.015360 0.010768 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0640  
ndcg@20: 0.0523  
diversity: 0.1950  


Epoch 441
-------------------------------
loss: 0.014187 0.010729 [ 2048/1237259]
loss: 0.017510 0.010725 [206848/1237259]
loss: 0.017573 0.010782 [411648/1237259]
loss: 0.021543 0.010724 [616448/1237259]
loss: 0.022025 0.010707 [821248/1237259]
loss: 0.021084 0.010724 [1026048/1237259]
loss: 0.014884 0.010854 [1230848/1237259]
Epoch 442
-------------------------------
loss: 0.019953 0.010640 [ 2048/1237259]
loss: 0.017860 0.010746 [206848/1237259]
loss: 0.019442 0.010928 [411648/1237259]
loss: 0.018472 0.010734 [616448/1237259]
loss: 0.017570 0.010749 [821248/1237259]
loss: 0.019915 0.010710 [1026048/1237259]
loss: 0.017199 0.010860 [1230848/1237259]
Epoch 443
-------------------------------
loss: 0.023019 0.010797 [ 2048/1237259]
loss: 0.015393 0.010853 [206848/1237259]
loss: 0.021988 0.010961 [411648/1237259]
loss: 0.018065 0.010848 [616448/1237259]
loss: 0.014769 0.010898 [821248/1237259]
loss: 0.014198 0.010668 [1026048/1237259]
loss: 0.017250 0.010700 [1230848/1237259]
Epoch 444
-------------------------------
loss: 0.021520 0.010854 [ 2048/1237259]
loss: 0.019513 0.010807 [206848/1237259]
loss: 0.016308 0.010779 [411648/1237259]
loss: 0.022294 0.010758 [616448/1237259]
loss: 0.022445 0.010809 [821248/1237259]
loss: 0.013269 0.010781 [1026048/1237259]
loss: 0.015978 0.010847 [1230848/1237259]
Epoch 445
-------------------------------
loss: 0.014782 0.010791 [ 2048/1237259]
loss: 0.022651 0.010795 [206848/1237259]
loss: 0.017223 0.010783 [411648/1237259]
loss: 0.023211 0.010691 [616448/1237259]
loss: 0.021834 0.010823 [821248/1237259]
loss: 0.018246 0.010900 [1026048/1237259]
loss: 0.015749 0.010905 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0639  
ndcg@20: 0.0523  
diversity: 0.1951  


Epoch 446
-------------------------------
loss: 0.021064 0.010802 [ 2048/1237259]
loss: 0.015704 0.010918 [206848/1237259]
loss: 0.016932 0.010820 [411648/1237259]
loss: 0.017150 0.010858 [616448/1237259]
loss: 0.016366 0.010748 [821248/1237259]
loss: 0.016337 0.010788 [1026048/1237259]
loss: 0.015702 0.010786 [1230848/1237259]
Epoch 447
-------------------------------
loss: 0.019591 0.010684 [ 2048/1237259]
loss: 0.020107 0.010856 [206848/1237259]
loss: 0.016038 0.010723 [411648/1237259]
loss: 0.022627 0.010802 [616448/1237259]
loss: 0.016026 0.010737 [821248/1237259]
loss: 0.018756 0.010867 [1026048/1237259]
loss: 0.015383 0.010807 [1230848/1237259]
Epoch 448
-------------------------------
loss: 0.017287 0.010784 [ 2048/1237259]
loss: 0.016520 0.010731 [206848/1237259]
loss: 0.019039 0.010861 [411648/1237259]
loss: 0.022562 0.010751 [616448/1237259]
loss: 0.012562 0.010806 [821248/1237259]
loss: 0.017646 0.010853 [1026048/1237259]
loss: 0.012578 0.010833 [1230848/1237259]
Epoch 449
-------------------------------
loss: 0.021952 0.010834 [ 2048/1237259]
loss: 0.017501 0.010892 [206848/1237259]
loss: 0.017571 0.010862 [411648/1237259]
loss: 0.014990 0.010809 [616448/1237259]
loss: 0.021388 0.010809 [821248/1237259]
loss: 0.015828 0.010784 [1026048/1237259]
loss: 0.015176 0.010837 [1230848/1237259]
Epoch 450
-------------------------------
loss: 0.014149 0.010784 [ 2048/1237259]
loss: 0.015589 0.010786 [206848/1237259]
loss: 0.018694 0.010970 [411648/1237259]
loss: 0.020101 0.010821 [616448/1237259]
loss: 0.021969 0.010819 [821248/1237259]
loss: 0.017416 0.010734 [1026048/1237259]
loss: 0.014010 0.010873 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0640  
ndcg@20: 0.0524  
diversity: 0.1952  


Epoch 451
-------------------------------
loss: 0.021724 0.010833 [ 2048/1237259]
loss: 0.017688 0.010897 [206848/1237259]
loss: 0.017257 0.010697 [411648/1237259]
loss: 0.016096 0.010840 [616448/1237259]
loss: 0.021494 0.010945 [821248/1237259]
loss: 0.018726 0.010909 [1026048/1237259]
loss: 0.013550 0.010826 [1230848/1237259]
Epoch 452
-------------------------------
loss: 0.014329 0.010889 [ 2048/1237259]
loss: 0.016521 0.010855 [206848/1237259]
loss: 0.019098 0.010830 [411648/1237259]
loss: 0.016035 0.010802 [616448/1237259]
loss: 0.017739 0.010814 [821248/1237259]
loss: 0.018471 0.010863 [1026048/1237259]
loss: 0.014792 0.010724 [1230848/1237259]
Epoch 453
-------------------------------
loss: 0.015765 0.010826 [ 2048/1237259]
loss: 0.016161 0.010992 [206848/1237259]
loss: 0.020260 0.010772 [411648/1237259]
loss: 0.013028 0.010868 [616448/1237259]
loss: 0.016358 0.010808 [821248/1237259]
loss: 0.016642 0.010842 [1026048/1237259]
loss: 0.015583 0.010893 [1230848/1237259]
Epoch 454
-------------------------------
loss: 0.018588 0.010854 [ 2048/1237259]
loss: 0.014972 0.010918 [206848/1237259]
loss: 0.016610 0.010722 [411648/1237259]
loss: 0.014458 0.010954 [616448/1237259]
loss: 0.018173 0.010872 [821248/1237259]
loss: 0.015440 0.010994 [1026048/1237259]
loss: 0.019537 0.010859 [1230848/1237259]
Epoch 455
-------------------------------
loss: 0.019963 0.010956 [ 2048/1237259]
loss: 0.016614 0.010924 [206848/1237259]
loss: 0.021000 0.010854 [411648/1237259]
loss: 0.017414 0.010898 [616448/1237259]
loss: 0.016358 0.010847 [821248/1237259]
loss: 0.018384 0.010810 [1026048/1237259]
loss: 0.019318 0.010847 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0641  
ndcg@20: 0.0525  
diversity: 0.1952  


Epoch 456
-------------------------------
loss: 0.015002 0.010848 [ 2048/1237259]
loss: 0.017468 0.010856 [206848/1237259]
loss: 0.015930 0.010905 [411648/1237259]
loss: 0.015528 0.010917 [616448/1237259]
loss: 0.017304 0.010926 [821248/1237259]
loss: 0.019526 0.010977 [1026048/1237259]
loss: 0.016637 0.010858 [1230848/1237259]
Epoch 457
-------------------------------
loss: 0.014792 0.010882 [ 2048/1237259]
loss: 0.017498 0.010877 [206848/1237259]
loss: 0.020852 0.010888 [411648/1237259]
loss: 0.017542 0.010872 [616448/1237259]
loss: 0.018396 0.010976 [821248/1237259]
loss: 0.017218 0.010928 [1026048/1237259]
loss: 0.019382 0.010875 [1230848/1237259]
Epoch 458
-------------------------------
loss: 0.018732 0.010853 [ 2048/1237259]
loss: 0.026098 0.010861 [206848/1237259]
loss: 0.017563 0.010931 [411648/1237259]
loss: 0.015088 0.010869 [616448/1237259]
loss: 0.018377 0.010824 [821248/1237259]
loss: 0.015565 0.010942 [1026048/1237259]
loss: 0.013174 0.010966 [1230848/1237259]
Epoch 459
-------------------------------
loss: 0.011976 0.011002 [ 2048/1237259]
loss: 0.022974 0.010839 [206848/1237259]
loss: 0.018319 0.010954 [411648/1237259]
loss: 0.022420 0.010941 [616448/1237259]
loss: 0.014105 0.010989 [821248/1237259]
loss: 0.018247 0.010933 [1026048/1237259]
loss: 0.018229 0.010977 [1230848/1237259]
Epoch 460
-------------------------------
loss: 0.014917 0.010789 [ 2048/1237259]
loss: 0.020819 0.010946 [206848/1237259]
loss: 0.017017 0.010830 [411648/1237259]
loss: 0.021002 0.010951 [616448/1237259]
loss: 0.013077 0.011035 [821248/1237259]
loss: 0.014213 0.010918 [1026048/1237259]
loss: 0.023440 0.010894 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0641  
ndcg@20: 0.0526  
diversity: 0.1953  


Epoch 461
-------------------------------
loss: 0.013761 0.010869 [ 2048/1237259]
loss: 0.016845 0.010998 [206848/1237259]
loss: 0.016901 0.010954 [411648/1237259]
loss: 0.014291 0.011086 [616448/1237259]
loss: 0.019363 0.010963 [821248/1237259]
loss: 0.019765 0.010852 [1026048/1237259]
loss: 0.015217 0.010924 [1230848/1237259]
Epoch 462
-------------------------------
loss: 0.016172 0.010892 [ 2048/1237259]
loss: 0.015459 0.010994 [206848/1237259]
loss: 0.018216 0.010923 [411648/1237259]
loss: 0.017400 0.010933 [616448/1237259]
loss: 0.015661 0.010999 [821248/1237259]
loss: 0.016212 0.010964 [1026048/1237259]
loss: 0.017550 0.010940 [1230848/1237259]
Epoch 463
-------------------------------
loss: 0.018214 0.010922 [ 2048/1237259]
loss: 0.016627 0.010972 [206848/1237259]
loss: 0.023735 0.010914 [411648/1237259]
loss: 0.015835 0.011034 [616448/1237259]
loss: 0.014051 0.011013 [821248/1237259]
loss: 0.016381 0.010945 [1026048/1237259]
loss: 0.022634 0.011037 [1230848/1237259]
Epoch 464
-------------------------------
loss: 0.015978 0.010778 [ 2048/1237259]
loss: 0.015367 0.011069 [206848/1237259]
loss: 0.017640 0.010798 [411648/1237259]
loss: 0.018552 0.010870 [616448/1237259]
loss: 0.018114 0.010926 [821248/1237259]
loss: 0.018739 0.011031 [1026048/1237259]
loss: 0.017031 0.010881 [1230848/1237259]
Epoch 465
-------------------------------
loss: 0.016132 0.011069 [ 2048/1237259]
loss: 0.018133 0.010933 [206848/1237259]
loss: 0.015707 0.010996 [411648/1237259]
loss: 0.015512 0.010985 [616448/1237259]
loss: 0.022452 0.010891 [821248/1237259]
loss: 0.013681 0.010942 [1026048/1237259]
loss: 0.015901 0.010910 [1230848/1237259]
Eval results: 
recall@20: 0.0640  
ndcg@20: 0.0525  
diversity: 0.1953  


Epoch 466
-------------------------------
loss: 0.014106 0.011074 [ 2048/1237259]
loss: 0.015100 0.010999 [206848/1237259]
loss: 0.013841 0.010937 [411648/1237259]
loss: 0.013178 0.011053 [616448/1237259]
loss: 0.013221 0.010910 [821248/1237259]
loss: 0.020102 0.010920 [1026048/1237259]
loss: 0.016236 0.011052 [1230848/1237259]
Epoch 467
-------------------------------
loss: 0.021979 0.011099 [ 2048/1237259]
loss: 0.016055 0.011066 [206848/1237259]
loss: 0.011379 0.010905 [411648/1237259]
loss: 0.012636 0.010986 [616448/1237259]
loss: 0.017274 0.011136 [821248/1237259]
loss: 0.014017 0.010998 [1026048/1237259]
loss: 0.018606 0.010949 [1230848/1237259]
Epoch 468
-------------------------------
loss: 0.016957 0.011016 [ 2048/1237259]
loss: 0.015547 0.011116 [206848/1237259]
loss: 0.020239 0.011099 [411648/1237259]
loss: 0.014490 0.010971 [616448/1237259]
loss: 0.017917 0.011065 [821248/1237259]
loss: 0.015318 0.011019 [1026048/1237259]
loss: 0.021205 0.010957 [1230848/1237259]
Epoch 469
-------------------------------
loss: 0.021823 0.010994 [ 2048/1237259]
loss: 0.022120 0.010933 [206848/1237259]
loss: 0.019762 0.011026 [411648/1237259]
loss: 0.018630 0.010961 [616448/1237259]
loss: 0.018163 0.011045 [821248/1237259]
loss: 0.017044 0.011075 [1026048/1237259]
loss: 0.016103 0.011046 [1230848/1237259]
Epoch 470
-------------------------------
loss: 0.017122 0.011045 [ 2048/1237259]
loss: 0.015025 0.010986 [206848/1237259]
loss: 0.017859 0.010923 [411648/1237259]
loss: 0.015567 0.011006 [616448/1237259]
loss: 0.017068 0.010967 [821248/1237259]
loss: 0.012154 0.011042 [1026048/1237259]
loss: 0.015563 0.011082 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0642  
ndcg@20: 0.0526  
diversity: 0.1951  


Epoch 471
-------------------------------
loss: 0.014903 0.010900 [ 2048/1237259]
loss: 0.016146 0.011081 [206848/1237259]
loss: 0.017028 0.010984 [411648/1237259]
loss: 0.015615 0.010986 [616448/1237259]
loss: 0.019564 0.011019 [821248/1237259]
loss: 0.013955 0.010989 [1026048/1237259]
loss: 0.015262 0.011024 [1230848/1237259]
Epoch 472
-------------------------------
loss: 0.019601 0.011078 [ 2048/1237259]
loss: 0.015695 0.010866 [206848/1237259]
loss: 0.013729 0.011076 [411648/1237259]
loss: 0.016753 0.011038 [616448/1237259]
loss: 0.018379 0.011116 [821248/1237259]
loss: 0.015586 0.010963 [1026048/1237259]
loss: 0.015783 0.011070 [1230848/1237259]
Epoch 473
-------------------------------
loss: 0.013400 0.010983 [ 2048/1237259]
loss: 0.021660 0.011028 [206848/1237259]
loss: 0.015457 0.011041 [411648/1237259]
loss: 0.014317 0.010965 [616448/1237259]
loss: 0.016708 0.011040 [821248/1237259]
loss: 0.023008 0.011120 [1026048/1237259]
loss: 0.016961 0.010918 [1230848/1237259]
Epoch 474
-------------------------------
loss: 0.019596 0.011089 [ 2048/1237259]
loss: 0.022684 0.010995 [206848/1237259]
loss: 0.010416 0.011131 [411648/1237259]
loss: 0.027096 0.011098 [616448/1237259]
loss: 0.015127 0.011176 [821248/1237259]
loss: 0.015434 0.011006 [1026048/1237259]
loss: 0.017321 0.011010 [1230848/1237259]
Epoch 475
-------------------------------
loss: 0.017271 0.011033 [ 2048/1237259]
loss: 0.018695 0.011049 [206848/1237259]
loss: 0.022880 0.011039 [411648/1237259]
loss: 0.022096 0.011026 [616448/1237259]
loss: 0.016123 0.011004 [821248/1237259]
loss: 0.020501 0.011015 [1026048/1237259]
loss: 0.021151 0.011090 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0642  
ndcg@20: 0.0526  
diversity: 0.1953  


Epoch 476
-------------------------------
loss: 0.016502 0.011098 [ 2048/1237259]
loss: 0.014302 0.011011 [206848/1237259]
loss: 0.017594 0.011046 [411648/1237259]
loss: 0.020388 0.011155 [616448/1237259]
loss: 0.015759 0.011031 [821248/1237259]
loss: 0.016026 0.011007 [1026048/1237259]
loss: 0.015913 0.011006 [1230848/1237259]
Epoch 477
-------------------------------
loss: 0.017991 0.010978 [ 2048/1237259]
loss: 0.020726 0.011016 [206848/1237259]
loss: 0.016514 0.011002 [411648/1237259]
loss: 0.022809 0.011200 [616448/1237259]
loss: 0.013633 0.011022 [821248/1237259]
loss: 0.018778 0.010984 [1026048/1237259]
loss: 0.015801 0.011174 [1230848/1237259]
Epoch 478
-------------------------------
loss: 0.019548 0.011200 [ 2048/1237259]
loss: 0.020454 0.011093 [206848/1237259]
loss: 0.021369 0.011197 [411648/1237259]
loss: 0.019631 0.010984 [616448/1237259]
loss: 0.017110 0.011007 [821248/1237259]
loss: 0.019751 0.011043 [1026048/1237259]
loss: 0.016041 0.010939 [1230848/1237259]
Epoch 479
-------------------------------
loss: 0.014955 0.011119 [ 2048/1237259]
loss: 0.020845 0.011138 [206848/1237259]
loss: 0.020204 0.011169 [411648/1237259]
loss: 0.016172 0.011144 [616448/1237259]
loss: 0.012587 0.011054 [821248/1237259]
loss: 0.014726 0.011084 [1026048/1237259]
loss: 0.017567 0.011127 [1230848/1237259]
Epoch 480
-------------------------------
loss: 0.016900 0.011105 [ 2048/1237259]
loss: 0.014815 0.011282 [206848/1237259]
loss: 0.020637 0.011114 [411648/1237259]
loss: 0.020538 0.011012 [616448/1237259]
loss: 0.016088 0.011103 [821248/1237259]
loss: 0.013114 0.011071 [1026048/1237259]
loss: 0.018951 0.011176 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0640  
ndcg@20: 0.0525  
diversity: 0.1955  


Epoch 481
-------------------------------
loss: 0.018843 0.011242 [ 2048/1237259]
loss: 0.013681 0.011034 [206848/1237259]
loss: 0.017471 0.011057 [411648/1237259]
loss: 0.014485 0.011072 [616448/1237259]
loss: 0.014135 0.011078 [821248/1237259]
loss: 0.020249 0.011083 [1026048/1237259]
loss: 0.012057 0.011057 [1230848/1237259]
Epoch 482
-------------------------------
loss: 0.015611 0.011035 [ 2048/1237259]
loss: 0.013453 0.011107 [206848/1237259]
loss: 0.013749 0.011147 [411648/1237259]
loss: 0.020870 0.011213 [616448/1237259]
loss: 0.014858 0.011192 [821248/1237259]
loss: 0.016793 0.011056 [1026048/1237259]
loss: 0.021032 0.011096 [1230848/1237259]
Epoch 483
-------------------------------
loss: 0.016799 0.011081 [ 2048/1237259]
loss: 0.017298 0.011134 [206848/1237259]
loss: 0.016895 0.011088 [411648/1237259]
loss: 0.020216 0.011134 [616448/1237259]
loss: 0.017339 0.011212 [821248/1237259]
loss: 0.013493 0.011117 [1026048/1237259]
loss: 0.021076 0.011146 [1230848/1237259]
Epoch 484
-------------------------------
loss: 0.014877 0.011132 [ 2048/1237259]
loss: 0.013565 0.011028 [206848/1237259]
loss: 0.018292 0.011098 [411648/1237259]
loss: 0.019805 0.011129 [616448/1237259]
loss: 0.013160 0.011133 [821248/1237259]
loss: 0.018691 0.011250 [1026048/1237259]
loss: 0.013893 0.011158 [1230848/1237259]
Epoch 485
-------------------------------
loss: 0.016538 0.011225 [ 2048/1237259]
loss: 0.019019 0.011180 [206848/1237259]
loss: 0.016612 0.011161 [411648/1237259]
loss: 0.019075 0.011155 [616448/1237259]
loss: 0.018085 0.011164 [821248/1237259]
loss: 0.021062 0.011220 [1026048/1237259]
loss: 0.016606 0.011190 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0645  
ndcg@20: 0.0528  
diversity: 0.1954  


Epoch 486
-------------------------------
loss: 0.013962 0.010966 [ 2048/1237259]
loss: 0.016303 0.011143 [206848/1237259]
loss: 0.016793 0.011170 [411648/1237259]
loss: 0.018685 0.011043 [616448/1237259]
loss: 0.017041 0.011063 [821248/1237259]
loss: 0.014265 0.011053 [1026048/1237259]
loss: 0.017699 0.011043 [1230848/1237259]
Epoch 487
-------------------------------
loss: 0.017656 0.011224 [ 2048/1237259]
loss: 0.016057 0.011053 [206848/1237259]
loss: 0.014219 0.011068 [411648/1237259]
loss: 0.017177 0.011170 [616448/1237259]
loss: 0.016099 0.011084 [821248/1237259]
loss: 0.013431 0.011093 [1026048/1237259]
loss: 0.016291 0.011201 [1230848/1237259]
Epoch 488
-------------------------------
loss: 0.012009 0.011213 [ 2048/1237259]
loss: 0.018631 0.011129 [206848/1237259]
loss: 0.018198 0.011094 [411648/1237259]
loss: 0.019332 0.011184 [616448/1237259]
loss: 0.017497 0.011088 [821248/1237259]
loss: 0.017424 0.011099 [1026048/1237259]
loss: 0.014418 0.011135 [1230848/1237259]
Epoch 489
-------------------------------
loss: 0.014607 0.011237 [ 2048/1237259]
loss: 0.016024 0.011222 [206848/1237259]
loss: 0.013417 0.011111 [411648/1237259]
loss: 0.020055 0.011194 [616448/1237259]
loss: 0.017141 0.011210 [821248/1237259]
loss: 0.017631 0.011025 [1026048/1237259]
loss: 0.016618 0.011161 [1230848/1237259]
Epoch 490
-------------------------------
loss: 0.017896 0.011303 [ 2048/1237259]
loss: 0.015991 0.011209 [206848/1237259]
loss: 0.017851 0.011153 [411648/1237259]
loss: 0.011371 0.011223 [616448/1237259]
loss: 0.022761 0.011317 [821248/1237259]
loss: 0.017222 0.011166 [1026048/1237259]
loss: 0.017555 0.011103 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0645  
ndcg@20: 0.0528  
diversity: 0.1954  


Epoch 491
-------------------------------
loss: 0.016243 0.011217 [ 2048/1237259]
loss: 0.015556 0.011097 [206848/1237259]
loss: 0.013997 0.011158 [411648/1237259]
loss: 0.016579 0.011101 [616448/1237259]
loss: 0.014489 0.011147 [821248/1237259]
loss: 0.012936 0.011214 [1026048/1237259]
loss: 0.012661 0.011186 [1230848/1237259]
Epoch 492
-------------------------------
loss: 0.015437 0.011167 [ 2048/1237259]
loss: 0.017345 0.011266 [206848/1237259]
loss: 0.015847 0.011043 [411648/1237259]
loss: 0.016918 0.011160 [616448/1237259]
loss: 0.017691 0.011131 [821248/1237259]
loss: 0.018828 0.011202 [1026048/1237259]
loss: 0.016933 0.011280 [1230848/1237259]
Epoch 493
-------------------------------
loss: 0.015353 0.011217 [ 2048/1237259]
loss: 0.022890 0.011143 [206848/1237259]
loss: 0.017220 0.011203 [411648/1237259]
loss: 0.015180 0.011217 [616448/1237259]
loss: 0.018710 0.011190 [821248/1237259]
loss: 0.016226 0.011098 [1026048/1237259]
loss: 0.016718 0.011277 [1230848/1237259]
Epoch 494
-------------------------------
loss: 0.015708 0.011149 [ 2048/1237259]
loss: 0.013974 0.011186 [206848/1237259]
loss: 0.023451 0.011265 [411648/1237259]
loss: 0.024290 0.011131 [616448/1237259]
loss: 0.013751 0.011193 [821248/1237259]
loss: 0.012984 0.011185 [1026048/1237259]
loss: 0.018659 0.011199 [1230848/1237259]
Epoch 495
-------------------------------
loss: 0.017570 0.011304 [ 2048/1237259]
loss: 0.017291 0.011209 [206848/1237259]
loss: 0.015336 0.011230 [411648/1237259]
loss: 0.016448 0.011256 [616448/1237259]
loss: 0.011638 0.011120 [821248/1237259]
loss: 0.019988 0.011284 [1026048/1237259]
loss: 0.015738 0.011165 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0646  
ndcg@20: 0.0529  
diversity: 0.1953  


Epoch 496
-------------------------------
loss: 0.018446 0.011262 [ 2048/1237259]
loss: 0.014986 0.011157 [206848/1237259]
loss: 0.018739 0.011368 [411648/1237259]
loss: 0.011865 0.011193 [616448/1237259]
loss: 0.019164 0.011269 [821248/1237259]
loss: 0.015083 0.011264 [1026048/1237259]
loss: 0.015237 0.011255 [1230848/1237259]
Epoch 497
-------------------------------
loss: 0.017543 0.011083 [ 2048/1237259]
loss: 0.014529 0.011224 [206848/1237259]
loss: 0.015334 0.011301 [411648/1237259]
loss: 0.012300 0.011284 [616448/1237259]
loss: 0.015568 0.011237 [821248/1237259]
loss: 0.015196 0.011191 [1026048/1237259]
loss: 0.018464 0.011030 [1230848/1237259]
Epoch 498
-------------------------------
loss: 0.020418 0.011257 [ 2048/1237259]
loss: 0.010776 0.011214 [206848/1237259]
loss: 0.017701 0.011250 [411648/1237259]
loss: 0.011607 0.011231 [616448/1237259]
loss: 0.019430 0.011279 [821248/1237259]
loss: 0.013594 0.011295 [1026048/1237259]
loss: 0.027084 0.011146 [1230848/1237259]
Epoch 499
-------------------------------
loss: 0.017142 0.011262 [ 2048/1237259]
loss: 0.014738 0.011278 [206848/1237259]
loss: 0.019146 0.011197 [411648/1237259]
loss: 0.016495 0.011192 [616448/1237259]
loss: 0.012598 0.011267 [821248/1237259]
loss: 0.019292 0.011275 [1026048/1237259]
loss: 0.015205 0.011249 [1230848/1237259]
Epoch 500
-------------------------------
loss: 0.014782 0.011218 [ 2048/1237259]
loss: 0.016920 0.011323 [206848/1237259]
loss: 0.014017 0.011220 [411648/1237259]
loss: 0.019426 0.011198 [616448/1237259]
loss: 0.014324 0.011288 [821248/1237259]
loss: 0.017122 0.011251 [1026048/1237259]
loss: 0.020234 0.011117 [1230848/1237259]
Eval results: 
recall@20: 0.0643  
ndcg@20: 0.0529  
diversity: 0.1953  


Epoch 501
-------------------------------
loss: 0.016358 0.011195 [ 2048/1237259]
loss: 0.013679 0.011279 [206848/1237259]
loss: 0.014115 0.011222 [411648/1237259]
loss: 0.012012 0.011324 [616448/1237259]
loss: 0.016407 0.011151 [821248/1237259]
loss: 0.018746 0.011205 [1026048/1237259]
loss: 0.011751 0.011256 [1230848/1237259]
Epoch 502
-------------------------------
loss: 0.017278 0.011135 [ 2048/1237259]
loss: 0.015148 0.011243 [206848/1237259]
loss: 0.016848 0.011205 [411648/1237259]
loss: 0.018082 0.011057 [616448/1237259]
loss: 0.016451 0.011299 [821248/1237259]
loss: 0.014568 0.011173 [1026048/1237259]
loss: 0.016882 0.011238 [1230848/1237259]
Epoch 503
-------------------------------
loss: 0.016504 0.011162 [ 2048/1237259]
loss: 0.013112 0.011327 [206848/1237259]
loss: 0.014821 0.011319 [411648/1237259]
loss: 0.015224 0.011273 [616448/1237259]
loss: 0.018589 0.011300 [821248/1237259]
loss: 0.017131 0.011198 [1026048/1237259]
loss: 0.014577 0.011231 [1230848/1237259]
Epoch 504
-------------------------------
loss: 0.014660 0.011314 [ 2048/1237259]
loss: 0.013769 0.011304 [206848/1237259]
loss: 0.018361 0.011259 [411648/1237259]
loss: 0.012270 0.011260 [616448/1237259]
loss: 0.014050 0.011197 [821248/1237259]
loss: 0.015352 0.011222 [1026048/1237259]
loss: 0.015480 0.011257 [1230848/1237259]
Epoch 505
-------------------------------
loss: 0.012075 0.011321 [ 2048/1237259]
loss: 0.016157 0.011309 [206848/1237259]
loss: 0.013059 0.011314 [411648/1237259]
loss: 0.018938 0.011212 [616448/1237259]
loss: 0.014716 0.011397 [821248/1237259]
loss: 0.012539 0.011249 [1026048/1237259]
loss: 0.010887 0.011242 [1230848/1237259]
Eval results: 
recall@20: 0.0643  
ndcg@20: 0.0528  
diversity: 0.1953  


Epoch 506
-------------------------------
loss: 0.014519 0.011213 [ 2048/1237259]
loss: 0.019895 0.011235 [206848/1237259]
loss: 0.019057 0.011297 [411648/1237259]
loss: 0.017198 0.011377 [616448/1237259]
loss: 0.016476 0.011181 [821248/1237259]
loss: 0.016021 0.011316 [1026048/1237259]
loss: 0.023703 0.011335 [1230848/1237259]
Epoch 507
-------------------------------
loss: 0.013331 0.011176 [ 2048/1237259]
loss: 0.018003 0.011181 [206848/1237259]
loss: 0.017568 0.011308 [411648/1237259]
loss: 0.016165 0.011284 [616448/1237259]
loss: 0.013222 0.011306 [821248/1237259]
loss: 0.015363 0.011293 [1026048/1237259]
loss: 0.024111 0.011300 [1230848/1237259]
Epoch 508
-------------------------------
loss: 0.022264 0.011321 [ 2048/1237259]
loss: 0.019029 0.011164 [206848/1237259]
loss: 0.012903 0.011372 [411648/1237259]
loss: 0.011688 0.011252 [616448/1237259]
loss: 0.014455 0.011275 [821248/1237259]
loss: 0.017210 0.011208 [1026048/1237259]
loss: 0.016467 0.011265 [1230848/1237259]
Epoch 509
-------------------------------
loss: 0.015536 0.011307 [ 2048/1237259]
loss: 0.014485 0.011175 [206848/1237259]
loss: 0.013810 0.011337 [411648/1237259]
loss: 0.011857 0.011284 [616448/1237259]
loss: 0.011624 0.011410 [821248/1237259]
loss: 0.016716 0.011237 [1026048/1237259]
loss: 0.013759 0.011464 [1230848/1237259]
Epoch 510
-------------------------------
loss: 0.022039 0.011248 [ 2048/1237259]
loss: 0.015276 0.011301 [206848/1237259]
loss: 0.021307 0.011249 [411648/1237259]
loss: 0.016949 0.011354 [616448/1237259]
loss: 0.019101 0.011204 [821248/1237259]
loss: 0.019240 0.011323 [1026048/1237259]
loss: 0.017185 0.011342 [1230848/1237259]
Eval results: 
recall@20: 0.0644  
ndcg@20: 0.0527  
diversity: 0.1953  


Epoch 511
-------------------------------
loss: 0.013640 0.011390 [ 2048/1237259]
loss: 0.020813 0.011345 [206848/1237259]
loss: 0.015535 0.011397 [411648/1237259]
loss: 0.017821 0.011232 [616448/1237259]
loss: 0.017419 0.011216 [821248/1237259]
loss: 0.013756 0.011425 [1026048/1237259]
loss: 0.020003 0.011305 [1230848/1237259]
Epoch 512
-------------------------------
loss: 0.021520 0.011302 [ 2048/1237259]
loss: 0.020781 0.011359 [206848/1237259]
loss: 0.009166 0.011324 [411648/1237259]
loss: 0.014216 0.011333 [616448/1237259]
loss: 0.016548 0.011426 [821248/1237259]
loss: 0.023214 0.011285 [1026048/1237259]
loss: 0.013178 0.011369 [1230848/1237259]
Epoch 513
-------------------------------
loss: 0.016560 0.011316 [ 2048/1237259]
loss: 0.014119 0.011213 [206848/1237259]
loss: 0.017909 0.011233 [411648/1237259]
loss: 0.011657 0.011352 [616448/1237259]
loss: 0.015834 0.011434 [821248/1237259]
loss: 0.016940 0.011382 [1026048/1237259]
loss: 0.019060 0.011242 [1230848/1237259]
Epoch 514
-------------------------------
loss: 0.015595 0.011169 [ 2048/1237259]
loss: 0.023045 0.011405 [206848/1237259]
loss: 0.016355 0.011458 [411648/1237259]
loss: 0.012608 0.011396 [616448/1237259]
loss: 0.021611 0.011334 [821248/1237259]
loss: 0.015441 0.011390 [1026048/1237259]
loss: 0.011753 0.011324 [1230848/1237259]
Epoch 515
-------------------------------
loss: 0.018974 0.011395 [ 2048/1237259]
loss: 0.015709 0.011420 [206848/1237259]
loss: 0.013310 0.011235 [411648/1237259]
loss: 0.017667 0.011311 [616448/1237259]
loss: 0.017640 0.011373 [821248/1237259]
loss: 0.015622 0.011421 [1026048/1237259]
loss: 0.015941 0.011410 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0643  
ndcg@20: 0.0526  
diversity: 0.1956  


Epoch 516
-------------------------------
loss: 0.018717 0.011329 [ 2048/1237259]
loss: 0.018139 0.011438 [206848/1237259]
loss: 0.014061 0.011306 [411648/1237259]
loss: 0.015542 0.011282 [616448/1237259]
loss: 0.020214 0.011375 [821248/1237259]
loss: 0.017502 0.011408 [1026048/1237259]
loss: 0.014639 0.011277 [1230848/1237259]
Epoch 517
-------------------------------
loss: 0.017322 0.011415 [ 2048/1237259]
loss: 0.011463 0.011328 [206848/1237259]
loss: 0.013007 0.011322 [411648/1237259]
loss: 0.015689 0.011448 [616448/1237259]
loss: 0.012979 0.011305 [821248/1237259]
loss: 0.015553 0.011453 [1026048/1237259]
loss: 0.013559 0.011342 [1230848/1237259]
Epoch 518
-------------------------------
loss: 0.015750 0.011393 [ 2048/1237259]
loss: 0.015957 0.011348 [206848/1237259]
loss: 0.014641 0.011353 [411648/1237259]
loss: 0.014060 0.011468 [616448/1237259]
loss: 0.012343 0.011387 [821248/1237259]
loss: 0.017232 0.011238 [1026048/1237259]
loss: 0.012067 0.011440 [1230848/1237259]
Epoch 519
-------------------------------
loss: 0.017569 0.011241 [ 2048/1237259]
loss: 0.016213 0.011416 [206848/1237259]
loss: 0.022096 0.011359 [411648/1237259]
loss: 0.012894 0.011327 [616448/1237259]
loss: 0.012899 0.011389 [821248/1237259]
loss: 0.018911 0.011392 [1026048/1237259]
loss: 0.017681 0.011272 [1230848/1237259]
Epoch 520
-------------------------------
loss: 0.016580 0.011392 [ 2048/1237259]
loss: 0.015150 0.011370 [206848/1237259]
loss: 0.015910 0.011334 [411648/1237259]
loss: 0.016474 0.011586 [616448/1237259]
loss: 0.012034 0.011280 [821248/1237259]
loss: 0.019836 0.011382 [1026048/1237259]
loss: 0.019211 0.011328 [1230848/1237259]
Eval results: 
recall@20: 0.0642  
ndcg@20: 0.0526  
diversity: 0.1955  


Epoch 521
-------------------------------
loss: 0.016860 0.011419 [ 2048/1237259]
loss: 0.019981 0.011432 [206848/1237259]
loss: 0.015664 0.011363 [411648/1237259]
loss: 0.015257 0.011361 [616448/1237259]
loss: 0.015891 0.011361 [821248/1237259]
loss: 0.014466 0.011401 [1026048/1237259]
loss: 0.017719 0.011472 [1230848/1237259]
Epoch 522
-------------------------------
loss: 0.018531 0.011338 [ 2048/1237259]
loss: 0.013524 0.011395 [206848/1237259]
loss: 0.014822 0.011477 [411648/1237259]
loss: 0.019473 0.011393 [616448/1237259]
loss: 0.016754 0.011432 [821248/1237259]
loss: 0.015948 0.011422 [1026048/1237259]
loss: 0.017553 0.011403 [1230848/1237259]
Epoch 523
-------------------------------
loss: 0.016836 0.011457 [ 2048/1237259]
loss: 0.011205 0.011401 [206848/1237259]
loss: 0.018617 0.011441 [411648/1237259]
loss: 0.014466 0.011347 [616448/1237259]
loss: 0.012972 0.011367 [821248/1237259]
loss: 0.015890 0.011290 [1026048/1237259]
loss: 0.015760 0.011393 [1230848/1237259]
Epoch 524
-------------------------------
loss: 0.020664 0.011447 [ 2048/1237259]
loss: 0.015846 0.011397 [206848/1237259]
loss: 0.016618 0.011376 [411648/1237259]
loss: 0.014277 0.011496 [616448/1237259]
loss: 0.014590 0.011290 [821248/1237259]
loss: 0.016203 0.011328 [1026048/1237259]
loss: 0.015382 0.011184 [1230848/1237259]
Epoch 525
-------------------------------
loss: 0.017292 0.011523 [ 2048/1237259]
loss: 0.016132 0.011441 [206848/1237259]
loss: 0.016021 0.011280 [411648/1237259]
loss: 0.014922 0.011369 [616448/1237259]
loss: 0.017561 0.011461 [821248/1237259]
loss: 0.013893 0.011515 [1026048/1237259]
loss: 0.015177 0.011483 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0644  
ndcg@20: 0.0528  
diversity: 0.1956  


Epoch 526
-------------------------------
loss: 0.016883 0.011475 [ 2048/1237259]
loss: 0.013327 0.011313 [206848/1237259]
loss: 0.017020 0.011308 [411648/1237259]
loss: 0.009905 0.011496 [616448/1237259]
loss: 0.019510 0.011427 [821248/1237259]
loss: 0.019187 0.011429 [1026048/1237259]
loss: 0.015848 0.011529 [1230848/1237259]
Epoch 527
-------------------------------
loss: 0.017137 0.011432 [ 2048/1237259]
loss: 0.016300 0.011511 [206848/1237259]
loss: 0.013700 0.011516 [411648/1237259]
loss: 0.012755 0.011374 [616448/1237259]
loss: 0.016029 0.011448 [821248/1237259]
loss: 0.015073 0.011311 [1026048/1237259]
loss: 0.012353 0.011463 [1230848/1237259]
Epoch 528
-------------------------------
loss: 0.017140 0.011508 [ 2048/1237259]
loss: 0.013042 0.011374 [206848/1237259]
loss: 0.016185 0.011269 [411648/1237259]
loss: 0.021089 0.011431 [616448/1237259]
loss: 0.014623 0.011307 [821248/1237259]
loss: 0.014491 0.011529 [1026048/1237259]
loss: 0.015628 0.011507 [1230848/1237259]
Epoch 529
-------------------------------
loss: 0.011550 0.011346 [ 2048/1237259]
loss: 0.014165 0.011492 [206848/1237259]
loss: 0.013830 0.011343 [411648/1237259]
loss: 0.014637 0.011513 [616448/1237259]
loss: 0.012193 0.011431 [821248/1237259]
loss: 0.010423 0.011381 [1026048/1237259]
loss: 0.023552 0.011441 [1230848/1237259]
Epoch 530
-------------------------------
loss: 0.016178 0.011432 [ 2048/1237259]
loss: 0.017107 0.011395 [206848/1237259]
loss: 0.023843 0.011386 [411648/1237259]
loss: 0.011949 0.011488 [616448/1237259]
loss: 0.015843 0.011486 [821248/1237259]
loss: 0.010850 0.011266 [1026048/1237259]
loss: 0.014431 0.011556 [1230848/1237259]
Eval results: 
recall@20: 0.0640  
ndcg@20: 0.0525  
diversity: 0.1956  


Epoch 531
-------------------------------
loss: 0.015422 0.011435 [ 2048/1237259]
loss: 0.010725 0.011427 [206848/1237259]
loss: 0.018063 0.011504 [411648/1237259]
loss: 0.013956 0.011478 [616448/1237259]
loss: 0.011970 0.011474 [821248/1237259]
loss: 0.018928 0.011507 [1026048/1237259]
loss: 0.011647 0.011434 [1230848/1237259]
Epoch 532
-------------------------------
loss: 0.015476 0.011481 [ 2048/1237259]
loss: 0.015224 0.011422 [206848/1237259]
loss: 0.011659 0.011422 [411648/1237259]
loss: 0.015614 0.011518 [616448/1237259]
loss: 0.015730 0.011419 [821248/1237259]
loss: 0.016205 0.011578 [1026048/1237259]
loss: 0.016025 0.011460 [1230848/1237259]
Epoch 533
-------------------------------
loss: 0.011524 0.011641 [ 2048/1237259]
loss: 0.010014 0.011438 [206848/1237259]
loss: 0.016168 0.011426 [411648/1237259]
loss: 0.014731 0.011341 [616448/1237259]
loss: 0.017556 0.011428 [821248/1237259]
loss: 0.011505 0.011543 [1026048/1237259]
loss: 0.012689 0.011487 [1230848/1237259]
Epoch 534
-------------------------------
loss: 0.017253 0.011474 [ 2048/1237259]
loss: 0.011951 0.011483 [206848/1237259]
loss: 0.020592 0.011526 [411648/1237259]
loss: 0.015789 0.011447 [616448/1237259]
loss: 0.012854 0.011455 [821248/1237259]
loss: 0.016359 0.011413 [1026048/1237259]
loss: 0.020143 0.011513 [1230848/1237259]
Epoch 535
-------------------------------
loss: 0.014163 0.011444 [ 2048/1237259]
loss: 0.012456 0.011276 [206848/1237259]
loss: 0.015294 0.011436 [411648/1237259]
loss: 0.018916 0.011414 [616448/1237259]
loss: 0.012325 0.011361 [821248/1237259]
loss: 0.016953 0.011501 [1026048/1237259]
loss: 0.015102 0.011453 [1230848/1237259]
Eval results: 
recall@20: 0.0644  
ndcg@20: 0.0527  
diversity: 0.1956  


Epoch 536
-------------------------------
loss: 0.014157 0.011564 [ 2048/1237259]
loss: 0.019845 0.011481 [206848/1237259]
loss: 0.013965 0.011421 [411648/1237259]
loss: 0.019058 0.011371 [616448/1237259]
loss: 0.014474 0.011519 [821248/1237259]
loss: 0.014799 0.011582 [1026048/1237259]
loss: 0.020031 0.011358 [1230848/1237259]
Epoch 537
-------------------------------
loss: 0.016928 0.011541 [ 2048/1237259]
loss: 0.018815 0.011593 [206848/1237259]
loss: 0.016020 0.011456 [411648/1237259]
loss: 0.015142 0.011515 [616448/1237259]
loss: 0.012845 0.011548 [821248/1237259]
loss: 0.012280 0.011434 [1026048/1237259]
loss: 0.018021 0.011568 [1230848/1237259]
Epoch 538
-------------------------------
loss: 0.012975 0.011353 [ 2048/1237259]
loss: 0.016645 0.011520 [206848/1237259]
loss: 0.018233 0.011543 [411648/1237259]
loss: 0.021357 0.011496 [616448/1237259]
loss: 0.021220 0.011516 [821248/1237259]
loss: 0.014935 0.011525 [1026048/1237259]
loss: 0.017685 0.011481 [1230848/1237259]
Epoch 539
-------------------------------
loss: 0.013499 0.011593 [ 2048/1237259]
loss: 0.014865 0.011450 [206848/1237259]
loss: 0.011984 0.011495 [411648/1237259]
loss: 0.012861 0.011414 [616448/1237259]
loss: 0.017530 0.011498 [821248/1237259]
loss: 0.015198 0.011472 [1026048/1237259]
loss: 0.012152 0.011384 [1230848/1237259]
Epoch 540
-------------------------------
loss: 0.014271 0.011520 [ 2048/1237259]
loss: 0.015008 0.011456 [206848/1237259]
loss: 0.016703 0.011499 [411648/1237259]
loss: 0.013621 0.011520 [616448/1237259]
loss: 0.012018 0.011550 [821248/1237259]
loss: 0.017191 0.011537 [1026048/1237259]
loss: 0.011771 0.011511 [1230848/1237259]
Eval results: 
recall@20: 0.0643  
ndcg@20: 0.0527  
diversity: 0.1955  


Epoch 541
-------------------------------
loss: 0.017006 0.011476 [ 2048/1237259]
loss: 0.015108 0.011452 [206848/1237259]
loss: 0.018866 0.011488 [411648/1237259]
loss: 0.016303 0.011506 [616448/1237259]
loss: 0.012740 0.011417 [821248/1237259]
loss: 0.012114 0.011388 [1026048/1237259]
loss: 0.018036 0.011630 [1230848/1237259]
Epoch 542
-------------------------------
loss: 0.015777 0.011430 [ 2048/1237259]
loss: 0.015961 0.011686 [206848/1237259]
loss: 0.017040 0.011511 [411648/1237259]
loss: 0.015992 0.011476 [616448/1237259]
loss: 0.016881 0.011511 [821248/1237259]
loss: 0.019741 0.011475 [1026048/1237259]
loss: 0.018685 0.011475 [1230848/1237259]
Epoch 543
-------------------------------
loss: 0.016748 0.011565 [ 2048/1237259]
loss: 0.014486 0.011567 [206848/1237259]
loss: 0.016633 0.011468 [411648/1237259]
loss: 0.013101 0.011528 [616448/1237259]
loss: 0.017263 0.011466 [821248/1237259]
loss: 0.021421 0.011504 [1026048/1237259]
loss: 0.013728 0.011466 [1230848/1237259]
Epoch 544
-------------------------------
loss: 0.020433 0.011527 [ 2048/1237259]
loss: 0.014432 0.011515 [206848/1237259]
loss: 0.015236 0.011646 [411648/1237259]
loss: 0.016033 0.011587 [616448/1237259]
loss: 0.013574 0.011506 [821248/1237259]
loss: 0.019389 0.011541 [1026048/1237259]
loss: 0.015079 0.011504 [1230848/1237259]
Epoch 545
-------------------------------
loss: 0.011151 0.011547 [ 2048/1237259]
loss: 0.014895 0.011463 [206848/1237259]
loss: 0.011436 0.011625 [411648/1237259]
loss: 0.018094 0.011402 [616448/1237259]
loss: 0.012741 0.011398 [821248/1237259]
loss: 0.017019 0.011464 [1026048/1237259]
loss: 0.012571 0.011661 [1230848/1237259]
Eval results: 
recall@20: 0.0644  
ndcg@20: 0.0528  
diversity: 0.1955  


Epoch 546
-------------------------------
loss: 0.015316 0.011539 [ 2048/1237259]
loss: 0.013728 0.011500 [206848/1237259]
loss: 0.014202 0.011544 [411648/1237259]
loss: 0.018824 0.011609 [616448/1237259]
loss: 0.013887 0.011522 [821248/1237259]
loss: 0.013677 0.011478 [1026048/1237259]
loss: 0.015993 0.011548 [1230848/1237259]
Epoch 547
-------------------------------
loss: 0.012813 0.011537 [ 2048/1237259]
loss: 0.020257 0.011616 [206848/1237259]
loss: 0.017170 0.011627 [411648/1237259]
loss: 0.015131 0.011560 [616448/1237259]
loss: 0.010838 0.011569 [821248/1237259]
loss: 0.017309 0.011565 [1026048/1237259]
loss: 0.013271 0.011515 [1230848/1237259]
Epoch 548
-------------------------------
loss: 0.015215 0.011521 [ 2048/1237259]
loss: 0.016464 0.011521 [206848/1237259]
loss: 0.016158 0.011554 [411648/1237259]
loss: 0.015200 0.011613 [616448/1237259]
loss: 0.017534 0.011530 [821248/1237259]
loss: 0.017624 0.011512 [1026048/1237259]
loss: 0.014038 0.011589 [1230848/1237259]
Epoch 549
-------------------------------
loss: 0.017662 0.011415 [ 2048/1237259]
loss: 0.013383 0.011668 [206848/1237259]
loss: 0.013593 0.011567 [411648/1237259]
loss: 0.015028 0.011597 [616448/1237259]
loss: 0.015172 0.011544 [821248/1237259]
loss: 0.012683 0.011507 [1026048/1237259]
loss: 0.013027 0.011587 [1230848/1237259]
Epoch 550
-------------------------------
loss: 0.015293 0.011686 [ 2048/1237259]
loss: 0.013422 0.011463 [206848/1237259]
loss: 0.016075 0.011556 [411648/1237259]
loss: 0.019565 0.011576 [616448/1237259]
loss: 0.014666 0.011558 [821248/1237259]
loss: 0.011291 0.011396 [1026048/1237259]
loss: 0.017342 0.011567 [1230848/1237259]
Eval results: 
recall@20: 0.0643  
ndcg@20: 0.0528  
diversity: 0.1956  


Epoch 551
-------------------------------
loss: 0.021245 0.011658 [ 2048/1237259]
loss: 0.016732 0.011568 [206848/1237259]
loss: 0.014852 0.011611 [411648/1237259]
loss: 0.012603 0.011539 [616448/1237259]
loss: 0.014529 0.011606 [821248/1237259]
loss: 0.015457 0.011554 [1026048/1237259]
loss: 0.013023 0.011590 [1230848/1237259]
Epoch 552
-------------------------------
loss: 0.014550 0.011521 [ 2048/1237259]
loss: 0.020615 0.011509 [206848/1237259]
loss: 0.014725 0.011660 [411648/1237259]
loss: 0.021104 0.011556 [616448/1237259]
loss: 0.016587 0.011515 [821248/1237259]
loss: 0.016958 0.011581 [1026048/1237259]
loss: 0.016526 0.011629 [1230848/1237259]
Epoch 553
-------------------------------
loss: 0.013020 0.011449 [ 2048/1237259]
loss: 0.016869 0.011578 [206848/1237259]
loss: 0.012212 0.011580 [411648/1237259]
loss: 0.013015 0.011458 [616448/1237259]
loss: 0.023148 0.011571 [821248/1237259]
loss: 0.012460 0.011726 [1026048/1237259]
loss: 0.016598 0.011603 [1230848/1237259]
Epoch 554
-------------------------------
loss: 0.016706 0.011682 [ 2048/1237259]
loss: 0.017699 0.011579 [206848/1237259]
loss: 0.014607 0.011550 [411648/1237259]
loss: 0.016756 0.011571 [616448/1237259]
loss: 0.014826 0.011550 [821248/1237259]
loss: 0.013570 0.011669 [1026048/1237259]
loss: 0.014713 0.011593 [1230848/1237259]
Epoch 555
-------------------------------
loss: 0.020752 0.011479 [ 2048/1237259]
loss: 0.021576 0.011559 [206848/1237259]
loss: 0.016016 0.011660 [411648/1237259]
loss: 0.015085 0.011549 [616448/1237259]
loss: 0.020761 0.011483 [821248/1237259]
loss: 0.020495 0.011647 [1026048/1237259]
loss: 0.019638 0.011643 [1230848/1237259]
Eval results: 
recall@20: 0.0642  
ndcg@20: 0.0526  
diversity: 0.1954  


Epoch 556
-------------------------------
loss: 0.015029 0.011646 [ 2048/1237259]
loss: 0.015246 0.011678 [206848/1237259]
loss: 0.012975 0.011558 [411648/1237259]
loss: 0.019033 0.011636 [616448/1237259]
loss: 0.011669 0.011555 [821248/1237259]
loss: 0.015682 0.011433 [1026048/1237259]
loss: 0.016947 0.011595 [1230848/1237259]
Epoch 557
-------------------------------
loss: 0.014723 0.011643 [ 2048/1237259]
loss: 0.009858 0.011610 [206848/1237259]
loss: 0.012759 0.011614 [411648/1237259]
loss: 0.017262 0.011503 [616448/1237259]
loss: 0.013415 0.011584 [821248/1237259]
loss: 0.014903 0.011588 [1026048/1237259]
loss: 0.014753 0.011606 [1230848/1237259]
Epoch 558
-------------------------------
loss: 0.014789 0.011640 [ 2048/1237259]
loss: 0.012985 0.011504 [206848/1237259]
loss: 0.013804 0.011661 [411648/1237259]
loss: 0.013324 0.011499 [616448/1237259]
loss: 0.014953 0.011603 [821248/1237259]
loss: 0.014096 0.011715 [1026048/1237259]
loss: 0.018346 0.011638 [1230848/1237259]
Epoch 559
-------------------------------
loss: 0.013446 0.011512 [ 2048/1237259]
loss: 0.014185 0.011547 [206848/1237259]
loss: 0.015334 0.011562 [411648/1237259]
loss: 0.011170 0.011678 [616448/1237259]
loss: 0.015526 0.011536 [821248/1237259]
loss: 0.017374 0.011597 [1026048/1237259]
loss: 0.015106 0.011603 [1230848/1237259]
Epoch 560
-------------------------------
loss: 0.013806 0.011651 [ 2048/1237259]
loss: 0.017613 0.011648 [206848/1237259]
loss: 0.020115 0.011576 [411648/1237259]
loss: 0.019098 0.011545 [616448/1237259]
loss: 0.017080 0.011714 [821248/1237259]
loss: 0.015981 0.011513 [1026048/1237259]
loss: 0.018228 0.011675 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0641  
ndcg@20: 0.0528  
diversity: 0.1958  


Epoch 561
-------------------------------
loss: 0.016553 0.011544 [ 2048/1237259]
loss: 0.016146 0.011557 [206848/1237259]
loss: 0.011871 0.011636 [411648/1237259]
loss: 0.019089 0.011877 [616448/1237259]
loss: 0.014767 0.011633 [821248/1237259]
loss: 0.016102 0.011576 [1026048/1237259]
loss: 0.012556 0.011678 [1230848/1237259]
Epoch 562
-------------------------------
loss: 0.012298 0.011521 [ 2048/1237259]
loss: 0.014464 0.011577 [206848/1237259]
loss: 0.017297 0.011665 [411648/1237259]
loss: 0.016963 0.011585 [616448/1237259]
loss: 0.010526 0.011504 [821248/1237259]
loss: 0.012272 0.011627 [1026048/1237259]
loss: 0.015447 0.011593 [1230848/1237259]
Epoch 563
-------------------------------
loss: 0.013513 0.011466 [ 2048/1237259]
loss: 0.017705 0.011593 [206848/1237259]
loss: 0.012758 0.011584 [411648/1237259]
loss: 0.017199 0.011577 [616448/1237259]
loss: 0.016841 0.011526 [821248/1237259]
loss: 0.014605 0.011700 [1026048/1237259]
loss: 0.014371 0.011626 [1230848/1237259]
Epoch 564
-------------------------------
loss: 0.013589 0.011555 [ 2048/1237259]
loss: 0.019611 0.011611 [206848/1237259]
loss: 0.017907 0.011602 [411648/1237259]
loss: 0.015252 0.011604 [616448/1237259]
loss: 0.013088 0.011576 [821248/1237259]
loss: 0.019901 0.011592 [1026048/1237259]
loss: 0.015931 0.011665 [1230848/1237259]
Epoch 565
-------------------------------
loss: 0.015322 0.011646 [ 2048/1237259]
loss: 0.015538 0.011673 [206848/1237259]
loss: 0.016121 0.011701 [411648/1237259]
loss: 0.015171 0.011728 [616448/1237259]
loss: 0.019268 0.011679 [821248/1237259]
loss: 0.016292 0.011614 [1026048/1237259]
loss: 0.019045 0.011484 [1230848/1237259]
Eval results: 
recall@20: 0.0644  
ndcg@20: 0.0528  
diversity: 0.1957  


Epoch 566
-------------------------------
loss: 0.013820 0.011511 [ 2048/1237259]
loss: 0.014517 0.011645 [206848/1237259]
loss: 0.014389 0.011679 [411648/1237259]
loss: 0.013129 0.011620 [616448/1237259]
loss: 0.012875 0.011604 [821248/1237259]
loss: 0.018904 0.011545 [1026048/1237259]
loss: 0.015258 0.011682 [1230848/1237259]
Epoch 567
-------------------------------
loss: 0.014739 0.011716 [ 2048/1237259]
loss: 0.011561 0.011640 [206848/1237259]
loss: 0.013977 0.011575 [411648/1237259]
loss: 0.020557 0.011725 [616448/1237259]
loss: 0.015842 0.011618 [821248/1237259]
loss: 0.012427 0.011594 [1026048/1237259]
loss: 0.011718 0.011537 [1230848/1237259]
Epoch 568
-------------------------------
loss: 0.013308 0.011726 [ 2048/1237259]
loss: 0.013806 0.011522 [206848/1237259]
loss: 0.017238 0.011587 [411648/1237259]
loss: 0.013954 0.011654 [616448/1237259]
loss: 0.016065 0.011695 [821248/1237259]
loss: 0.010877 0.011528 [1026048/1237259]
loss: 0.013106 0.011648 [1230848/1237259]
Epoch 569
-------------------------------
loss: 0.012491 0.011498 [ 2048/1237259]
loss: 0.010744 0.011662 [206848/1237259]
loss: 0.015377 0.011724 [411648/1237259]
loss: 0.016083 0.011563 [616448/1237259]
loss: 0.013236 0.011699 [821248/1237259]
loss: 0.018591 0.011657 [1026048/1237259]
loss: 0.013807 0.011655 [1230848/1237259]
Epoch 570
-------------------------------
loss: 0.014910 0.011702 [ 2048/1237259]
loss: 0.012955 0.011798 [206848/1237259]
loss: 0.020330 0.011715 [411648/1237259]
loss: 0.012558 0.011777 [616448/1237259]
loss: 0.012631 0.011663 [821248/1237259]
loss: 0.019797 0.011625 [1026048/1237259]
loss: 0.016069 0.011802 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0646  
ndcg@20: 0.0528  
diversity: 0.1956  


Epoch 571
-------------------------------
loss: 0.017466 0.011636 [ 2048/1237259]
loss: 0.018344 0.011674 [206848/1237259]
loss: 0.013488 0.011697 [411648/1237259]
loss: 0.015373 0.011521 [616448/1237259]
loss: 0.012739 0.011783 [821248/1237259]
loss: 0.013117 0.011703 [1026048/1237259]
loss: 0.015317 0.011574 [1230848/1237259]
Epoch 572
-------------------------------
loss: 0.011387 0.011713 [ 2048/1237259]
loss: 0.011483 0.011700 [206848/1237259]
loss: 0.014775 0.011775 [411648/1237259]
loss: 0.017370 0.011730 [616448/1237259]
loss: 0.017443 0.011746 [821248/1237259]
loss: 0.009777 0.011814 [1026048/1237259]
loss: 0.012643 0.011712 [1230848/1237259]
Epoch 573
-------------------------------
loss: 0.011956 0.011711 [ 2048/1237259]
loss: 0.009634 0.011666 [206848/1237259]
loss: 0.009785 0.011688 [411648/1237259]
loss: 0.013954 0.011656 [616448/1237259]
loss: 0.013105 0.011790 [821248/1237259]
loss: 0.014690 0.011727 [1026048/1237259]
loss: 0.014481 0.011680 [1230848/1237259]
Epoch 574
-------------------------------
loss: 0.014300 0.011668 [ 2048/1237259]
loss: 0.017856 0.011644 [206848/1237259]
loss: 0.014323 0.011722 [411648/1237259]
loss: 0.016556 0.011708 [616448/1237259]
loss: 0.016095 0.011750 [821248/1237259]
loss: 0.012035 0.011722 [1026048/1237259]
loss: 0.013752 0.011745 [1230848/1237259]
Epoch 575
-------------------------------
loss: 0.015858 0.011684 [ 2048/1237259]
loss: 0.010994 0.011575 [206848/1237259]
loss: 0.013506 0.011670 [411648/1237259]
loss: 0.016523 0.011687 [616448/1237259]
loss: 0.019522 0.011651 [821248/1237259]
loss: 0.015278 0.011776 [1026048/1237259]
loss: 0.018068 0.011763 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0645  
ndcg@20: 0.0527  
diversity: 0.1959  


Epoch 576
-------------------------------
loss: 0.017321 0.011709 [ 2048/1237259]
loss: 0.013017 0.011641 [206848/1237259]
loss: 0.014133 0.011763 [411648/1237259]
loss: 0.020421 0.011655 [616448/1237259]
loss: 0.014185 0.011703 [821248/1237259]
loss: 0.011607 0.011705 [1026048/1237259]
loss: 0.010994 0.011698 [1230848/1237259]
Epoch 577
-------------------------------
loss: 0.011716 0.011804 [ 2048/1237259]
loss: 0.015020 0.011570 [206848/1237259]
loss: 0.016073 0.011840 [411648/1237259]
loss: 0.012406 0.011707 [616448/1237259]
loss: 0.012586 0.011718 [821248/1237259]
loss: 0.015484 0.011789 [1026048/1237259]
loss: 0.012736 0.011747 [1230848/1237259]
Epoch 578
-------------------------------
loss: 0.018970 0.011776 [ 2048/1237259]
loss: 0.013607 0.011814 [206848/1237259]
loss: 0.018928 0.011654 [411648/1237259]
loss: 0.014327 0.011774 [616448/1237259]
loss: 0.014947 0.011712 [821248/1237259]
loss: 0.014088 0.011748 [1026048/1237259]
loss: 0.014387 0.011723 [1230848/1237259]
Epoch 579
-------------------------------
loss: 0.014338 0.011722 [ 2048/1237259]
loss: 0.018301 0.011858 [206848/1237259]
loss: 0.013655 0.011617 [411648/1237259]
loss: 0.021814 0.011639 [616448/1237259]
loss: 0.012272 0.011785 [821248/1237259]
loss: 0.015899 0.011683 [1026048/1237259]
loss: 0.017452 0.011826 [1230848/1237259]
Epoch 580
-------------------------------
loss: 0.016934 0.011715 [ 2048/1237259]
loss: 0.013985 0.011756 [206848/1237259]
loss: 0.015579 0.011708 [411648/1237259]
loss: 0.012818 0.011742 [616448/1237259]
loss: 0.015283 0.011773 [821248/1237259]
loss: 0.013737 0.011729 [1026048/1237259]
loss: 0.015530 0.011793 [1230848/1237259]
Eval results: 
recall@20: 0.0645  
ndcg@20: 0.0527  
diversity: 0.1958  


Epoch 581
-------------------------------
loss: 0.020004 0.011729 [ 2048/1237259]
loss: 0.014480 0.011720 [206848/1237259]
loss: 0.012035 0.011606 [411648/1237259]
loss: 0.024650 0.011718 [616448/1237259]
loss: 0.014013 0.011727 [821248/1237259]
loss: 0.012939 0.011797 [1026048/1237259]
loss: 0.017235 0.011692 [1230848/1237259]
Epoch 582
-------------------------------
loss: 0.011834 0.011800 [ 2048/1237259]
loss: 0.015019 0.011852 [206848/1237259]
loss: 0.015079 0.011713 [411648/1237259]
loss: 0.017921 0.011730 [616448/1237259]
loss: 0.013576 0.011708 [821248/1237259]
loss: 0.014954 0.011812 [1026048/1237259]
loss: 0.015968 0.011728 [1230848/1237259]
Epoch 583
-------------------------------
loss: 0.013350 0.011716 [ 2048/1237259]
loss: 0.012249 0.011733 [206848/1237259]
loss: 0.011532 0.011719 [411648/1237259]
loss: 0.015445 0.011782 [616448/1237259]
loss: 0.017785 0.011702 [821248/1237259]
loss: 0.015708 0.011651 [1026048/1237259]
loss: 0.014322 0.011686 [1230848/1237259]
Epoch 584
-------------------------------
loss: 0.019632 0.011724 [ 2048/1237259]
loss: 0.017482 0.011805 [206848/1237259]
loss: 0.017576 0.011743 [411648/1237259]
loss: 0.019547 0.011681 [616448/1237259]
loss: 0.013324 0.011776 [821248/1237259]
loss: 0.015055 0.011689 [1026048/1237259]
loss: 0.012236 0.011733 [1230848/1237259]
Epoch 585
-------------------------------
loss: 0.017230 0.011760 [ 2048/1237259]
loss: 0.017839 0.011697 [206848/1237259]
loss: 0.013217 0.011751 [411648/1237259]
loss: 0.013459 0.011844 [616448/1237259]
loss: 0.021640 0.011740 [821248/1237259]
loss: 0.016294 0.011733 [1026048/1237259]
loss: 0.015962 0.011760 [1230848/1237259]
Eval results: 
recall@20: 0.0646  
ndcg@20: 0.0527  
diversity: 0.1957  


Epoch 586
-------------------------------
loss: 0.016204 0.011769 [ 2048/1237259]
loss: 0.014271 0.011809 [206848/1237259]
loss: 0.015745 0.011758 [411648/1237259]
loss: 0.013902 0.011687 [616448/1237259]
loss: 0.014176 0.011857 [821248/1237259]
loss: 0.014651 0.011734 [1026048/1237259]
loss: 0.012998 0.011742 [1230848/1237259]
Epoch 587
-------------------------------
loss: 0.013952 0.011740 [ 2048/1237259]
loss: 0.016542 0.011708 [206848/1237259]
loss: 0.015230 0.011859 [411648/1237259]
loss: 0.015160 0.011774 [616448/1237259]
loss: 0.015034 0.011888 [821248/1237259]
loss: 0.013507 0.011778 [1026048/1237259]
loss: 0.012679 0.011845 [1230848/1237259]
Epoch 588
-------------------------------
loss: 0.015021 0.011703 [ 2048/1237259]
loss: 0.013828 0.011705 [206848/1237259]
loss: 0.014872 0.011669 [411648/1237259]
loss: 0.012900 0.011701 [616448/1237259]
loss: 0.014279 0.011808 [821248/1237259]
loss: 0.013596 0.011708 [1026048/1237259]
loss: 0.017761 0.011773 [1230848/1237259]
Epoch 589
-------------------------------
loss: 0.016505 0.011854 [ 2048/1237259]
loss: 0.012292 0.011845 [206848/1237259]
loss: 0.014920 0.011806 [411648/1237259]
loss: 0.018424 0.011743 [616448/1237259]
loss: 0.017199 0.011706 [821248/1237259]
loss: 0.018261 0.011742 [1026048/1237259]
loss: 0.013936 0.011803 [1230848/1237259]
Epoch 590
-------------------------------
loss: 0.019369 0.011759 [ 2048/1237259]
loss: 0.012141 0.011754 [206848/1237259]
loss: 0.012859 0.011750 [411648/1237259]
loss: 0.018841 0.011689 [616448/1237259]
loss: 0.014189 0.011841 [821248/1237259]
loss: 0.013343 0.011791 [1026048/1237259]
loss: 0.009491 0.011880 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0646  
ndcg@20: 0.0526  
diversity: 0.1959  


Epoch 591
-------------------------------
loss: 0.014781 0.011743 [ 2048/1237259]
loss: 0.013618 0.011868 [206848/1237259]
loss: 0.015327 0.011756 [411648/1237259]
loss: 0.017035 0.011675 [616448/1237259]
loss: 0.012160 0.011796 [821248/1237259]
loss: 0.016952 0.011838 [1026048/1237259]
loss: 0.018450 0.011822 [1230848/1237259]
Epoch 592
-------------------------------
loss: 0.012397 0.011756 [ 2048/1237259]
loss: 0.015230 0.011864 [206848/1237259]
loss: 0.014771 0.011813 [411648/1237259]
loss: 0.010875 0.011759 [616448/1237259]
loss: 0.014761 0.011684 [821248/1237259]
loss: 0.016158 0.011774 [1026048/1237259]
loss: 0.014850 0.011838 [1230848/1237259]
Epoch 593
-------------------------------
loss: 0.018666 0.011822 [ 2048/1237259]
loss: 0.016470 0.011886 [206848/1237259]
loss: 0.013762 0.011758 [411648/1237259]
loss: 0.013387 0.011862 [616448/1237259]
loss: 0.019095 0.011771 [821248/1237259]
loss: 0.013972 0.011843 [1026048/1237259]
loss: 0.014288 0.011763 [1230848/1237259]
Epoch 594
-------------------------------
loss: 0.013394 0.011852 [ 2048/1237259]
loss: 0.012964 0.011715 [206848/1237259]
loss: 0.017541 0.011834 [411648/1237259]
loss: 0.017862 0.011708 [616448/1237259]
loss: 0.015287 0.011802 [821248/1237259]
loss: 0.017129 0.011715 [1026048/1237259]
loss: 0.016241 0.011782 [1230848/1237259]
Epoch 595
-------------------------------
loss: 0.014851 0.011798 [ 2048/1237259]
loss: 0.012494 0.011729 [206848/1237259]
loss: 0.018404 0.011661 [411648/1237259]
loss: 0.013125 0.011919 [616448/1237259]
loss: 0.013168 0.011658 [821248/1237259]
loss: 0.013443 0.011772 [1026048/1237259]
loss: 0.013603 0.011805 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0646  
ndcg@20: 0.0528  
diversity: 0.1958  


Epoch 596
-------------------------------
loss: 0.014140 0.011896 [ 2048/1237259]
loss: 0.014823 0.011776 [206848/1237259]
loss: 0.017421 0.011671 [411648/1237259]
loss: 0.011422 0.011696 [616448/1237259]
loss: 0.013661 0.011762 [821248/1237259]
loss: 0.013273 0.011792 [1026048/1237259]
loss: 0.016742 0.011763 [1230848/1237259]
Epoch 597
-------------------------------
loss: 0.015080 0.011917 [ 2048/1237259]
loss: 0.015120 0.011887 [206848/1237259]
loss: 0.014589 0.011888 [411648/1237259]
loss: 0.014694 0.011889 [616448/1237259]
loss: 0.013242 0.011777 [821248/1237259]
loss: 0.019827 0.011746 [1026048/1237259]
loss: 0.016084 0.011837 [1230848/1237259]
Epoch 598
-------------------------------
loss: 0.011253 0.011762 [ 2048/1237259]
loss: 0.013142 0.011813 [206848/1237259]
loss: 0.011983 0.011763 [411648/1237259]
loss: 0.012301 0.011730 [616448/1237259]
loss: 0.014990 0.011958 [821248/1237259]
loss: 0.018893 0.011883 [1026048/1237259]
loss: 0.016221 0.011854 [1230848/1237259]
Epoch 599
-------------------------------
loss: 0.010579 0.011845 [ 2048/1237259]
loss: 0.014024 0.011935 [206848/1237259]
loss: 0.016147 0.011815 [411648/1237259]
loss: 0.017052 0.011790 [616448/1237259]
loss: 0.014390 0.011716 [821248/1237259]
loss: 0.011889 0.011933 [1026048/1237259]
loss: 0.013659 0.011717 [1230848/1237259]
Epoch 600
-------------------------------
loss: 0.014106 0.011811 [ 2048/1237259]
loss: 0.020741 0.011800 [206848/1237259]
loss: 0.015799 0.011830 [411648/1237259]
loss: 0.015752 0.011974 [616448/1237259]
loss: 0.014280 0.011708 [821248/1237259]
loss: 0.016802 0.011850 [1026048/1237259]
loss: 0.010200 0.011787 [1230848/1237259]
Eval results: 
recall@20: 0.0646  
ndcg@20: 0.0528  
diversity: 0.1957  


Epoch 601
-------------------------------
loss: 0.012320 0.011937 [ 2048/1237259]
loss: 0.011783 0.011868 [206848/1237259]
loss: 0.013928 0.011826 [411648/1237259]
loss: 0.014537 0.011726 [616448/1237259]
loss: 0.014169 0.011772 [821248/1237259]
loss: 0.013771 0.011788 [1026048/1237259]
loss: 0.011378 0.011751 [1230848/1237259]
Epoch 602
-------------------------------
loss: 0.013290 0.011736 [ 2048/1237259]
loss: 0.017053 0.011814 [206848/1237259]
loss: 0.015322 0.011826 [411648/1237259]
loss: 0.015020 0.011824 [616448/1237259]
loss: 0.012258 0.011757 [821248/1237259]
loss: 0.017139 0.011824 [1026048/1237259]
loss: 0.014949 0.011768 [1230848/1237259]
Epoch 603
-------------------------------
loss: 0.018711 0.011757 [ 2048/1237259]
loss: 0.014552 0.011813 [206848/1237259]
loss: 0.014749 0.011864 [411648/1237259]
loss: 0.010240 0.011823 [616448/1237259]
loss: 0.015694 0.011826 [821248/1237259]
loss: 0.020044 0.011780 [1026048/1237259]
loss: 0.017584 0.011881 [1230848/1237259]
Epoch 604
-------------------------------
loss: 0.015344 0.011824 [ 2048/1237259]
loss: 0.017151 0.011785 [206848/1237259]
loss: 0.012272 0.011771 [411648/1237259]
loss: 0.014432 0.011810 [616448/1237259]
loss: 0.018833 0.011844 [821248/1237259]
loss: 0.016658 0.011684 [1026048/1237259]
loss: 0.016240 0.011778 [1230848/1237259]
Epoch 605
-------------------------------
loss: 0.012898 0.011770 [ 2048/1237259]
loss: 0.016782 0.011822 [206848/1237259]
loss: 0.013692 0.011948 [411648/1237259]
loss: 0.014683 0.011904 [616448/1237259]
loss: 0.012860 0.011866 [821248/1237259]
loss: 0.012857 0.011841 [1026048/1237259]
loss: 0.014762 0.011784 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0647  
ndcg@20: 0.0529  
diversity: 0.1957  


Epoch 606
-------------------------------
loss: 0.014823 0.011789 [ 2048/1237259]
loss: 0.013711 0.011816 [206848/1237259]
loss: 0.012306 0.011892 [411648/1237259]
loss: 0.013526 0.011847 [616448/1237259]
loss: 0.015559 0.011846 [821248/1237259]
loss: 0.012527 0.011854 [1026048/1237259]
loss: 0.015706 0.011800 [1230848/1237259]
Epoch 607
-------------------------------
loss: 0.013796 0.011712 [ 2048/1237259]
loss: 0.019765 0.011719 [206848/1237259]
loss: 0.012916 0.011882 [411648/1237259]
loss: 0.016877 0.011973 [616448/1237259]
loss: 0.017121 0.011746 [821248/1237259]
loss: 0.015803 0.011874 [1026048/1237259]
loss: 0.015781 0.011884 [1230848/1237259]
Epoch 608
-------------------------------
loss: 0.012187 0.011896 [ 2048/1237259]
loss: 0.012292 0.011777 [206848/1237259]
loss: 0.014412 0.011860 [411648/1237259]
loss: 0.018605 0.011921 [616448/1237259]
loss: 0.017123 0.011801 [821248/1237259]
loss: 0.017061 0.011803 [1026048/1237259]
loss: 0.016321 0.011815 [1230848/1237259]
Epoch 609
-------------------------------
loss: 0.015875 0.011802 [ 2048/1237259]
loss: 0.022479 0.012028 [206848/1237259]
loss: 0.009987 0.011800 [411648/1237259]
loss: 0.013381 0.011880 [616448/1237259]
loss: 0.016337 0.011815 [821248/1237259]
loss: 0.012784 0.011847 [1026048/1237259]
loss: 0.014675 0.011900 [1230848/1237259]
Epoch 610
-------------------------------
loss: 0.020214 0.011874 [ 2048/1237259]
loss: 0.016858 0.011883 [206848/1237259]
loss: 0.014187 0.011915 [411648/1237259]
loss: 0.012127 0.011758 [616448/1237259]
loss: 0.016072 0.011899 [821248/1237259]
loss: 0.022819 0.011881 [1026048/1237259]
loss: 0.009603 0.011833 [1230848/1237259]
Eval results: 
recall@20: 0.0647  
ndcg@20: 0.0529  
diversity: 0.1958  


Epoch 611
-------------------------------
loss: 0.015835 0.011817 [ 2048/1237259]
loss: 0.017137 0.011899 [206848/1237259]
loss: 0.015167 0.011890 [411648/1237259]
loss: 0.014392 0.011889 [616448/1237259]
loss: 0.013976 0.011837 [821248/1237259]
loss: 0.012931 0.011947 [1026048/1237259]
loss: 0.012444 0.011910 [1230848/1237259]
Epoch 612
-------------------------------
loss: 0.016980 0.011900 [ 2048/1237259]
loss: 0.013724 0.011890 [206848/1237259]
loss: 0.013630 0.012013 [411648/1237259]
loss: 0.011390 0.011835 [616448/1237259]
loss: 0.018282 0.011855 [821248/1237259]
loss: 0.012649 0.011717 [1026048/1237259]
loss: 0.013185 0.011886 [1230848/1237259]
Epoch 613
-------------------------------
loss: 0.012612 0.011860 [ 2048/1237259]
loss: 0.020567 0.011882 [206848/1237259]
loss: 0.017527 0.011915 [411648/1237259]
loss: 0.014736 0.011896 [616448/1237259]
loss: 0.014888 0.011863 [821248/1237259]
loss: 0.016833 0.011832 [1026048/1237259]
loss: 0.015874 0.011848 [1230848/1237259]
Epoch 614
-------------------------------
loss: 0.016652 0.011825 [ 2048/1237259]
loss: 0.014992 0.011836 [206848/1237259]
loss: 0.018742 0.011937 [411648/1237259]
loss: 0.015658 0.011899 [616448/1237259]
loss: 0.017485 0.011819 [821248/1237259]
loss: 0.011809 0.011972 [1026048/1237259]
loss: 0.016532 0.011858 [1230848/1237259]
Epoch 615
-------------------------------
loss: 0.013557 0.011842 [ 2048/1237259]
loss: 0.017728 0.011912 [206848/1237259]
loss: 0.010769 0.011886 [411648/1237259]
loss: 0.014898 0.011874 [616448/1237259]
loss: 0.011846 0.011961 [821248/1237259]
loss: 0.013869 0.011896 [1026048/1237259]
loss: 0.016142 0.012013 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0646  
ndcg@20: 0.0529  
diversity: 0.1956  


Epoch 616
-------------------------------
loss: 0.015491 0.011783 [ 2048/1237259]
loss: 0.012632 0.011848 [206848/1237259]
loss: 0.014657 0.011966 [411648/1237259]
loss: 0.014428 0.011979 [616448/1237259]
loss: 0.016277 0.011827 [821248/1237259]
loss: 0.013175 0.011787 [1026048/1237259]
loss: 0.017365 0.011893 [1230848/1237259]
Epoch 617
-------------------------------
loss: 0.013856 0.011970 [ 2048/1237259]
loss: 0.019292 0.011877 [206848/1237259]
loss: 0.011208 0.011906 [411648/1237259]
loss: 0.016892 0.011966 [616448/1237259]
loss: 0.010547 0.011756 [821248/1237259]
loss: 0.014597 0.011950 [1026048/1237259]
loss: 0.019141 0.011945 [1230848/1237259]
Epoch 618
-------------------------------
loss: 0.013007 0.011774 [ 2048/1237259]
loss: 0.012667 0.012010 [206848/1237259]
loss: 0.017738 0.011853 [411648/1237259]
loss: 0.014212 0.011843 [616448/1237259]
loss: 0.012004 0.011914 [821248/1237259]
loss: 0.010957 0.011773 [1026048/1237259]
loss: 0.019680 0.011887 [1230848/1237259]
Epoch 619
-------------------------------
loss: 0.013164 0.011889 [ 2048/1237259]
loss: 0.014698 0.011796 [206848/1237259]
loss: 0.009970 0.011799 [411648/1237259]
loss: 0.012586 0.011983 [616448/1237259]
loss: 0.012496 0.011824 [821248/1237259]
loss: 0.011009 0.011889 [1026048/1237259]
loss: 0.015345 0.011888 [1230848/1237259]
Epoch 620
-------------------------------
loss: 0.015378 0.011896 [ 2048/1237259]
loss: 0.016304 0.011826 [206848/1237259]
loss: 0.011892 0.011860 [411648/1237259]
loss: 0.011502 0.012018 [616448/1237259]
loss: 0.014063 0.011980 [821248/1237259]
loss: 0.012001 0.011971 [1026048/1237259]
loss: 0.018641 0.011932 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0648  
ndcg@20: 0.0530  
diversity: 0.1960  


Epoch 621
-------------------------------
loss: 0.018295 0.011875 [ 2048/1237259]
loss: 0.016005 0.012044 [206848/1237259]
loss: 0.013467 0.011825 [411648/1237259]
loss: 0.021350 0.011993 [616448/1237259]
loss: 0.015718 0.011823 [821248/1237259]
loss: 0.014142 0.011768 [1026048/1237259]
loss: 0.014374 0.011865 [1230848/1237259]
Epoch 622
-------------------------------
loss: 0.014719 0.011844 [ 2048/1237259]
loss: 0.015084 0.011840 [206848/1237259]
loss: 0.017484 0.011940 [411648/1237259]
loss: 0.011653 0.011912 [616448/1237259]
loss: 0.012800 0.011931 [821248/1237259]
loss: 0.015265 0.011918 [1026048/1237259]
loss: 0.012555 0.011847 [1230848/1237259]
Epoch 623
-------------------------------
loss: 0.012853 0.011945 [ 2048/1237259]
loss: 0.016847 0.011936 [206848/1237259]
loss: 0.014644 0.011945 [411648/1237259]
loss: 0.014168 0.011979 [616448/1237259]
loss: 0.011880 0.011904 [821248/1237259]
loss: 0.013239 0.011938 [1026048/1237259]
loss: 0.010890 0.011957 [1230848/1237259]
Epoch 624
-------------------------------
loss: 0.014851 0.011813 [ 2048/1237259]
loss: 0.012476 0.011959 [206848/1237259]
loss: 0.013548 0.011873 [411648/1237259]
loss: 0.015600 0.011801 [616448/1237259]
loss: 0.020613 0.011944 [821248/1237259]
loss: 0.013344 0.011896 [1026048/1237259]
loss: 0.015036 0.011875 [1230848/1237259]
Epoch 625
-------------------------------
loss: 0.015350 0.011973 [ 2048/1237259]
loss: 0.015459 0.011974 [206848/1237259]
loss: 0.013919 0.011916 [411648/1237259]
loss: 0.016670 0.011846 [616448/1237259]
loss: 0.014972 0.011907 [821248/1237259]
loss: 0.016930 0.011839 [1026048/1237259]
loss: 0.017182 0.011951 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0647  
ndcg@20: 0.0529  
diversity: 0.1960  


Epoch 626
-------------------------------
loss: 0.012954 0.011847 [ 2048/1237259]
loss: 0.014973 0.011949 [206848/1237259]
loss: 0.015264 0.011822 [411648/1237259]
loss: 0.013980 0.011963 [616448/1237259]
loss: 0.018321 0.011915 [821248/1237259]
loss: 0.009030 0.012018 [1026048/1237259]
loss: 0.015864 0.012060 [1230848/1237259]
Epoch 627
-------------------------------
loss: 0.014553 0.011950 [ 2048/1237259]
loss: 0.011644 0.011847 [206848/1237259]
loss: 0.010402 0.011882 [411648/1237259]
loss: 0.013800 0.012025 [616448/1237259]
loss: 0.017048 0.011985 [821248/1237259]
loss: 0.012967 0.011979 [1026048/1237259]
loss: 0.014992 0.011966 [1230848/1237259]
Epoch 628
-------------------------------
loss: 0.010458 0.011900 [ 2048/1237259]
loss: 0.015498 0.011994 [206848/1237259]
loss: 0.014814 0.011874 [411648/1237259]
loss: 0.017468 0.011966 [616448/1237259]
loss: 0.014680 0.012022 [821248/1237259]
loss: 0.019124 0.011831 [1026048/1237259]
loss: 0.013762 0.012001 [1230848/1237259]
Epoch 629
-------------------------------
loss: 0.018090 0.011944 [ 2048/1237259]
loss: 0.011932 0.011967 [206848/1237259]
loss: 0.016008 0.012090 [411648/1237259]
loss: 0.014613 0.011888 [616448/1237259]
loss: 0.017908 0.011964 [821248/1237259]
loss: 0.013164 0.011942 [1026048/1237259]
loss: 0.014823 0.011918 [1230848/1237259]
Epoch 630
-------------------------------
loss: 0.011557 0.011938 [ 2048/1237259]
loss: 0.018801 0.011785 [206848/1237259]
loss: 0.013669 0.011972 [411648/1237259]
loss: 0.011203 0.012035 [616448/1237259]
loss: 0.022211 0.012047 [821248/1237259]
loss: 0.015427 0.011902 [1026048/1237259]
loss: 0.012753 0.011909 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0531  
diversity: 0.1960  


Epoch 631
-------------------------------
loss: 0.013697 0.011996 [ 2048/1237259]
loss: 0.015539 0.011976 [206848/1237259]
loss: 0.015667 0.012032 [411648/1237259]
loss: 0.013987 0.012079 [616448/1237259]
loss: 0.011830 0.012063 [821248/1237259]
loss: 0.017461 0.011907 [1026048/1237259]
loss: 0.012878 0.011917 [1230848/1237259]
Epoch 632
-------------------------------
loss: 0.016638 0.011949 [ 2048/1237259]
loss: 0.012939 0.012003 [206848/1237259]
loss: 0.016254 0.011976 [411648/1237259]
loss: 0.014783 0.011974 [616448/1237259]
loss: 0.011990 0.011866 [821248/1237259]
loss: 0.011616 0.011914 [1026048/1237259]
loss: 0.019641 0.011918 [1230848/1237259]
Epoch 633
-------------------------------
loss: 0.014885 0.012024 [ 2048/1237259]
loss: 0.014024 0.011965 [206848/1237259]
loss: 0.014476 0.011946 [411648/1237259]
loss: 0.012989 0.011901 [616448/1237259]
loss: 0.017418 0.011998 [821248/1237259]
loss: 0.016072 0.011930 [1026048/1237259]
loss: 0.014226 0.011978 [1230848/1237259]
Epoch 634
-------------------------------
loss: 0.013898 0.011938 [ 2048/1237259]
loss: 0.012924 0.011852 [206848/1237259]
loss: 0.016699 0.011890 [411648/1237259]
loss: 0.009920 0.011941 [616448/1237259]
loss: 0.015852 0.011896 [821248/1237259]
loss: 0.011226 0.012018 [1026048/1237259]
loss: 0.013595 0.012038 [1230848/1237259]
Epoch 635
-------------------------------
loss: 0.017204 0.011961 [ 2048/1237259]
loss: 0.015269 0.011886 [206848/1237259]
loss: 0.013583 0.012013 [411648/1237259]
loss: 0.014392 0.012028 [616448/1237259]
loss: 0.014197 0.011918 [821248/1237259]
loss: 0.014113 0.012018 [1026048/1237259]
loss: 0.015779 0.011894 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0532  
diversity: 0.1959  


Epoch 636
-------------------------------
loss: 0.015429 0.011981 [ 2048/1237259]
loss: 0.013166 0.011934 [206848/1237259]
loss: 0.013796 0.011996 [411648/1237259]
loss: 0.014266 0.012016 [616448/1237259]
loss: 0.013234 0.011852 [821248/1237259]
loss: 0.015693 0.011965 [1026048/1237259]
loss: 0.015967 0.012112 [1230848/1237259]
Epoch 637
-------------------------------
loss: 0.009997 0.011899 [ 2048/1237259]
loss: 0.013948 0.012022 [206848/1237259]
loss: 0.015748 0.011817 [411648/1237259]
loss: 0.015758 0.011854 [616448/1237259]
loss: 0.012127 0.011956 [821248/1237259]
loss: 0.011135 0.011887 [1026048/1237259]
loss: 0.010299 0.011907 [1230848/1237259]
Epoch 638
-------------------------------
loss: 0.013257 0.012084 [ 2048/1237259]
loss: 0.016063 0.011976 [206848/1237259]
loss: 0.012671 0.011840 [411648/1237259]
loss: 0.015193 0.011934 [616448/1237259]
loss: 0.011429 0.011964 [821248/1237259]
loss: 0.011809 0.011975 [1026048/1237259]
loss: 0.018848 0.011980 [1230848/1237259]
Epoch 639
-------------------------------
loss: 0.010266 0.012028 [ 2048/1237259]
loss: 0.015082 0.011966 [206848/1237259]
loss: 0.012492 0.012136 [411648/1237259]
loss: 0.010777 0.012008 [616448/1237259]
loss: 0.014812 0.011913 [821248/1237259]
loss: 0.012765 0.011929 [1026048/1237259]
loss: 0.016644 0.012034 [1230848/1237259]
Epoch 640
-------------------------------
loss: 0.012935 0.011899 [ 2048/1237259]
loss: 0.011878 0.012054 [206848/1237259]
loss: 0.013989 0.012001 [411648/1237259]
loss: 0.011654 0.011939 [616448/1237259]
loss: 0.013911 0.011934 [821248/1237259]
loss: 0.012515 0.012004 [1026048/1237259]
loss: 0.013406 0.012037 [1230848/1237259]
Eval results: 
recall@20: 0.0648  
ndcg@20: 0.0531  
diversity: 0.1957  


Epoch 641
-------------------------------
loss: 0.014731 0.011884 [ 2048/1237259]
loss: 0.012948 0.011950 [206848/1237259]
loss: 0.014838 0.011933 [411648/1237259]
loss: 0.012807 0.012001 [616448/1237259]
loss: 0.016991 0.011889 [821248/1237259]
loss: 0.014182 0.012066 [1026048/1237259]
loss: 0.011684 0.011936 [1230848/1237259]
Epoch 642
-------------------------------
loss: 0.011010 0.012021 [ 2048/1237259]
loss: 0.014070 0.012059 [206848/1237259]
loss: 0.012602 0.012021 [411648/1237259]
loss: 0.011275 0.011956 [616448/1237259]
loss: 0.012033 0.011961 [821248/1237259]
loss: 0.012375 0.012001 [1026048/1237259]
loss: 0.010168 0.012218 [1230848/1237259]
Epoch 643
-------------------------------
loss: 0.012043 0.011888 [ 2048/1237259]
loss: 0.010066 0.011907 [206848/1237259]
loss: 0.018118 0.011981 [411648/1237259]
loss: 0.012091 0.012084 [616448/1237259]
loss: 0.014134 0.012067 [821248/1237259]
loss: 0.012562 0.011945 [1026048/1237259]
loss: 0.012372 0.012005 [1230848/1237259]
Epoch 644
-------------------------------
loss: 0.013007 0.011984 [ 2048/1237259]
loss: 0.018212 0.012087 [206848/1237259]
loss: 0.018811 0.012058 [411648/1237259]
loss: 0.015032 0.012022 [616448/1237259]
loss: 0.016174 0.011864 [821248/1237259]
loss: 0.015244 0.011982 [1026048/1237259]
loss: 0.016816 0.012029 [1230848/1237259]
Epoch 645
-------------------------------
loss: 0.014310 0.012018 [ 2048/1237259]
loss: 0.018479 0.011917 [206848/1237259]
loss: 0.009042 0.012037 [411648/1237259]
loss: 0.016022 0.011964 [616448/1237259]
loss: 0.014877 0.011954 [821248/1237259]
loss: 0.008719 0.012083 [1026048/1237259]
loss: 0.015778 0.011995 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0531  
diversity: 0.1957  


Epoch 646
-------------------------------
loss: 0.011217 0.012056 [ 2048/1237259]
loss: 0.008697 0.011974 [206848/1237259]
loss: 0.010001 0.012067 [411648/1237259]
loss: 0.012606 0.012013 [616448/1237259]
loss: 0.012750 0.012022 [821248/1237259]
loss: 0.011110 0.012140 [1026048/1237259]
loss: 0.012677 0.011925 [1230848/1237259]
Epoch 647
-------------------------------
loss: 0.009558 0.012102 [ 2048/1237259]
loss: 0.018273 0.012042 [206848/1237259]
loss: 0.013698 0.011975 [411648/1237259]
loss: 0.010673 0.012041 [616448/1237259]
loss: 0.017946 0.012120 [821248/1237259]
loss: 0.017321 0.012020 [1026048/1237259]
loss: 0.017076 0.011974 [1230848/1237259]
Epoch 648
-------------------------------
loss: 0.013812 0.012090 [ 2048/1237259]
loss: 0.016870 0.012088 [206848/1237259]
loss: 0.012303 0.012113 [411648/1237259]
loss: 0.015499 0.012011 [616448/1237259]
loss: 0.011735 0.012035 [821248/1237259]
loss: 0.014108 0.012002 [1026048/1237259]
loss: 0.015677 0.012051 [1230848/1237259]
Epoch 649
-------------------------------
loss: 0.017691 0.012132 [ 2048/1237259]
loss: 0.015677 0.012097 [206848/1237259]
loss: 0.013039 0.011887 [411648/1237259]
loss: 0.012281 0.012040 [616448/1237259]
loss: 0.012494 0.011880 [821248/1237259]
loss: 0.016889 0.011949 [1026048/1237259]
loss: 0.010635 0.012131 [1230848/1237259]
Epoch 650
-------------------------------
loss: 0.016832 0.011943 [ 2048/1237259]
loss: 0.010703 0.012051 [206848/1237259]
loss: 0.013715 0.012073 [411648/1237259]
loss: 0.016119 0.012108 [616448/1237259]
loss: 0.013736 0.011988 [821248/1237259]
loss: 0.014054 0.012070 [1026048/1237259]
loss: 0.011492 0.011970 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0531  
diversity: 0.1957  


Epoch 651
-------------------------------
loss: 0.013519 0.012070 [ 2048/1237259]
loss: 0.012906 0.011962 [206848/1237259]
loss: 0.012793 0.012090 [411648/1237259]
loss: 0.014526 0.012133 [616448/1237259]
loss: 0.015105 0.012050 [821248/1237259]
loss: 0.012003 0.012002 [1026048/1237259]
loss: 0.017059 0.012026 [1230848/1237259]
Epoch 652
-------------------------------
loss: 0.012856 0.012143 [ 2048/1237259]
loss: 0.014051 0.012012 [206848/1237259]
loss: 0.012975 0.012082 [411648/1237259]
loss: 0.015093 0.012088 [616448/1237259]
loss: 0.014758 0.011963 [821248/1237259]
loss: 0.013451 0.012151 [1026048/1237259]
loss: 0.014107 0.012143 [1230848/1237259]
Epoch 653
-------------------------------
loss: 0.011792 0.011952 [ 2048/1237259]
loss: 0.012734 0.012023 [206848/1237259]
loss: 0.022132 0.012050 [411648/1237259]
loss: 0.018096 0.012119 [616448/1237259]
loss: 0.015290 0.012062 [821248/1237259]
loss: 0.012271 0.011953 [1026048/1237259]
loss: 0.011983 0.012012 [1230848/1237259]
Epoch 654
-------------------------------
loss: 0.012552 0.011957 [ 2048/1237259]
loss: 0.011478 0.012009 [206848/1237259]
loss: 0.012234 0.012051 [411648/1237259]
loss: 0.015320 0.012022 [616448/1237259]
loss: 0.014738 0.012056 [821248/1237259]
loss: 0.009603 0.012057 [1026048/1237259]
loss: 0.013150 0.011979 [1230848/1237259]
Epoch 655
-------------------------------
loss: 0.014428 0.012001 [ 2048/1237259]
loss: 0.018312 0.012153 [206848/1237259]
loss: 0.012836 0.012052 [411648/1237259]
loss: 0.014035 0.011969 [616448/1237259]
loss: 0.011771 0.012005 [821248/1237259]
loss: 0.013150 0.012167 [1026048/1237259]
loss: 0.015286 0.012017 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0532  
diversity: 0.1958  


Epoch 656
-------------------------------
loss: 0.009380 0.012116 [ 2048/1237259]
loss: 0.013243 0.012181 [206848/1237259]
loss: 0.011675 0.012218 [411648/1237259]
loss: 0.015310 0.012163 [616448/1237259]
loss: 0.012796 0.012164 [821248/1237259]
loss: 0.012882 0.012047 [1026048/1237259]
loss: 0.016026 0.011930 [1230848/1237259]
Epoch 657
-------------------------------
loss: 0.015232 0.012025 [ 2048/1237259]
loss: 0.018027 0.012138 [206848/1237259]
loss: 0.013179 0.012119 [411648/1237259]
loss: 0.013694 0.012083 [616448/1237259]
loss: 0.015879 0.012160 [821248/1237259]
loss: 0.013762 0.011921 [1026048/1237259]
loss: 0.013561 0.012114 [1230848/1237259]
Epoch 658
-------------------------------
loss: 0.014228 0.012019 [ 2048/1237259]
loss: 0.017095 0.012024 [206848/1237259]
loss: 0.013566 0.012061 [411648/1237259]
loss: 0.011717 0.012050 [616448/1237259]
loss: 0.014736 0.012039 [821248/1237259]
loss: 0.015841 0.012082 [1026048/1237259]
loss: 0.014471 0.012193 [1230848/1237259]
Epoch 659
-------------------------------
loss: 0.015881 0.012208 [ 2048/1237259]
loss: 0.014481 0.012035 [206848/1237259]
loss: 0.009453 0.012025 [411648/1237259]
loss: 0.013559 0.011956 [616448/1237259]
loss: 0.016387 0.012068 [821248/1237259]
loss: 0.012699 0.012103 [1026048/1237259]
loss: 0.010448 0.011865 [1230848/1237259]
Epoch 660
-------------------------------
loss: 0.015519 0.011931 [ 2048/1237259]
loss: 0.012507 0.012032 [206848/1237259]
loss: 0.017615 0.012140 [411648/1237259]
loss: 0.010803 0.012022 [616448/1237259]
loss: 0.010967 0.011998 [821248/1237259]
loss: 0.013535 0.012143 [1026048/1237259]
loss: 0.014103 0.012162 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0532  
diversity: 0.1958  


Epoch 661
-------------------------------
loss: 0.009537 0.012143 [ 2048/1237259]
loss: 0.021727 0.012032 [206848/1237259]
loss: 0.010348 0.012135 [411648/1237259]
loss: 0.011446 0.012094 [616448/1237259]
loss: 0.008952 0.012076 [821248/1237259]
loss: 0.019330 0.012003 [1026048/1237259]
loss: 0.012436 0.011990 [1230848/1237259]
Epoch 662
-------------------------------
loss: 0.012263 0.012090 [ 2048/1237259]
loss: 0.016293 0.012081 [206848/1237259]
loss: 0.008539 0.011994 [411648/1237259]
loss: 0.013232 0.012037 [616448/1237259]
loss: 0.015725 0.012168 [821248/1237259]
loss: 0.014812 0.011975 [1026048/1237259]
loss: 0.015036 0.012039 [1230848/1237259]
Epoch 663
-------------------------------
loss: 0.011429 0.012115 [ 2048/1237259]
loss: 0.017330 0.012143 [206848/1237259]
loss: 0.012947 0.012150 [411648/1237259]
loss: 0.014248 0.012021 [616448/1237259]
loss: 0.015366 0.012122 [821248/1237259]
loss: 0.014933 0.012143 [1026048/1237259]
loss: 0.012794 0.012221 [1230848/1237259]
Epoch 664
-------------------------------
loss: 0.013845 0.011989 [ 2048/1237259]
loss: 0.014055 0.012163 [206848/1237259]
loss: 0.013896 0.012098 [411648/1237259]
loss: 0.012934 0.011951 [616448/1237259]
loss: 0.012764 0.012031 [821248/1237259]
loss: 0.019279 0.012000 [1026048/1237259]
loss: 0.014899 0.012040 [1230848/1237259]
Epoch 665
-------------------------------
loss: 0.013455 0.011971 [ 2048/1237259]
loss: 0.016952 0.012014 [206848/1237259]
loss: 0.018568 0.012103 [411648/1237259]
loss: 0.016815 0.012031 [616448/1237259]
loss: 0.012256 0.012103 [821248/1237259]
loss: 0.015764 0.012121 [1026048/1237259]
loss: 0.015782 0.012072 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0653  
ndcg@20: 0.0534  
diversity: 0.1957  


Epoch 666
-------------------------------
loss: 0.013952 0.012122 [ 2048/1237259]
loss: 0.012401 0.012170 [206848/1237259]
loss: 0.014724 0.012077 [411648/1237259]
loss: 0.010199 0.012154 [616448/1237259]
loss: 0.016664 0.011997 [821248/1237259]
loss: 0.012349 0.012117 [1026048/1237259]
loss: 0.012561 0.012185 [1230848/1237259]
Epoch 667
-------------------------------
loss: 0.013195 0.012145 [ 2048/1237259]
loss: 0.013135 0.012124 [206848/1237259]
loss: 0.013076 0.012085 [411648/1237259]
loss: 0.016073 0.011959 [616448/1237259]
loss: 0.011426 0.012081 [821248/1237259]
loss: 0.013952 0.012028 [1026048/1237259]
loss: 0.016613 0.012153 [1230848/1237259]
Epoch 668
-------------------------------
loss: 0.011437 0.012039 [ 2048/1237259]
loss: 0.010370 0.012177 [206848/1237259]
loss: 0.013841 0.012024 [411648/1237259]
loss: 0.013707 0.012120 [616448/1237259]
loss: 0.016056 0.012134 [821248/1237259]
loss: 0.017583 0.012002 [1026048/1237259]
loss: 0.016834 0.012112 [1230848/1237259]
Epoch 669
-------------------------------
loss: 0.016019 0.012122 [ 2048/1237259]
loss: 0.012333 0.012060 [206848/1237259]
loss: 0.015365 0.012054 [411648/1237259]
loss: 0.009658 0.012020 [616448/1237259]
loss: 0.014711 0.011987 [821248/1237259]
loss: 0.016300 0.012102 [1026048/1237259]
loss: 0.016973 0.012038 [1230848/1237259]
Epoch 670
-------------------------------
loss: 0.010951 0.012110 [ 2048/1237259]
loss: 0.014120 0.012062 [206848/1237259]
loss: 0.014595 0.012147 [411648/1237259]
loss: 0.011503 0.012121 [616448/1237259]
loss: 0.011909 0.011972 [821248/1237259]
loss: 0.013591 0.012153 [1026048/1237259]
loss: 0.018316 0.012139 [1230848/1237259]
Eval results: 
recall@20: 0.0652  
ndcg@20: 0.0533  
diversity: 0.1960  


Epoch 671
-------------------------------
loss: 0.011221 0.012121 [ 2048/1237259]
loss: 0.011731 0.012106 [206848/1237259]
loss: 0.012293 0.012118 [411648/1237259]
loss: 0.015152 0.012008 [616448/1237259]
loss: 0.014564 0.012094 [821248/1237259]
loss: 0.011161 0.012079 [1026048/1237259]
loss: 0.013080 0.012178 [1230848/1237259]
Epoch 672
-------------------------------
loss: 0.012089 0.012126 [ 2048/1237259]
loss: 0.009272 0.012174 [206848/1237259]
loss: 0.012840 0.012060 [411648/1237259]
loss: 0.016255 0.012103 [616448/1237259]
loss: 0.009815 0.012035 [821248/1237259]
loss: 0.010433 0.012074 [1026048/1237259]
loss: 0.017347 0.012092 [1230848/1237259]
Epoch 673
-------------------------------
loss: 0.014076 0.012133 [ 2048/1237259]
loss: 0.012616 0.012132 [206848/1237259]
loss: 0.017346 0.012062 [411648/1237259]
loss: 0.011062 0.012208 [616448/1237259]
loss: 0.012144 0.012033 [821248/1237259]
loss: 0.016450 0.012126 [1026048/1237259]
loss: 0.012819 0.012138 [1230848/1237259]
Epoch 674
-------------------------------
loss: 0.012083 0.012015 [ 2048/1237259]
loss: 0.013202 0.012109 [206848/1237259]
loss: 0.014066 0.012235 [411648/1237259]
loss: 0.012430 0.012028 [616448/1237259]
loss: 0.011397 0.012232 [821248/1237259]
loss: 0.011573 0.012276 [1026048/1237259]
loss: 0.014051 0.012219 [1230848/1237259]
Epoch 675
-------------------------------
loss: 0.016719 0.012229 [ 2048/1237259]
loss: 0.015537 0.012103 [206848/1237259]
loss: 0.014759 0.012158 [411648/1237259]
loss: 0.014194 0.012117 [616448/1237259]
loss: 0.015902 0.012135 [821248/1237259]
loss: 0.009316 0.012149 [1026048/1237259]
loss: 0.016590 0.012199 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0532  
diversity: 0.1957  


Epoch 676
-------------------------------
loss: 0.014953 0.011965 [ 2048/1237259]
loss: 0.015084 0.012052 [206848/1237259]
loss: 0.011585 0.012182 [411648/1237259]
loss: 0.013008 0.012151 [616448/1237259]
loss: 0.015422 0.012121 [821248/1237259]
loss: 0.016672 0.012119 [1026048/1237259]
loss: 0.011798 0.012118 [1230848/1237259]
Epoch 677
-------------------------------
loss: 0.011395 0.012155 [ 2048/1237259]
loss: 0.015443 0.012098 [206848/1237259]
loss: 0.012335 0.012214 [411648/1237259]
loss: 0.014285 0.012314 [616448/1237259]
loss: 0.011438 0.012233 [821248/1237259]
loss: 0.015714 0.012071 [1026048/1237259]
loss: 0.012417 0.012204 [1230848/1237259]
Epoch 678
-------------------------------
loss: 0.016306 0.012077 [ 2048/1237259]
loss: 0.015639 0.012073 [206848/1237259]
loss: 0.019381 0.012139 [411648/1237259]
loss: 0.013146 0.012067 [616448/1237259]
loss: 0.012492 0.012247 [821248/1237259]
loss: 0.016630 0.012110 [1026048/1237259]
loss: 0.010673 0.012184 [1230848/1237259]
Epoch 679
-------------------------------
loss: 0.020248 0.012132 [ 2048/1237259]
loss: 0.013773 0.012231 [206848/1237259]
loss: 0.011064 0.012116 [411648/1237259]
loss: 0.012517 0.012226 [616448/1237259]
loss: 0.013708 0.012117 [821248/1237259]
loss: 0.013955 0.012217 [1026048/1237259]
loss: 0.011757 0.012238 [1230848/1237259]
Epoch 680
-------------------------------
loss: 0.009648 0.012095 [ 2048/1237259]
loss: 0.013661 0.012091 [206848/1237259]
loss: 0.018540 0.012194 [411648/1237259]
loss: 0.015848 0.012210 [616448/1237259]
loss: 0.014960 0.012109 [821248/1237259]
loss: 0.011771 0.012160 [1026048/1237259]
loss: 0.010016 0.012160 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0532  
diversity: 0.1958  


Epoch 681
-------------------------------
loss: 0.008833 0.012131 [ 2048/1237259]
loss: 0.018729 0.012213 [206848/1237259]
loss: 0.012583 0.012179 [411648/1237259]
loss: 0.014990 0.012166 [616448/1237259]
loss: 0.012340 0.011963 [821248/1237259]
loss: 0.012560 0.012070 [1026048/1237259]
loss: 0.015013 0.012019 [1230848/1237259]
Epoch 682
-------------------------------
loss: 0.013250 0.012088 [ 2048/1237259]
loss: 0.017590 0.012195 [206848/1237259]
loss: 0.012772 0.012115 [411648/1237259]
loss: 0.014659 0.012082 [616448/1237259]
loss: 0.012685 0.012230 [821248/1237259]
loss: 0.014050 0.012026 [1026048/1237259]
loss: 0.013781 0.011968 [1230848/1237259]
Epoch 683
-------------------------------
loss: 0.013515 0.012053 [ 2048/1237259]
loss: 0.013639 0.012125 [206848/1237259]
loss: 0.008387 0.012119 [411648/1237259]
loss: 0.012368 0.012079 [616448/1237259]
loss: 0.013989 0.012096 [821248/1237259]
loss: 0.010560 0.012054 [1026048/1237259]
loss: 0.017583 0.012214 [1230848/1237259]
Epoch 684
-------------------------------
loss: 0.012154 0.012151 [ 2048/1237259]
loss: 0.014169 0.012239 [206848/1237259]
loss: 0.017431 0.012114 [411648/1237259]
loss: 0.010567 0.012164 [616448/1237259]
loss: 0.015850 0.012253 [821248/1237259]
loss: 0.012028 0.012192 [1026048/1237259]
loss: 0.011060 0.012049 [1230848/1237259]
Epoch 685
-------------------------------
loss: 0.013945 0.012255 [ 2048/1237259]
loss: 0.013058 0.012088 [206848/1237259]
loss: 0.015335 0.012086 [411648/1237259]
loss: 0.015064 0.012217 [616448/1237259]
loss: 0.014459 0.012040 [821248/1237259]
loss: 0.011904 0.012052 [1026048/1237259]
loss: 0.013360 0.012199 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0532  
diversity: 0.1960  


Epoch 686
-------------------------------
loss: 0.016004 0.012103 [ 2048/1237259]
loss: 0.013438 0.012137 [206848/1237259]
loss: 0.012295 0.012111 [411648/1237259]
loss: 0.012639 0.012261 [616448/1237259]
loss: 0.015635 0.012208 [821248/1237259]
loss: 0.014295 0.012236 [1026048/1237259]
loss: 0.012928 0.012298 [1230848/1237259]
Epoch 687
-------------------------------
loss: 0.014482 0.012211 [ 2048/1237259]
loss: 0.013430 0.012051 [206848/1237259]
loss: 0.012469 0.012145 [411648/1237259]
loss: 0.016876 0.012106 [616448/1237259]
loss: 0.016010 0.012008 [821248/1237259]
loss: 0.013391 0.012261 [1026048/1237259]
loss: 0.011445 0.012037 [1230848/1237259]
Epoch 688
-------------------------------
loss: 0.012149 0.012158 [ 2048/1237259]
loss: 0.011590 0.012108 [206848/1237259]
loss: 0.015245 0.012048 [411648/1237259]
loss: 0.013965 0.012144 [616448/1237259]
loss: 0.014981 0.012102 [821248/1237259]
loss: 0.013374 0.012128 [1026048/1237259]
loss: 0.014642 0.012128 [1230848/1237259]
Epoch 689
-------------------------------
loss: 0.012318 0.012179 [ 2048/1237259]
loss: 0.012621 0.012265 [206848/1237259]
loss: 0.011617 0.012086 [411648/1237259]
loss: 0.013149 0.012146 [616448/1237259]
loss: 0.012426 0.012060 [821248/1237259]
loss: 0.012106 0.012198 [1026048/1237259]
loss: 0.014588 0.012026 [1230848/1237259]
Epoch 690
-------------------------------
loss: 0.015291 0.012249 [ 2048/1237259]
loss: 0.011028 0.012191 [206848/1237259]
loss: 0.009653 0.012112 [411648/1237259]
loss: 0.014937 0.012131 [616448/1237259]
loss: 0.009846 0.012104 [821248/1237259]
loss: 0.009785 0.012244 [1026048/1237259]
loss: 0.010712 0.012285 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0533  
diversity: 0.1959  


Epoch 691
-------------------------------
loss: 0.012111 0.012117 [ 2048/1237259]
loss: 0.015835 0.012124 [206848/1237259]
loss: 0.014003 0.012100 [411648/1237259]
loss: 0.015359 0.012225 [616448/1237259]
loss: 0.016718 0.012112 [821248/1237259]
loss: 0.015693 0.012148 [1026048/1237259]
loss: 0.016252 0.012184 [1230848/1237259]
Epoch 692
-------------------------------
loss: 0.015882 0.012057 [ 2048/1237259]
loss: 0.013580 0.012213 [206848/1237259]
loss: 0.012510 0.012225 [411648/1237259]
loss: 0.015340 0.012139 [616448/1237259]
loss: 0.014794 0.012200 [821248/1237259]
loss: 0.008719 0.012123 [1026048/1237259]
loss: 0.019281 0.012147 [1230848/1237259]
Epoch 693
-------------------------------
loss: 0.011337 0.012217 [ 2048/1237259]
loss: 0.013488 0.012105 [206848/1237259]
loss: 0.012572 0.012150 [411648/1237259]
loss: 0.013459 0.012239 [616448/1237259]
loss: 0.016654 0.012233 [821248/1237259]
loss: 0.011644 0.012216 [1026048/1237259]
loss: 0.016677 0.012195 [1230848/1237259]
Epoch 694
-------------------------------
loss: 0.013309 0.012074 [ 2048/1237259]
loss: 0.017569 0.012192 [206848/1237259]
loss: 0.009540 0.012107 [411648/1237259]
loss: 0.019783 0.012200 [616448/1237259]
loss: 0.012112 0.012265 [821248/1237259]
loss: 0.015206 0.012109 [1026048/1237259]
loss: 0.017840 0.012143 [1230848/1237259]
Epoch 695
-------------------------------
loss: 0.017623 0.012088 [ 2048/1237259]
loss: 0.014477 0.012175 [206848/1237259]
loss: 0.013597 0.012170 [411648/1237259]
loss: 0.014840 0.012125 [616448/1237259]
loss: 0.014524 0.012114 [821248/1237259]
loss: 0.012577 0.012217 [1026048/1237259]
loss: 0.012925 0.012181 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0533  
diversity: 0.1958  


Epoch 696
-------------------------------
loss: 0.012592 0.012118 [ 2048/1237259]
loss: 0.015416 0.012069 [206848/1237259]
loss: 0.012762 0.012181 [411648/1237259]
loss: 0.015752 0.012222 [616448/1237259]
loss: 0.012423 0.012126 [821248/1237259]
loss: 0.012002 0.012258 [1026048/1237259]
loss: 0.011479 0.012123 [1230848/1237259]
Epoch 697
-------------------------------
loss: 0.012580 0.012285 [ 2048/1237259]
loss: 0.016710 0.012234 [206848/1237259]
loss: 0.010933 0.012279 [411648/1237259]
loss: 0.014942 0.012180 [616448/1237259]
loss: 0.015668 0.012163 [821248/1237259]
loss: 0.012039 0.012117 [1026048/1237259]
loss: 0.010900 0.012072 [1230848/1237259]
Epoch 698
-------------------------------
loss: 0.009547 0.012089 [ 2048/1237259]
loss: 0.013641 0.012132 [206848/1237259]
loss: 0.012805 0.012204 [411648/1237259]
loss: 0.011803 0.012243 [616448/1237259]
loss: 0.017008 0.012172 [821248/1237259]
loss: 0.010609 0.012226 [1026048/1237259]
loss: 0.015399 0.012084 [1230848/1237259]
Epoch 699
-------------------------------
loss: 0.014811 0.012099 [ 2048/1237259]
loss: 0.013852 0.012135 [206848/1237259]
loss: 0.014421 0.012196 [411648/1237259]
loss: 0.014463 0.012187 [616448/1237259]
loss: 0.013738 0.012183 [821248/1237259]
loss: 0.011324 0.011985 [1026048/1237259]
loss: 0.011522 0.012208 [1230848/1237259]
Epoch 700
-------------------------------
loss: 0.021962 0.012239 [ 2048/1237259]
loss: 0.010733 0.012226 [206848/1237259]
loss: 0.011824 0.012156 [411648/1237259]
loss: 0.016513 0.012110 [616448/1237259]
loss: 0.012154 0.012158 [821248/1237259]
loss: 0.013187 0.012237 [1026048/1237259]
loss: 0.018410 0.012131 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0532  
diversity: 0.1959  


Epoch 701
-------------------------------
loss: 0.010642 0.012238 [ 2048/1237259]
loss: 0.012105 0.012158 [206848/1237259]
loss: 0.017746 0.012186 [411648/1237259]
loss: 0.011833 0.012221 [616448/1237259]
loss: 0.012359 0.012123 [821248/1237259]
loss: 0.015993 0.012225 [1026048/1237259]
loss: 0.014431 0.012239 [1230848/1237259]
Epoch 702
-------------------------------
loss: 0.015329 0.012138 [ 2048/1237259]
loss: 0.015617 0.012151 [206848/1237259]
loss: 0.018234 0.012166 [411648/1237259]
loss: 0.011046 0.012088 [616448/1237259]
loss: 0.015618 0.012170 [821248/1237259]
loss: 0.018111 0.012147 [1026048/1237259]
loss: 0.011390 0.012188 [1230848/1237259]
Epoch 703
-------------------------------
loss: 0.016372 0.012206 [ 2048/1237259]
loss: 0.013956 0.012244 [206848/1237259]
loss: 0.014320 0.012260 [411648/1237259]
loss: 0.011237 0.012123 [616448/1237259]
loss: 0.014450 0.012266 [821248/1237259]
loss: 0.015736 0.012302 [1026048/1237259]
loss: 0.010312 0.012115 [1230848/1237259]
Epoch 704
-------------------------------
loss: 0.016179 0.012240 [ 2048/1237259]
loss: 0.017207 0.012063 [206848/1237259]
loss: 0.012865 0.012219 [411648/1237259]
loss: 0.014329 0.012170 [616448/1237259]
loss: 0.017980 0.012145 [821248/1237259]
loss: 0.010580 0.012057 [1026048/1237259]
loss: 0.013406 0.012101 [1230848/1237259]
Epoch 705
-------------------------------
loss: 0.015766 0.012168 [ 2048/1237259]
loss: 0.015496 0.012185 [206848/1237259]
loss: 0.013692 0.012257 [411648/1237259]
loss: 0.015429 0.012128 [616448/1237259]
loss: 0.019534 0.012092 [821248/1237259]
loss: 0.011652 0.012130 [1026048/1237259]
loss: 0.019580 0.012153 [1230848/1237259]
Eval results: 
recall@20: 0.0647  
ndcg@20: 0.0532  
diversity: 0.1959  


Epoch 706
-------------------------------
loss: 0.015628 0.012240 [ 2048/1237259]
loss: 0.009679 0.012264 [206848/1237259]
loss: 0.013343 0.012171 [411648/1237259]
loss: 0.013596 0.012043 [616448/1237259]
loss: 0.015681 0.012205 [821248/1237259]
loss: 0.014206 0.012125 [1026048/1237259]
loss: 0.017452 0.012046 [1230848/1237259]
Epoch 707
-------------------------------
loss: 0.010866 0.012238 [ 2048/1237259]
loss: 0.014859 0.012275 [206848/1237259]
loss: 0.013556 0.012269 [411648/1237259]
loss: 0.007800 0.012111 [616448/1237259]
loss: 0.012234 0.012281 [821248/1237259]
loss: 0.012699 0.012229 [1026048/1237259]
loss: 0.015584 0.012188 [1230848/1237259]
Epoch 708
-------------------------------
loss: 0.020888 0.012171 [ 2048/1237259]
loss: 0.014156 0.012146 [206848/1237259]
loss: 0.015191 0.012218 [411648/1237259]
loss: 0.014474 0.012174 [616448/1237259]
loss: 0.013404 0.012304 [821248/1237259]
loss: 0.015048 0.012308 [1026048/1237259]
loss: 0.012016 0.012228 [1230848/1237259]
Epoch 709
-------------------------------
loss: 0.014686 0.012128 [ 2048/1237259]
loss: 0.014126 0.012211 [206848/1237259]
loss: 0.010346 0.012209 [411648/1237259]
loss: 0.017270 0.012223 [616448/1237259]
loss: 0.015662 0.012163 [821248/1237259]
loss: 0.011230 0.012141 [1026048/1237259]
loss: 0.013819 0.012129 [1230848/1237259]
Epoch 710
-------------------------------
loss: 0.015689 0.012348 [ 2048/1237259]
loss: 0.014281 0.012124 [206848/1237259]
loss: 0.010473 0.012188 [411648/1237259]
loss: 0.012859 0.012293 [616448/1237259]
loss: 0.015916 0.012245 [821248/1237259]
loss: 0.009658 0.012307 [1026048/1237259]
loss: 0.012543 0.012198 [1230848/1237259]
Eval results: 
recall@20: 0.0648  
ndcg@20: 0.0531  
diversity: 0.1958  


Epoch 711
-------------------------------
loss: 0.012358 0.012247 [ 2048/1237259]
loss: 0.015159 0.012221 [206848/1237259]
loss: 0.010326 0.012133 [411648/1237259]
loss: 0.010529 0.012264 [616448/1237259]
loss: 0.014757 0.012161 [821248/1237259]
loss: 0.013699 0.012227 [1026048/1237259]
loss: 0.015805 0.012185 [1230848/1237259]
Epoch 712
-------------------------------
loss: 0.014034 0.012139 [ 2048/1237259]
loss: 0.018743 0.012220 [206848/1237259]
loss: 0.013119 0.012253 [411648/1237259]
loss: 0.015307 0.012165 [616448/1237259]
loss: 0.012163 0.012255 [821248/1237259]
loss: 0.016467 0.012223 [1026048/1237259]
loss: 0.010525 0.012331 [1230848/1237259]
Epoch 713
-------------------------------
loss: 0.012313 0.012318 [ 2048/1237259]
loss: 0.011912 0.012138 [206848/1237259]
loss: 0.012983 0.012211 [411648/1237259]
loss: 0.013371 0.012201 [616448/1237259]
loss: 0.009983 0.012228 [821248/1237259]
loss: 0.009505 0.012184 [1026048/1237259]
loss: 0.012476 0.012357 [1230848/1237259]
Epoch 714
-------------------------------
loss: 0.013727 0.012288 [ 2048/1237259]
loss: 0.011788 0.012339 [206848/1237259]
loss: 0.014082 0.012282 [411648/1237259]
loss: 0.013904 0.012266 [616448/1237259]
loss: 0.012775 0.012232 [821248/1237259]
loss: 0.011466 0.012351 [1026048/1237259]
loss: 0.011240 0.012280 [1230848/1237259]
Epoch 715
-------------------------------
loss: 0.009887 0.012127 [ 2048/1237259]
loss: 0.013772 0.012208 [206848/1237259]
loss: 0.013843 0.012274 [411648/1237259]
loss: 0.012873 0.012219 [616448/1237259]
loss: 0.015654 0.012159 [821248/1237259]
loss: 0.015059 0.012271 [1026048/1237259]
loss: 0.014503 0.012357 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0532  
diversity: 0.1958  


Epoch 716
-------------------------------
loss: 0.017837 0.012290 [ 2048/1237259]
loss: 0.011895 0.012240 [206848/1237259]
loss: 0.014240 0.012201 [411648/1237259]
loss: 0.015027 0.012296 [616448/1237259]
loss: 0.012506 0.012175 [821248/1237259]
loss: 0.012270 0.012080 [1026048/1237259]
loss: 0.011513 0.012250 [1230848/1237259]
Epoch 717
-------------------------------
loss: 0.013857 0.012155 [ 2048/1237259]
loss: 0.015928 0.012220 [206848/1237259]
loss: 0.017630 0.012198 [411648/1237259]
loss: 0.015702 0.012171 [616448/1237259]
loss: 0.011216 0.012255 [821248/1237259]
loss: 0.010098 0.012244 [1026048/1237259]
loss: 0.018383 0.012200 [1230848/1237259]
Epoch 718
-------------------------------
loss: 0.015632 0.012196 [ 2048/1237259]
loss: 0.015154 0.012148 [206848/1237259]
loss: 0.014051 0.012040 [411648/1237259]
loss: 0.013322 0.012178 [616448/1237259]
loss: 0.013663 0.012189 [821248/1237259]
loss: 0.016416 0.012176 [1026048/1237259]
loss: 0.010887 0.012269 [1230848/1237259]
Epoch 719
-------------------------------
loss: 0.012314 0.012267 [ 2048/1237259]
loss: 0.012617 0.012364 [206848/1237259]
loss: 0.013004 0.012121 [411648/1237259]
loss: 0.013617 0.012312 [616448/1237259]
loss: 0.012461 0.012417 [821248/1237259]
loss: 0.011702 0.012144 [1026048/1237259]
loss: 0.011979 0.012257 [1230848/1237259]
Epoch 720
-------------------------------
loss: 0.012277 0.012089 [ 2048/1237259]
loss: 0.021377 0.012169 [206848/1237259]
loss: 0.015594 0.012197 [411648/1237259]
loss: 0.014272 0.012288 [616448/1237259]
loss: 0.017997 0.012297 [821248/1237259]
loss: 0.016364 0.012324 [1026048/1237259]
loss: 0.013952 0.012161 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0532  
diversity: 0.1959  


Epoch 721
-------------------------------
loss: 0.014107 0.012216 [ 2048/1237259]
loss: 0.010815 0.012392 [206848/1237259]
loss: 0.012023 0.012267 [411648/1237259]
loss: 0.012002 0.012391 [616448/1237259]
loss: 0.013075 0.012187 [821248/1237259]
loss: 0.013364 0.012148 [1026048/1237259]
loss: 0.010454 0.012303 [1230848/1237259]
Epoch 722
-------------------------------
loss: 0.014930 0.012101 [ 2048/1237259]
loss: 0.011623 0.012174 [206848/1237259]
loss: 0.013787 0.012099 [411648/1237259]
loss: 0.015761 0.012150 [616448/1237259]
loss: 0.014106 0.012221 [821248/1237259]
loss: 0.015366 0.012177 [1026048/1237259]
loss: 0.012849 0.012294 [1230848/1237259]
Epoch 723
-------------------------------
loss: 0.013189 0.012262 [ 2048/1237259]
loss: 0.013569 0.012231 [206848/1237259]
loss: 0.013724 0.012175 [411648/1237259]
loss: 0.012806 0.012213 [616448/1237259]
loss: 0.015412 0.012191 [821248/1237259]
loss: 0.017302 0.012077 [1026048/1237259]
loss: 0.011232 0.012218 [1230848/1237259]
Epoch 724
-------------------------------
loss: 0.015731 0.012184 [ 2048/1237259]
loss: 0.012796 0.012207 [206848/1237259]
loss: 0.015419 0.012287 [411648/1237259]
loss: 0.011919 0.012312 [616448/1237259]
loss: 0.015245 0.012199 [821248/1237259]
loss: 0.018718 0.012107 [1026048/1237259]
loss: 0.014811 0.012196 [1230848/1237259]
Epoch 725
-------------------------------
loss: 0.016665 0.012268 [ 2048/1237259]
loss: 0.011522 0.012404 [206848/1237259]
loss: 0.010305 0.012223 [411648/1237259]
loss: 0.009219 0.012145 [616448/1237259]
loss: 0.013710 0.012151 [821248/1237259]
loss: 0.016573 0.012274 [1026048/1237259]
loss: 0.012615 0.012168 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0532  
diversity: 0.1958  


Epoch 726
-------------------------------
loss: 0.014794 0.012322 [ 2048/1237259]
loss: 0.011788 0.012441 [206848/1237259]
loss: 0.014577 0.012217 [411648/1237259]
loss: 0.014830 0.012203 [616448/1237259]
loss: 0.016482 0.012055 [821248/1237259]
loss: 0.012503 0.012271 [1026048/1237259]
loss: 0.011402 0.012195 [1230848/1237259]
Epoch 727
-------------------------------
loss: 0.013745 0.012242 [ 2048/1237259]
loss: 0.013666 0.012170 [206848/1237259]
loss: 0.011290 0.012257 [411648/1237259]
loss: 0.011980 0.012221 [616448/1237259]
loss: 0.015252 0.012162 [821248/1237259]
loss: 0.011385 0.012287 [1026048/1237259]
loss: 0.009427 0.012255 [1230848/1237259]
Epoch 728
-------------------------------
loss: 0.015272 0.012348 [ 2048/1237259]
loss: 0.014700 0.012330 [206848/1237259]
loss: 0.013791 0.012256 [411648/1237259]
loss: 0.010663 0.012235 [616448/1237259]
loss: 0.015935 0.012242 [821248/1237259]
loss: 0.013838 0.012284 [1026048/1237259]
loss: 0.016145 0.012250 [1230848/1237259]
Epoch 729
-------------------------------
loss: 0.011428 0.012174 [ 2048/1237259]
loss: 0.012831 0.012247 [206848/1237259]
loss: 0.013134 0.012204 [411648/1237259]
loss: 0.013614 0.012231 [616448/1237259]
loss: 0.014888 0.012318 [821248/1237259]
loss: 0.015078 0.012243 [1026048/1237259]
loss: 0.018759 0.012374 [1230848/1237259]
Epoch 730
-------------------------------
loss: 0.015294 0.012190 [ 2048/1237259]
loss: 0.016965 0.012422 [206848/1237259]
loss: 0.011985 0.012212 [411648/1237259]
loss: 0.016616 0.012261 [616448/1237259]
loss: 0.012540 0.012268 [821248/1237259]
loss: 0.013385 0.012238 [1026048/1237259]
loss: 0.015536 0.012322 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0654  
ndcg@20: 0.0534  
diversity: 0.1959  


Epoch 731
-------------------------------
loss: 0.012569 0.012128 [ 2048/1237259]
loss: 0.011551 0.012191 [206848/1237259]
loss: 0.015134 0.012316 [411648/1237259]
loss: 0.011451 0.012373 [616448/1237259]
loss: 0.010411 0.012267 [821248/1237259]
loss: 0.013448 0.012258 [1026048/1237259]
loss: 0.014940 0.012157 [1230848/1237259]
Epoch 732
-------------------------------
loss: 0.015446 0.012227 [ 2048/1237259]
loss: 0.017069 0.012366 [206848/1237259]
loss: 0.013213 0.012406 [411648/1237259]
loss: 0.015958 0.012263 [616448/1237259]
loss: 0.014057 0.012280 [821248/1237259]
loss: 0.012947 0.012295 [1026048/1237259]
loss: 0.012929 0.012255 [1230848/1237259]
Epoch 733
-------------------------------
loss: 0.021625 0.012358 [ 2048/1237259]
loss: 0.015931 0.012244 [206848/1237259]
loss: 0.014057 0.012338 [411648/1237259]
loss: 0.015918 0.012142 [616448/1237259]
loss: 0.017701 0.012254 [821248/1237259]
loss: 0.015687 0.012333 [1026048/1237259]
loss: 0.015443 0.012229 [1230848/1237259]
Epoch 734
-------------------------------
loss: 0.015501 0.012352 [ 2048/1237259]
loss: 0.015098 0.012219 [206848/1237259]
loss: 0.012921 0.012302 [411648/1237259]
loss: 0.011560 0.012196 [616448/1237259]
loss: 0.016984 0.012314 [821248/1237259]
loss: 0.015217 0.012111 [1026048/1237259]
loss: 0.011918 0.012193 [1230848/1237259]
Epoch 735
-------------------------------
loss: 0.015674 0.012217 [ 2048/1237259]
loss: 0.009938 0.012270 [206848/1237259]
loss: 0.015983 0.012161 [411648/1237259]
loss: 0.016460 0.012281 [616448/1237259]
loss: 0.012274 0.012305 [821248/1237259]
loss: 0.011799 0.012284 [1026048/1237259]
loss: 0.014769 0.012271 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0531  
diversity: 0.1960  


Epoch 736
-------------------------------
loss: 0.017668 0.012344 [ 2048/1237259]
loss: 0.012749 0.012285 [206848/1237259]
loss: 0.013669 0.012142 [411648/1237259]
loss: 0.014671 0.012217 [616448/1237259]
loss: 0.011667 0.012320 [821248/1237259]
loss: 0.011282 0.012171 [1026048/1237259]
loss: 0.019486 0.012269 [1230848/1237259]
Epoch 737
-------------------------------
loss: 0.012243 0.012315 [ 2048/1237259]
loss: 0.013987 0.012188 [206848/1237259]
loss: 0.011351 0.012262 [411648/1237259]
loss: 0.011157 0.012272 [616448/1237259]
loss: 0.012219 0.012426 [821248/1237259]
loss: 0.012402 0.012450 [1026048/1237259]
loss: 0.016471 0.012333 [1230848/1237259]
Epoch 738
-------------------------------
loss: 0.012367 0.012192 [ 2048/1237259]
loss: 0.015691 0.012300 [206848/1237259]
loss: 0.014591 0.012346 [411648/1237259]
loss: 0.010922 0.012309 [616448/1237259]
loss: 0.011137 0.012349 [821248/1237259]
loss: 0.012610 0.012214 [1026048/1237259]
loss: 0.014556 0.012213 [1230848/1237259]
Epoch 739
-------------------------------
loss: 0.014926 0.012250 [ 2048/1237259]
loss: 0.014711 0.012283 [206848/1237259]
loss: 0.011970 0.012372 [411648/1237259]
loss: 0.016113 0.012255 [616448/1237259]
loss: 0.013414 0.012344 [821248/1237259]
loss: 0.012841 0.012360 [1026048/1237259]
loss: 0.013578 0.012344 [1230848/1237259]
Epoch 740
-------------------------------
loss: 0.013784 0.012275 [ 2048/1237259]
loss: 0.014097 0.012354 [206848/1237259]
loss: 0.015540 0.012372 [411648/1237259]
loss: 0.015267 0.012252 [616448/1237259]
loss: 0.014680 0.012331 [821248/1237259]
loss: 0.014443 0.012324 [1026048/1237259]
loss: 0.012360 0.012379 [1230848/1237259]
Eval results: 
recall@20: 0.0648  
ndcg@20: 0.0530  
diversity: 0.1958  


Epoch 741
-------------------------------
loss: 0.018893 0.012327 [ 2048/1237259]
loss: 0.016619 0.012296 [206848/1237259]
loss: 0.011151 0.012431 [411648/1237259]
loss: 0.016997 0.012341 [616448/1237259]
loss: 0.016392 0.012101 [821248/1237259]
loss: 0.014660 0.012259 [1026048/1237259]
loss: 0.012501 0.012262 [1230848/1237259]
Epoch 742
-------------------------------
loss: 0.012455 0.012204 [ 2048/1237259]
loss: 0.011125 0.012243 [206848/1237259]
loss: 0.016277 0.012166 [411648/1237259]
loss: 0.012734 0.012252 [616448/1237259]
loss: 0.009604 0.012391 [821248/1237259]
loss: 0.014058 0.012456 [1026048/1237259]
loss: 0.010566 0.012246 [1230848/1237259]
Epoch 743
-------------------------------
loss: 0.011738 0.012288 [ 2048/1237259]
loss: 0.014048 0.012326 [206848/1237259]
loss: 0.011032 0.012313 [411648/1237259]
loss: 0.012299 0.012256 [616448/1237259]
loss: 0.014113 0.012397 [821248/1237259]
loss: 0.012772 0.012349 [1026048/1237259]
loss: 0.009856 0.012228 [1230848/1237259]
Epoch 744
-------------------------------
loss: 0.010942 0.012374 [ 2048/1237259]
loss: 0.016341 0.012321 [206848/1237259]
loss: 0.012158 0.012380 [411648/1237259]
loss: 0.014697 0.012384 [616448/1237259]
loss: 0.012290 0.012459 [821248/1237259]
loss: 0.017295 0.012322 [1026048/1237259]
loss: 0.016179 0.012357 [1230848/1237259]
Epoch 745
-------------------------------
loss: 0.017237 0.012353 [ 2048/1237259]
loss: 0.013288 0.012259 [206848/1237259]
loss: 0.013033 0.012259 [411648/1237259]
loss: 0.014371 0.012341 [616448/1237259]
loss: 0.012358 0.012216 [821248/1237259]
loss: 0.011379 0.012355 [1026048/1237259]
loss: 0.013071 0.012468 [1230848/1237259]
Eval results: 
recall@20: 0.0647  
ndcg@20: 0.0531  
diversity: 0.1957  


Epoch 746
-------------------------------
loss: 0.009115 0.012199 [ 2048/1237259]
loss: 0.013662 0.012362 [206848/1237259]
loss: 0.013994 0.012330 [411648/1237259]
loss: 0.016592 0.012322 [616448/1237259]
loss: 0.013472 0.012322 [821248/1237259]
loss: 0.014923 0.012364 [1026048/1237259]
loss: 0.012964 0.012337 [1230848/1237259]
Epoch 747
-------------------------------
loss: 0.011784 0.012347 [ 2048/1237259]
loss: 0.012479 0.012392 [206848/1237259]
loss: 0.018081 0.012318 [411648/1237259]
loss: 0.011204 0.012231 [616448/1237259]
loss: 0.011636 0.012384 [821248/1237259]
loss: 0.014375 0.012406 [1026048/1237259]
loss: 0.012854 0.012499 [1230848/1237259]
Epoch 748
-------------------------------
loss: 0.011148 0.012275 [ 2048/1237259]
loss: 0.014775 0.012146 [206848/1237259]
loss: 0.013971 0.012323 [411648/1237259]
loss: 0.010941 0.012322 [616448/1237259]
loss: 0.013632 0.012252 [821248/1237259]
loss: 0.011890 0.012254 [1026048/1237259]
loss: 0.012744 0.012302 [1230848/1237259]
Epoch 749
-------------------------------
loss: 0.010409 0.012317 [ 2048/1237259]
loss: 0.011796 0.012195 [206848/1237259]
loss: 0.014046 0.012361 [411648/1237259]
loss: 0.012208 0.012305 [616448/1237259]
loss: 0.009374 0.012255 [821248/1237259]
loss: 0.014263 0.012309 [1026048/1237259]
loss: 0.012823 0.012315 [1230848/1237259]
Epoch 750
-------------------------------
loss: 0.011435 0.012402 [ 2048/1237259]
loss: 0.011220 0.012263 [206848/1237259]
loss: 0.015526 0.012355 [411648/1237259]
loss: 0.013761 0.012372 [616448/1237259]
loss: 0.016833 0.012378 [821248/1237259]
loss: 0.013541 0.012360 [1026048/1237259]
loss: 0.014777 0.012358 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0533  
diversity: 0.1958  


Epoch 751
-------------------------------
loss: 0.013271 0.012317 [ 2048/1237259]
loss: 0.014024 0.012276 [206848/1237259]
loss: 0.015427 0.012248 [411648/1237259]
loss: 0.012026 0.012291 [616448/1237259]
loss: 0.011861 0.012321 [821248/1237259]
loss: 0.012764 0.012280 [1026048/1237259]
loss: 0.011885 0.012367 [1230848/1237259]
Epoch 752
-------------------------------
loss: 0.015850 0.012327 [ 2048/1237259]
loss: 0.013202 0.012500 [206848/1237259]
loss: 0.012639 0.012342 [411648/1237259]
loss: 0.013291 0.012278 [616448/1237259]
loss: 0.014840 0.012184 [821248/1237259]
loss: 0.012703 0.012383 [1026048/1237259]
loss: 0.013871 0.012255 [1230848/1237259]
Epoch 753
-------------------------------
loss: 0.012262 0.012329 [ 2048/1237259]
loss: 0.014386 0.012358 [206848/1237259]
loss: 0.013225 0.012315 [411648/1237259]
loss: 0.014438 0.012222 [616448/1237259]
loss: 0.012204 0.012207 [821248/1237259]
loss: 0.009248 0.012341 [1026048/1237259]
loss: 0.012141 0.012393 [1230848/1237259]
Epoch 754
-------------------------------
loss: 0.011283 0.012371 [ 2048/1237259]
loss: 0.017058 0.012257 [206848/1237259]
loss: 0.014270 0.012327 [411648/1237259]
loss: 0.011526 0.012316 [616448/1237259]
loss: 0.014371 0.012443 [821248/1237259]
loss: 0.010965 0.012264 [1026048/1237259]
loss: 0.012284 0.012306 [1230848/1237259]
Epoch 755
-------------------------------
loss: 0.009926 0.012421 [ 2048/1237259]
loss: 0.013494 0.012211 [206848/1237259]
loss: 0.012571 0.012184 [411648/1237259]
loss: 0.014765 0.012329 [616448/1237259]
loss: 0.010301 0.012222 [821248/1237259]
loss: 0.011348 0.012223 [1026048/1237259]
loss: 0.012105 0.012386 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0532  
diversity: 0.1959  


Epoch 756
-------------------------------
loss: 0.015834 0.012422 [ 2048/1237259]
loss: 0.010229 0.012432 [206848/1237259]
loss: 0.011379 0.012334 [411648/1237259]
loss: 0.013825 0.012404 [616448/1237259]
loss: 0.009068 0.012360 [821248/1237259]
loss: 0.011476 0.012306 [1026048/1237259]
loss: 0.013139 0.012228 [1230848/1237259]
Epoch 757
-------------------------------
loss: 0.017896 0.012419 [ 2048/1237259]
loss: 0.009338 0.012391 [206848/1237259]
loss: 0.011512 0.012363 [411648/1237259]
loss: 0.018346 0.012378 [616448/1237259]
loss: 0.019048 0.012180 [821248/1237259]
loss: 0.010693 0.012443 [1026048/1237259]
loss: 0.018522 0.012447 [1230848/1237259]
Epoch 758
-------------------------------
loss: 0.010207 0.012339 [ 2048/1237259]
loss: 0.014064 0.012262 [206848/1237259]
loss: 0.013960 0.012431 [411648/1237259]
loss: 0.017963 0.012451 [616448/1237259]
loss: 0.009754 0.012379 [821248/1237259]
loss: 0.016055 0.012250 [1026048/1237259]
loss: 0.014467 0.012316 [1230848/1237259]
Epoch 759
-------------------------------
loss: 0.015815 0.012215 [ 2048/1237259]
loss: 0.011690 0.012332 [206848/1237259]
loss: 0.014208 0.012422 [411648/1237259]
loss: 0.011419 0.012374 [616448/1237259]
loss: 0.012759 0.012228 [821248/1237259]
loss: 0.011798 0.012434 [1026048/1237259]
loss: 0.010302 0.012331 [1230848/1237259]
Epoch 760
-------------------------------
loss: 0.017352 0.012208 [ 2048/1237259]
loss: 0.010799 0.012488 [206848/1237259]
loss: 0.011672 0.012286 [411648/1237259]
loss: 0.015840 0.012353 [616448/1237259]
loss: 0.015011 0.012203 [821248/1237259]
loss: 0.011450 0.012283 [1026048/1237259]
loss: 0.011118 0.012353 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0532  
diversity: 0.1958  


Epoch 761
-------------------------------
loss: 0.012767 0.012298 [ 2048/1237259]
loss: 0.014741 0.012346 [206848/1237259]
loss: 0.017257 0.012312 [411648/1237259]
loss: 0.011502 0.012433 [616448/1237259]
loss: 0.013800 0.012365 [821248/1237259]
loss: 0.012269 0.012330 [1026048/1237259]
loss: 0.009665 0.012290 [1230848/1237259]
Epoch 762
-------------------------------
loss: 0.010376 0.012412 [ 2048/1237259]
loss: 0.011556 0.012297 [206848/1237259]
loss: 0.015433 0.012178 [411648/1237259]
loss: 0.014306 0.012324 [616448/1237259]
loss: 0.013236 0.012272 [821248/1237259]
loss: 0.011246 0.012420 [1026048/1237259]
loss: 0.013778 0.012351 [1230848/1237259]
Epoch 763
-------------------------------
loss: 0.024842 0.012394 [ 2048/1237259]
loss: 0.012980 0.012288 [206848/1237259]
loss: 0.015036 0.012343 [411648/1237259]
loss: 0.016210 0.012328 [616448/1237259]
loss: 0.013192 0.012356 [821248/1237259]
loss: 0.012912 0.012355 [1026048/1237259]
loss: 0.012511 0.012391 [1230848/1237259]
Epoch 764
-------------------------------
loss: 0.015639 0.012426 [ 2048/1237259]
loss: 0.012081 0.012273 [206848/1237259]
loss: 0.012473 0.012319 [411648/1237259]
loss: 0.010265 0.012370 [616448/1237259]
loss: 0.013617 0.012291 [821248/1237259]
loss: 0.013929 0.012336 [1026048/1237259]
loss: 0.013748 0.012430 [1230848/1237259]
Epoch 765
-------------------------------
loss: 0.012769 0.012318 [ 2048/1237259]
loss: 0.020206 0.012179 [206848/1237259]
loss: 0.012503 0.012318 [411648/1237259]
loss: 0.015195 0.012266 [616448/1237259]
loss: 0.010609 0.012376 [821248/1237259]
loss: 0.012996 0.012345 [1026048/1237259]
loss: 0.012229 0.012461 [1230848/1237259]
Eval results: 
recall@20: 0.0652  
ndcg@20: 0.0534  
diversity: 0.1960  


Epoch 766
-------------------------------
loss: 0.015157 0.012321 [ 2048/1237259]
loss: 0.013048 0.012422 [206848/1237259]
loss: 0.011143 0.012213 [411648/1237259]
loss: 0.011244 0.012384 [616448/1237259]
loss: 0.012177 0.012332 [821248/1237259]
loss: 0.015765 0.012351 [1026048/1237259]
loss: 0.009779 0.012279 [1230848/1237259]
Epoch 767
-------------------------------
loss: 0.010107 0.012427 [ 2048/1237259]
loss: 0.014325 0.012293 [206848/1237259]
loss: 0.011911 0.012304 [411648/1237259]
loss: 0.013606 0.012384 [616448/1237259]
loss: 0.018348 0.012474 [821248/1237259]
loss: 0.013866 0.012443 [1026048/1237259]
loss: 0.012776 0.012593 [1230848/1237259]
Epoch 768
-------------------------------
loss: 0.012579 0.012323 [ 2048/1237259]
loss: 0.013860 0.012378 [206848/1237259]
loss: 0.014376 0.012375 [411648/1237259]
loss: 0.014686 0.012305 [616448/1237259]
loss: 0.011736 0.012404 [821248/1237259]
loss: 0.012507 0.012354 [1026048/1237259]
loss: 0.011845 0.012300 [1230848/1237259]
Epoch 769
-------------------------------
loss: 0.009617 0.012377 [ 2048/1237259]
loss: 0.012878 0.012323 [206848/1237259]
loss: 0.018111 0.012359 [411648/1237259]
loss: 0.011835 0.012424 [616448/1237259]
loss: 0.015739 0.012309 [821248/1237259]
loss: 0.014697 0.012312 [1026048/1237259]
loss: 0.012747 0.012309 [1230848/1237259]
Epoch 770
-------------------------------
loss: 0.011485 0.012269 [ 2048/1237259]
loss: 0.015369 0.012263 [206848/1237259]
loss: 0.010524 0.012245 [411648/1237259]
loss: 0.015589 0.012337 [616448/1237259]
loss: 0.013837 0.012319 [821248/1237259]
loss: 0.012608 0.012270 [1026048/1237259]
loss: 0.015300 0.012365 [1230848/1237259]
Eval results: 
recall@20: 0.0652  
ndcg@20: 0.0533  
diversity: 0.1958  


Epoch 771
-------------------------------
loss: 0.009310 0.012295 [ 2048/1237259]
loss: 0.012070 0.012277 [206848/1237259]
loss: 0.012912 0.012366 [411648/1237259]
loss: 0.016887 0.012321 [616448/1237259]
loss: 0.013375 0.012397 [821248/1237259]
loss: 0.014090 0.012392 [1026048/1237259]
loss: 0.011904 0.012340 [1230848/1237259]
Epoch 772
-------------------------------
loss: 0.011961 0.012381 [ 2048/1237259]
loss: 0.011696 0.012438 [206848/1237259]
loss: 0.015959 0.012341 [411648/1237259]
loss: 0.014586 0.012268 [616448/1237259]
loss: 0.012354 0.012264 [821248/1237259]
loss: 0.013204 0.012409 [1026048/1237259]
loss: 0.012209 0.012376 [1230848/1237259]
Epoch 773
-------------------------------
loss: 0.012027 0.012401 [ 2048/1237259]
loss: 0.012996 0.012317 [206848/1237259]
loss: 0.011798 0.012323 [411648/1237259]
loss: 0.011689 0.012293 [616448/1237259]
loss: 0.015592 0.012424 [821248/1237259]
loss: 0.017174 0.012417 [1026048/1237259]
loss: 0.013018 0.012360 [1230848/1237259]
Epoch 774
-------------------------------
loss: 0.014264 0.012383 [ 2048/1237259]
loss: 0.012454 0.012464 [206848/1237259]
loss: 0.009650 0.012306 [411648/1237259]
loss: 0.011683 0.012335 [616448/1237259]
loss: 0.018230 0.012388 [821248/1237259]
loss: 0.010464 0.012339 [1026048/1237259]
loss: 0.013754 0.012380 [1230848/1237259]
Epoch 775
-------------------------------
loss: 0.014248 0.012404 [ 2048/1237259]
loss: 0.013459 0.012378 [206848/1237259]
loss: 0.015689 0.012326 [411648/1237259]
loss: 0.013887 0.012378 [616448/1237259]
loss: 0.013386 0.012475 [821248/1237259]
loss: 0.014750 0.012397 [1026048/1237259]
loss: 0.011131 0.012505 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0532  
diversity: 0.1960  


Epoch 776
-------------------------------
loss: 0.012674 0.012372 [ 2048/1237259]
loss: 0.017109 0.012420 [206848/1237259]
loss: 0.011450 0.012396 [411648/1237259]
loss: 0.015613 0.012388 [616448/1237259]
loss: 0.010043 0.012476 [821248/1237259]
loss: 0.012875 0.012391 [1026048/1237259]
loss: 0.014234 0.012384 [1230848/1237259]
Epoch 777
-------------------------------
loss: 0.017788 0.012366 [ 2048/1237259]
loss: 0.010992 0.012436 [206848/1237259]
loss: 0.010800 0.012344 [411648/1237259]
loss: 0.012478 0.012398 [616448/1237259]
loss: 0.014276 0.012384 [821248/1237259]
loss: 0.016405 0.012345 [1026048/1237259]
loss: 0.013297 0.012252 [1230848/1237259]
Epoch 778
-------------------------------
loss: 0.010915 0.012368 [ 2048/1237259]
loss: 0.013200 0.012490 [206848/1237259]
loss: 0.011482 0.012392 [411648/1237259]
loss: 0.015349 0.012387 [616448/1237259]
loss: 0.011630 0.012434 [821248/1237259]
loss: 0.014485 0.012453 [1026048/1237259]
loss: 0.015912 0.012362 [1230848/1237259]
Epoch 779
-------------------------------
loss: 0.011149 0.012390 [ 2048/1237259]
loss: 0.014611 0.012396 [206848/1237259]
loss: 0.015642 0.012389 [411648/1237259]
loss: 0.012286 0.012425 [616448/1237259]
loss: 0.014191 0.012472 [821248/1237259]
loss: 0.011357 0.012342 [1026048/1237259]
loss: 0.010910 0.012448 [1230848/1237259]
Epoch 780
-------------------------------
loss: 0.009944 0.012496 [ 2048/1237259]
loss: 0.009863 0.012340 [206848/1237259]
loss: 0.013269 0.012343 [411648/1237259]
loss: 0.011272 0.012340 [616448/1237259]
loss: 0.008292 0.012370 [821248/1237259]
loss: 0.017243 0.012369 [1026048/1237259]
loss: 0.011818 0.012444 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0531  
diversity: 0.1956  


Epoch 781
-------------------------------
loss: 0.015967 0.012363 [ 2048/1237259]
loss: 0.012470 0.012293 [206848/1237259]
loss: 0.012322 0.012401 [411648/1237259]
loss: 0.012780 0.012392 [616448/1237259]
loss: 0.015635 0.012234 [821248/1237259]
loss: 0.012915 0.012261 [1026048/1237259]
loss: 0.010453 0.012506 [1230848/1237259]
Epoch 782
-------------------------------
loss: 0.010025 0.012274 [ 2048/1237259]
loss: 0.013989 0.012463 [206848/1237259]
loss: 0.013403 0.012397 [411648/1237259]
loss: 0.012653 0.012341 [616448/1237259]
loss: 0.013174 0.012325 [821248/1237259]
loss: 0.011843 0.012352 [1026048/1237259]
loss: 0.010863 0.012246 [1230848/1237259]
Epoch 783
-------------------------------
loss: 0.010876 0.012360 [ 2048/1237259]
loss: 0.015309 0.012305 [206848/1237259]
loss: 0.014577 0.012303 [411648/1237259]
loss: 0.010370 0.012407 [616448/1237259]
loss: 0.017707 0.012439 [821248/1237259]
loss: 0.011811 0.012284 [1026048/1237259]
loss: 0.009725 0.012333 [1230848/1237259]
Epoch 784
-------------------------------
loss: 0.012834 0.012392 [ 2048/1237259]
loss: 0.010390 0.012421 [206848/1237259]
loss: 0.010321 0.012431 [411648/1237259]
loss: 0.010288 0.012438 [616448/1237259]
loss: 0.013254 0.012332 [821248/1237259]
loss: 0.013782 0.012287 [1026048/1237259]
loss: 0.014245 0.012312 [1230848/1237259]
Epoch 785
-------------------------------
loss: 0.014764 0.012305 [ 2048/1237259]
loss: 0.017697 0.012322 [206848/1237259]
loss: 0.013144 0.012468 [411648/1237259]
loss: 0.010363 0.012352 [616448/1237259]
loss: 0.011979 0.012274 [821248/1237259]
loss: 0.011210 0.012431 [1026048/1237259]
loss: 0.012405 0.012418 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0532  
diversity: 0.1958  


Epoch 786
-------------------------------
loss: 0.011535 0.012466 [ 2048/1237259]
loss: 0.012331 0.012317 [206848/1237259]
loss: 0.013833 0.012534 [411648/1237259]
loss: 0.014336 0.012416 [616448/1237259]
loss: 0.012009 0.012483 [821248/1237259]
loss: 0.013028 0.012349 [1026048/1237259]
loss: 0.011556 0.012406 [1230848/1237259]
Epoch 787
-------------------------------
loss: 0.014566 0.012358 [ 2048/1237259]
loss: 0.014813 0.012499 [206848/1237259]
loss: 0.013253 0.012305 [411648/1237259]
loss: 0.016529 0.012332 [616448/1237259]
loss: 0.019320 0.012450 [821248/1237259]
loss: 0.010459 0.012397 [1026048/1237259]
loss: 0.013655 0.012420 [1230848/1237259]
Epoch 788
-------------------------------
loss: 0.013414 0.012427 [ 2048/1237259]
loss: 0.016876 0.012332 [206848/1237259]
loss: 0.012652 0.012300 [411648/1237259]
loss: 0.013085 0.012346 [616448/1237259]
loss: 0.009876 0.012409 [821248/1237259]
loss: 0.010311 0.012412 [1026048/1237259]
loss: 0.012078 0.012401 [1230848/1237259]
Epoch 789
-------------------------------
loss: 0.011281 0.012320 [ 2048/1237259]
loss: 0.013681 0.012348 [206848/1237259]
loss: 0.013415 0.012478 [411648/1237259]
loss: 0.015877 0.012377 [616448/1237259]
loss: 0.013995 0.012341 [821248/1237259]
loss: 0.011313 0.012307 [1026048/1237259]
loss: 0.016558 0.012465 [1230848/1237259]
Epoch 790
-------------------------------
loss: 0.010309 0.012422 [ 2048/1237259]
loss: 0.009833 0.012482 [206848/1237259]
loss: 0.010774 0.012548 [411648/1237259]
loss: 0.011042 0.012335 [616448/1237259]
loss: 0.016113 0.012412 [821248/1237259]
loss: 0.011333 0.012411 [1026048/1237259]
loss: 0.009384 0.012523 [1230848/1237259]
Eval results: 
recall@20: 0.0652  
ndcg@20: 0.0532  
diversity: 0.1959  


Epoch 791
-------------------------------
loss: 0.013610 0.012385 [ 2048/1237259]
loss: 0.015701 0.012395 [206848/1237259]
loss: 0.016622 0.012484 [411648/1237259]
loss: 0.011567 0.012348 [616448/1237259]
loss: 0.009080 0.012462 [821248/1237259]
loss: 0.011236 0.012298 [1026048/1237259]
loss: 0.012857 0.012404 [1230848/1237259]
Epoch 792
-------------------------------
loss: 0.009235 0.012389 [ 2048/1237259]
loss: 0.013420 0.012235 [206848/1237259]
loss: 0.010206 0.012270 [411648/1237259]
loss: 0.011237 0.012343 [616448/1237259]
loss: 0.012749 0.012321 [821248/1237259]
loss: 0.012211 0.012408 [1026048/1237259]
loss: 0.015552 0.012392 [1230848/1237259]
Epoch 793
-------------------------------
loss: 0.012862 0.012376 [ 2048/1237259]
loss: 0.014614 0.012340 [206848/1237259]
loss: 0.013199 0.012482 [411648/1237259]
loss: 0.013616 0.012373 [616448/1237259]
loss: 0.011375 0.012381 [821248/1237259]
loss: 0.012075 0.012425 [1026048/1237259]
loss: 0.010831 0.012340 [1230848/1237259]
Epoch 794
-------------------------------
loss: 0.015438 0.012222 [ 2048/1237259]
loss: 0.009768 0.012372 [206848/1237259]
loss: 0.016168 0.012444 [411648/1237259]
loss: 0.015527 0.012446 [616448/1237259]
loss: 0.010652 0.012498 [821248/1237259]
loss: 0.014915 0.012507 [1026048/1237259]
loss: 0.014058 0.012464 [1230848/1237259]
Epoch 795
-------------------------------
loss: 0.011548 0.012305 [ 2048/1237259]
loss: 0.019207 0.012321 [206848/1237259]
loss: 0.014502 0.012491 [411648/1237259]
loss: 0.014498 0.012435 [616448/1237259]
loss: 0.015567 0.012295 [821248/1237259]
loss: 0.015415 0.012345 [1026048/1237259]
loss: 0.011109 0.012366 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0532  
diversity: 0.1960  


Epoch 796
-------------------------------
loss: 0.011447 0.012424 [ 2048/1237259]
loss: 0.010115 0.012463 [206848/1237259]
loss: 0.010866 0.012318 [411648/1237259]
loss: 0.011460 0.012446 [616448/1237259]
loss: 0.013200 0.012476 [821248/1237259]
loss: 0.010601 0.012409 [1026048/1237259]
loss: 0.016809 0.012447 [1230848/1237259]
Epoch 797
-------------------------------
loss: 0.012207 0.012312 [ 2048/1237259]
loss: 0.010367 0.012471 [206848/1237259]
loss: 0.013365 0.012428 [411648/1237259]
loss: 0.016877 0.012388 [616448/1237259]
loss: 0.013980 0.012460 [821248/1237259]
loss: 0.016427 0.012403 [1026048/1237259]
loss: 0.014372 0.012216 [1230848/1237259]
Epoch 798
-------------------------------
loss: 0.009212 0.012308 [ 2048/1237259]
loss: 0.012393 0.012404 [206848/1237259]
loss: 0.014015 0.012535 [411648/1237259]
loss: 0.011390 0.012329 [616448/1237259]
loss: 0.012955 0.012451 [821248/1237259]
loss: 0.010437 0.012361 [1026048/1237259]
loss: 0.012083 0.012452 [1230848/1237259]
Epoch 799
-------------------------------
loss: 0.013657 0.012227 [ 2048/1237259]
loss: 0.015776 0.012442 [206848/1237259]
loss: 0.011934 0.012403 [411648/1237259]
loss: 0.016146 0.012355 [616448/1237259]
loss: 0.011583 0.012385 [821248/1237259]
loss: 0.011610 0.012482 [1026048/1237259]
loss: 0.014719 0.012434 [1230848/1237259]
Epoch 800
-------------------------------
loss: 0.013091 0.012447 [ 2048/1237259]
loss: 0.011008 0.012401 [206848/1237259]
loss: 0.013040 0.012361 [411648/1237259]
loss: 0.009355 0.012487 [616448/1237259]
loss: 0.009956 0.012430 [821248/1237259]
loss: 0.010532 0.012262 [1026048/1237259]
loss: 0.015061 0.012430 [1230848/1237259]
Eval results: 
recall@20: 0.0652  
ndcg@20: 0.0533  
diversity: 0.1957  


Epoch 801
-------------------------------
loss: 0.017871 0.012432 [ 2048/1237259]
loss: 0.017062 0.012354 [206848/1237259]
loss: 0.011336 0.012494 [411648/1237259]
loss: 0.013254 0.012462 [616448/1237259]
loss: 0.013327 0.012535 [821248/1237259]
loss: 0.017601 0.012372 [1026048/1237259]
loss: 0.010532 0.012448 [1230848/1237259]
Epoch 802
-------------------------------
loss: 0.010571 0.012354 [ 2048/1237259]
loss: 0.014690 0.012487 [206848/1237259]
loss: 0.014709 0.012337 [411648/1237259]
loss: 0.011557 0.012407 [616448/1237259]
loss: 0.010327 0.012371 [821248/1237259]
loss: 0.013330 0.012429 [1026048/1237259]
loss: 0.010002 0.012466 [1230848/1237259]
Epoch 803
-------------------------------
loss: 0.014277 0.012371 [ 2048/1237259]
loss: 0.015195 0.012386 [206848/1237259]
loss: 0.012113 0.012401 [411648/1237259]
loss: 0.011028 0.012481 [616448/1237259]
loss: 0.012940 0.012434 [821248/1237259]
loss: 0.012408 0.012329 [1026048/1237259]
loss: 0.011148 0.012377 [1230848/1237259]
Epoch 804
-------------------------------
loss: 0.010055 0.012418 [ 2048/1237259]
loss: 0.011306 0.012383 [206848/1237259]
loss: 0.010750 0.012396 [411648/1237259]
loss: 0.016725 0.012524 [616448/1237259]
loss: 0.012609 0.012412 [821248/1237259]
loss: 0.011723 0.012364 [1026048/1237259]
loss: 0.017880 0.012460 [1230848/1237259]
Epoch 805
-------------------------------
loss: 0.013176 0.012288 [ 2048/1237259]
loss: 0.008139 0.012429 [206848/1237259]
loss: 0.014276 0.012433 [411648/1237259]
loss: 0.013058 0.012277 [616448/1237259]
loss: 0.013980 0.012458 [821248/1237259]
loss: 0.014641 0.012409 [1026048/1237259]
loss: 0.010707 0.012396 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0532  
diversity: 0.1957  


Epoch 806
-------------------------------
loss: 0.018654 0.012487 [ 2048/1237259]
loss: 0.016273 0.012323 [206848/1237259]
loss: 0.012701 0.012518 [411648/1237259]
loss: 0.014148 0.012491 [616448/1237259]
loss: 0.018670 0.012526 [821248/1237259]
loss: 0.012083 0.012376 [1026048/1237259]
loss: 0.015962 0.012479 [1230848/1237259]
Epoch 807
-------------------------------
loss: 0.014457 0.012324 [ 2048/1237259]
loss: 0.012634 0.012469 [206848/1237259]
loss: 0.011186 0.012461 [411648/1237259]
loss: 0.010874 0.012405 [616448/1237259]
loss: 0.010501 0.012456 [821248/1237259]
loss: 0.012820 0.012381 [1026048/1237259]
loss: 0.013457 0.012513 [1230848/1237259]
Epoch 808
-------------------------------
loss: 0.010647 0.012415 [ 2048/1237259]
loss: 0.016367 0.012295 [206848/1237259]
loss: 0.013376 0.012511 [411648/1237259]
loss: 0.007895 0.012422 [616448/1237259]
loss: 0.011962 0.012470 [821248/1237259]
loss: 0.015148 0.012408 [1026048/1237259]
loss: 0.012626 0.012416 [1230848/1237259]
Epoch 809
-------------------------------
loss: 0.012074 0.012522 [ 2048/1237259]
loss: 0.015042 0.012341 [206848/1237259]
loss: 0.012454 0.012514 [411648/1237259]
loss: 0.011106 0.012424 [616448/1237259]
loss: 0.013870 0.012457 [821248/1237259]
loss: 0.010231 0.012530 [1026048/1237259]
loss: 0.011842 0.012430 [1230848/1237259]
Epoch 810
-------------------------------
loss: 0.014602 0.012297 [ 2048/1237259]
loss: 0.016074 0.012542 [206848/1237259]
loss: 0.014168 0.012445 [411648/1237259]
loss: 0.008810 0.012514 [616448/1237259]
loss: 0.014130 0.012516 [821248/1237259]
loss: 0.012136 0.012378 [1026048/1237259]
loss: 0.012424 0.012450 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0532  
diversity: 0.1957  


Epoch 811
-------------------------------
loss: 0.013062 0.012627 [ 2048/1237259]
loss: 0.011513 0.012394 [206848/1237259]
loss: 0.009138 0.012471 [411648/1237259]
loss: 0.010881 0.012469 [616448/1237259]
loss: 0.015051 0.012419 [821248/1237259]
loss: 0.011413 0.012491 [1026048/1237259]
loss: 0.009278 0.012534 [1230848/1237259]
Epoch 812
-------------------------------
loss: 0.012840 0.012530 [ 2048/1237259]
loss: 0.011905 0.012425 [206848/1237259]
loss: 0.013445 0.012395 [411648/1237259]
loss: 0.014107 0.012474 [616448/1237259]
loss: 0.012893 0.012397 [821248/1237259]
loss: 0.012445 0.012384 [1026048/1237259]
loss: 0.013775 0.012451 [1230848/1237259]
Epoch 813
-------------------------------
loss: 0.014928 0.012498 [ 2048/1237259]
loss: 0.012301 0.012542 [206848/1237259]
loss: 0.009008 0.012597 [411648/1237259]
loss: 0.010825 0.012413 [616448/1237259]
loss: 0.014711 0.012334 [821248/1237259]
loss: 0.013362 0.012388 [1026048/1237259]
loss: 0.010890 0.012395 [1230848/1237259]
Epoch 814
-------------------------------
loss: 0.015675 0.012350 [ 2048/1237259]
loss: 0.011746 0.012511 [206848/1237259]
loss: 0.012756 0.012498 [411648/1237259]
loss: 0.012847 0.012392 [616448/1237259]
loss: 0.012256 0.012420 [821248/1237259]
loss: 0.009045 0.012521 [1026048/1237259]
loss: 0.014261 0.012515 [1230848/1237259]
Epoch 815
-------------------------------
loss: 0.010694 0.012426 [ 2048/1237259]
loss: 0.013413 0.012413 [206848/1237259]
loss: 0.015504 0.012553 [411648/1237259]
loss: 0.020398 0.012471 [616448/1237259]
loss: 0.011876 0.012360 [821248/1237259]
loss: 0.015087 0.012543 [1026048/1237259]
loss: 0.010279 0.012441 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0532  
diversity: 0.1958  


Epoch 816
-------------------------------
loss: 0.021237 0.012348 [ 2048/1237259]
loss: 0.009269 0.012418 [206848/1237259]
loss: 0.010636 0.012529 [411648/1237259]
loss: 0.009108 0.012389 [616448/1237259]
loss: 0.014483 0.012443 [821248/1237259]
loss: 0.011408 0.012468 [1026048/1237259]
loss: 0.014615 0.012518 [1230848/1237259]
Epoch 817
-------------------------------
loss: 0.011458 0.012469 [ 2048/1237259]
loss: 0.010437 0.012400 [206848/1237259]
loss: 0.008995 0.012482 [411648/1237259]
loss: 0.012409 0.012378 [616448/1237259]
loss: 0.012468 0.012395 [821248/1237259]
loss: 0.011788 0.012371 [1026048/1237259]
loss: 0.014951 0.012387 [1230848/1237259]
Epoch 818
-------------------------------
loss: 0.013749 0.012349 [ 2048/1237259]
loss: 0.011005 0.012393 [206848/1237259]
loss: 0.011484 0.012351 [411648/1237259]
loss: 0.013075 0.012473 [616448/1237259]
loss: 0.010330 0.012438 [821248/1237259]
loss: 0.009837 0.012408 [1026048/1237259]
loss: 0.012346 0.012444 [1230848/1237259]
Epoch 819
-------------------------------
loss: 0.015324 0.012527 [ 2048/1237259]
loss: 0.014687 0.012470 [206848/1237259]
loss: 0.012350 0.012456 [411648/1237259]
loss: 0.012906 0.012531 [616448/1237259]
loss: 0.013471 0.012492 [821248/1237259]
loss: 0.013473 0.012466 [1026048/1237259]
loss: 0.013602 0.012335 [1230848/1237259]
Epoch 820
-------------------------------
loss: 0.011727 0.012481 [ 2048/1237259]
loss: 0.011628 0.012435 [206848/1237259]
loss: 0.013094 0.012480 [411648/1237259]
loss: 0.012190 0.012588 [616448/1237259]
loss: 0.015776 0.012502 [821248/1237259]
loss: 0.010319 0.012459 [1026048/1237259]
loss: 0.012027 0.012549 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0532  
diversity: 0.1957  


Epoch 821
-------------------------------
loss: 0.013955 0.012401 [ 2048/1237259]
loss: 0.013577 0.012429 [206848/1237259]
loss: 0.016210 0.012445 [411648/1237259]
loss: 0.012456 0.012570 [616448/1237259]
loss: 0.010535 0.012474 [821248/1237259]
loss: 0.013060 0.012475 [1026048/1237259]
loss: 0.020985 0.012362 [1230848/1237259]
Epoch 822
-------------------------------
loss: 0.008366 0.012474 [ 2048/1237259]
loss: 0.014085 0.012617 [206848/1237259]
loss: 0.012727 0.012373 [411648/1237259]
loss: 0.014973 0.012519 [616448/1237259]
loss: 0.014676 0.012341 [821248/1237259]
loss: 0.018122 0.012531 [1026048/1237259]
loss: 0.011679 0.012543 [1230848/1237259]
Epoch 823
-------------------------------
loss: 0.012850 0.012477 [ 2048/1237259]
loss: 0.013530 0.012475 [206848/1237259]
loss: 0.013763 0.012505 [411648/1237259]
loss: 0.013936 0.012471 [616448/1237259]
loss: 0.012449 0.012415 [821248/1237259]
loss: 0.013503 0.012600 [1026048/1237259]
loss: 0.009920 0.012372 [1230848/1237259]
Epoch 824
-------------------------------
loss: 0.011714 0.012374 [ 2048/1237259]
loss: 0.011385 0.012538 [206848/1237259]
loss: 0.012293 0.012479 [411648/1237259]
loss: 0.011737 0.012525 [616448/1237259]
loss: 0.015572 0.012370 [821248/1237259]
loss: 0.013632 0.012359 [1026048/1237259]
loss: 0.012357 0.012515 [1230848/1237259]
Epoch 825
-------------------------------
loss: 0.011959 0.012491 [ 2048/1237259]
loss: 0.012096 0.012335 [206848/1237259]
loss: 0.016797 0.012438 [411648/1237259]
loss: 0.014993 0.012357 [616448/1237259]
loss: 0.012761 0.012449 [821248/1237259]
loss: 0.013970 0.012585 [1026048/1237259]
loss: 0.012039 0.012460 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0533  
diversity: 0.1957  


Epoch 826
-------------------------------
loss: 0.014839 0.012406 [ 2048/1237259]
loss: 0.015979 0.012358 [206848/1237259]
loss: 0.014575 0.012557 [411648/1237259]
loss: 0.009918 0.012316 [616448/1237259]
loss: 0.014229 0.012379 [821248/1237259]
loss: 0.011514 0.012533 [1026048/1237259]
loss: 0.015339 0.012480 [1230848/1237259]
Epoch 827
-------------------------------
loss: 0.008303 0.012339 [ 2048/1237259]
loss: 0.018421 0.012443 [206848/1237259]
loss: 0.012495 0.012426 [411648/1237259]
loss: 0.009869 0.012518 [616448/1237259]
loss: 0.011292 0.012525 [821248/1237259]
loss: 0.015135 0.012425 [1026048/1237259]
loss: 0.009148 0.012499 [1230848/1237259]
Epoch 828
-------------------------------
loss: 0.015712 0.012427 [ 2048/1237259]
loss: 0.011792 0.012363 [206848/1237259]
loss: 0.014945 0.012426 [411648/1237259]
loss: 0.009807 0.012343 [616448/1237259]
loss: 0.011576 0.012549 [821248/1237259]
loss: 0.015130 0.012463 [1026048/1237259]
loss: 0.012503 0.012440 [1230848/1237259]
Epoch 829
-------------------------------
loss: 0.009842 0.012481 [ 2048/1237259]
loss: 0.013267 0.012536 [206848/1237259]
loss: 0.009285 0.012413 [411648/1237259]
loss: 0.010174 0.012343 [616448/1237259]
loss: 0.010784 0.012511 [821248/1237259]
loss: 0.009624 0.012337 [1026048/1237259]
loss: 0.011711 0.012416 [1230848/1237259]
Epoch 830
-------------------------------
loss: 0.011264 0.012399 [ 2048/1237259]
loss: 0.015646 0.012538 [206848/1237259]
loss: 0.012352 0.012546 [411648/1237259]
loss: 0.013046 0.012428 [616448/1237259]
loss: 0.014265 0.012515 [821248/1237259]
loss: 0.011309 0.012419 [1026048/1237259]
loss: 0.011348 0.012513 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0533  
diversity: 0.1958  


Epoch 831
-------------------------------
loss: 0.009765 0.012420 [ 2048/1237259]
loss: 0.011793 0.012383 [206848/1237259]
loss: 0.015037 0.012397 [411648/1237259]
loss: 0.009183 0.012500 [616448/1237259]
loss: 0.012090 0.012385 [821248/1237259]
loss: 0.013383 0.012493 [1026048/1237259]
loss: 0.013500 0.012302 [1230848/1237259]
Epoch 832
-------------------------------
loss: 0.015310 0.012443 [ 2048/1237259]
loss: 0.011596 0.012387 [206848/1237259]
loss: 0.018271 0.012479 [411648/1237259]
loss: 0.009315 0.012607 [616448/1237259]
loss: 0.012306 0.012514 [821248/1237259]
loss: 0.013283 0.012508 [1026048/1237259]
loss: 0.012926 0.012474 [1230848/1237259]
Epoch 833
-------------------------------
loss: 0.015219 0.012460 [ 2048/1237259]
loss: 0.013834 0.012434 [206848/1237259]
loss: 0.014048 0.012610 [411648/1237259]
loss: 0.010322 0.012452 [616448/1237259]
loss: 0.015376 0.012504 [821248/1237259]
loss: 0.011696 0.012361 [1026048/1237259]
loss: 0.012274 0.012417 [1230848/1237259]
Epoch 834
-------------------------------
loss: 0.015763 0.012413 [ 2048/1237259]
loss: 0.009136 0.012479 [206848/1237259]
loss: 0.010185 0.012462 [411648/1237259]
loss: 0.011342 0.012370 [616448/1237259]
loss: 0.014150 0.012404 [821248/1237259]
loss: 0.012677 0.012490 [1026048/1237259]
loss: 0.012604 0.012496 [1230848/1237259]
Epoch 835
-------------------------------
loss: 0.011961 0.012455 [ 2048/1237259]
loss: 0.014794 0.012351 [206848/1237259]
loss: 0.011984 0.012427 [411648/1237259]
loss: 0.014682 0.012531 [616448/1237259]
loss: 0.012927 0.012509 [821248/1237259]
loss: 0.013296 0.012440 [1026048/1237259]
loss: 0.009074 0.012461 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0535  
diversity: 0.1961  


Epoch 836
-------------------------------
loss: 0.012933 0.012517 [ 2048/1237259]
loss: 0.014442 0.012486 [206848/1237259]
loss: 0.013444 0.012375 [411648/1237259]
loss: 0.010664 0.012441 [616448/1237259]
loss: 0.012460 0.012342 [821248/1237259]
loss: 0.012816 0.012469 [1026048/1237259]
loss: 0.013304 0.012483 [1230848/1237259]
Epoch 837
-------------------------------
loss: 0.009456 0.012445 [ 2048/1237259]
loss: 0.010990 0.012474 [206848/1237259]
loss: 0.012116 0.012417 [411648/1237259]
loss: 0.012302 0.012462 [616448/1237259]
loss: 0.009306 0.012383 [821248/1237259]
loss: 0.011948 0.012512 [1026048/1237259]
loss: 0.013688 0.012607 [1230848/1237259]
Epoch 838
-------------------------------
loss: 0.011852 0.012488 [ 2048/1237259]
loss: 0.013083 0.012476 [206848/1237259]
loss: 0.010521 0.012490 [411648/1237259]
loss: 0.011890 0.012476 [616448/1237259]
loss: 0.010488 0.012405 [821248/1237259]
loss: 0.014537 0.012516 [1026048/1237259]
loss: 0.010891 0.012509 [1230848/1237259]
Epoch 839
-------------------------------
loss: 0.014959 0.012541 [ 2048/1237259]
loss: 0.014894 0.012504 [206848/1237259]
loss: 0.009697 0.012384 [411648/1237259]
loss: 0.014722 0.012313 [616448/1237259]
loss: 0.010052 0.012565 [821248/1237259]
loss: 0.018224 0.012583 [1026048/1237259]
loss: 0.011646 0.012376 [1230848/1237259]
Epoch 840
-------------------------------
loss: 0.014176 0.012551 [ 2048/1237259]
loss: 0.012954 0.012540 [206848/1237259]
loss: 0.010885 0.012445 [411648/1237259]
loss: 0.011423 0.012462 [616448/1237259]
loss: 0.012849 0.012505 [821248/1237259]
loss: 0.014620 0.012487 [1026048/1237259]
loss: 0.011113 0.012481 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0533  
diversity: 0.1957  


Epoch 841
-------------------------------
loss: 0.012462 0.012429 [ 2048/1237259]
loss: 0.013904 0.012399 [206848/1237259]
loss: 0.012365 0.012451 [411648/1237259]
loss: 0.014137 0.012533 [616448/1237259]
loss: 0.010290 0.012470 [821248/1237259]
loss: 0.011693 0.012564 [1026048/1237259]
loss: 0.014455 0.012451 [1230848/1237259]
Epoch 842
-------------------------------
loss: 0.013457 0.012405 [ 2048/1237259]
loss: 0.012767 0.012466 [206848/1237259]
loss: 0.011914 0.012536 [411648/1237259]
loss: 0.016396 0.012535 [616448/1237259]
loss: 0.014847 0.012408 [821248/1237259]
loss: 0.012050 0.012495 [1026048/1237259]
loss: 0.011224 0.012392 [1230848/1237259]
Epoch 843
-------------------------------
loss: 0.011226 0.012376 [ 2048/1237259]
loss: 0.013965 0.012553 [206848/1237259]
loss: 0.015358 0.012534 [411648/1237259]
loss: 0.012685 0.012584 [616448/1237259]
loss: 0.013652 0.012376 [821248/1237259]
loss: 0.013745 0.012556 [1026048/1237259]
loss: 0.013256 0.012491 [1230848/1237259]
Epoch 844
-------------------------------
loss: 0.010578 0.012338 [ 2048/1237259]
loss: 0.011239 0.012396 [206848/1237259]
loss: 0.011318 0.012444 [411648/1237259]
loss: 0.013688 0.012454 [616448/1237259]
loss: 0.011848 0.012478 [821248/1237259]
loss: 0.010845 0.012408 [1026048/1237259]
loss: 0.013615 0.012393 [1230848/1237259]
Epoch 845
-------------------------------
loss: 0.015316 0.012611 [ 2048/1237259]
loss: 0.013629 0.012340 [206848/1237259]
loss: 0.015333 0.012498 [411648/1237259]
loss: 0.014259 0.012526 [616448/1237259]
loss: 0.010691 0.012411 [821248/1237259]
loss: 0.010948 0.012714 [1026048/1237259]
loss: 0.012019 0.012379 [1230848/1237259]
Eval results: 
recall@20: 0.0647  
ndcg@20: 0.0532  
diversity: 0.1956  


Epoch 846
-------------------------------
loss: 0.015107 0.012553 [ 2048/1237259]
loss: 0.016463 0.012540 [206848/1237259]
loss: 0.014045 0.012565 [411648/1237259]
loss: 0.012579 0.012498 [616448/1237259]
loss: 0.009346 0.012530 [821248/1237259]
loss: 0.012260 0.012539 [1026048/1237259]
loss: 0.012911 0.012731 [1230848/1237259]
Epoch 847
-------------------------------
loss: 0.010807 0.012494 [ 2048/1237259]
loss: 0.014526 0.012454 [206848/1237259]
loss: 0.011412 0.012557 [411648/1237259]
loss: 0.012365 0.012489 [616448/1237259]
loss: 0.012528 0.012449 [821248/1237259]
loss: 0.020790 0.012498 [1026048/1237259]
loss: 0.013419 0.012543 [1230848/1237259]
Epoch 848
-------------------------------
loss: 0.010976 0.012456 [ 2048/1237259]
loss: 0.014466 0.012607 [206848/1237259]
loss: 0.013162 0.012433 [411648/1237259]
loss: 0.009621 0.012618 [616448/1237259]
loss: 0.015149 0.012404 [821248/1237259]
loss: 0.014434 0.012424 [1026048/1237259]
loss: 0.013838 0.012489 [1230848/1237259]
Epoch 849
-------------------------------
loss: 0.011517 0.012413 [ 2048/1237259]
loss: 0.012683 0.012532 [206848/1237259]
loss: 0.010806 0.012614 [411648/1237259]
loss: 0.014816 0.012594 [616448/1237259]
loss: 0.014740 0.012507 [821248/1237259]
loss: 0.013996 0.012506 [1026048/1237259]
loss: 0.013089 0.012478 [1230848/1237259]
Epoch 850
-------------------------------
loss: 0.012685 0.012439 [ 2048/1237259]
loss: 0.014056 0.012590 [206848/1237259]
loss: 0.014914 0.012638 [411648/1237259]
loss: 0.016840 0.012441 [616448/1237259]
loss: 0.011323 0.012438 [821248/1237259]
loss: 0.012432 0.012489 [1026048/1237259]
loss: 0.015259 0.012455 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0533  
diversity: 0.1958  


Epoch 851
-------------------------------
loss: 0.008164 0.012524 [ 2048/1237259]
loss: 0.009790 0.012515 [206848/1237259]
loss: 0.012276 0.012384 [411648/1237259]
loss: 0.010382 0.012581 [616448/1237259]
loss: 0.011873 0.012323 [821248/1237259]
loss: 0.013549 0.012449 [1026048/1237259]
loss: 0.011479 0.012596 [1230848/1237259]
Epoch 852
-------------------------------
loss: 0.013905 0.012578 [ 2048/1237259]
loss: 0.012516 0.012377 [206848/1237259]
loss: 0.009758 0.012556 [411648/1237259]
loss: 0.011599 0.012583 [616448/1237259]
loss: 0.016469 0.012480 [821248/1237259]
loss: 0.011423 0.012478 [1026048/1237259]
loss: 0.012529 0.012587 [1230848/1237259]
Epoch 853
-------------------------------
loss: 0.011318 0.012518 [ 2048/1237259]
loss: 0.016548 0.012490 [206848/1237259]
loss: 0.012615 0.012585 [411648/1237259]
loss: 0.011414 0.012579 [616448/1237259]
loss: 0.012948 0.012480 [821248/1237259]
loss: 0.014641 0.012491 [1026048/1237259]
loss: 0.011844 0.012546 [1230848/1237259]
Epoch 854
-------------------------------
loss: 0.017200 0.012478 [ 2048/1237259]
loss: 0.013039 0.012489 [206848/1237259]
loss: 0.013723 0.012563 [411648/1237259]
loss: 0.010908 0.012464 [616448/1237259]
loss: 0.012463 0.012578 [821248/1237259]
loss: 0.016942 0.012251 [1026048/1237259]
loss: 0.011646 0.012540 [1230848/1237259]
Epoch 855
-------------------------------
loss: 0.012006 0.012589 [ 2048/1237259]
loss: 0.010832 0.012495 [206848/1237259]
loss: 0.017198 0.012538 [411648/1237259]
loss: 0.010229 0.012459 [616448/1237259]
loss: 0.013660 0.012513 [821248/1237259]
loss: 0.011936 0.012450 [1026048/1237259]
loss: 0.014567 0.012455 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0533  
diversity: 0.1961  


Epoch 856
-------------------------------
loss: 0.013418 0.012564 [ 2048/1237259]
loss: 0.015136 0.012485 [206848/1237259]
loss: 0.015168 0.012485 [411648/1237259]
loss: 0.011708 0.012420 [616448/1237259]
loss: 0.010041 0.012460 [821248/1237259]
loss: 0.016251 0.012678 [1026048/1237259]
loss: 0.016507 0.012593 [1230848/1237259]
Epoch 857
-------------------------------
loss: 0.011956 0.012574 [ 2048/1237259]
loss: 0.011617 0.012417 [206848/1237259]
loss: 0.011258 0.012727 [411648/1237259]
loss: 0.018397 0.012556 [616448/1237259]
loss: 0.011103 0.012552 [821248/1237259]
loss: 0.016267 0.012401 [1026048/1237259]
loss: 0.016877 0.012470 [1230848/1237259]
Epoch 858
-------------------------------
loss: 0.008908 0.012494 [ 2048/1237259]
loss: 0.014216 0.012371 [206848/1237259]
loss: 0.012611 0.012620 [411648/1237259]
loss: 0.011458 0.012606 [616448/1237259]
loss: 0.014744 0.012483 [821248/1237259]
loss: 0.010867 0.012519 [1026048/1237259]
loss: 0.010334 0.012604 [1230848/1237259]
Epoch 859
-------------------------------
loss: 0.011950 0.012442 [ 2048/1237259]
loss: 0.011361 0.012581 [206848/1237259]
loss: 0.015350 0.012729 [411648/1237259]
loss: 0.011187 0.012482 [616448/1237259]
loss: 0.010789 0.012448 [821248/1237259]
loss: 0.012028 0.012522 [1026048/1237259]
loss: 0.011597 0.012460 [1230848/1237259]
Epoch 860
-------------------------------
loss: 0.016260 0.012522 [ 2048/1237259]
loss: 0.013877 0.012474 [206848/1237259]
loss: 0.013623 0.012371 [411648/1237259]
loss: 0.012497 0.012517 [616448/1237259]
loss: 0.012082 0.012462 [821248/1237259]
loss: 0.013314 0.012503 [1026048/1237259]
loss: 0.011211 0.012603 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0534  
diversity: 0.1960  


Epoch 861
-------------------------------
loss: 0.009960 0.012662 [ 2048/1237259]
loss: 0.014284 0.012447 [206848/1237259]
loss: 0.014601 0.012519 [411648/1237259]
loss: 0.015959 0.012444 [616448/1237259]
loss: 0.010314 0.012574 [821248/1237259]
loss: 0.012237 0.012502 [1026048/1237259]
loss: 0.012990 0.012603 [1230848/1237259]
Epoch 862
-------------------------------
loss: 0.014934 0.012461 [ 2048/1237259]
loss: 0.013286 0.012437 [206848/1237259]
loss: 0.009831 0.012578 [411648/1237259]
loss: 0.010960 0.012540 [616448/1237259]
loss: 0.012765 0.012525 [821248/1237259]
loss: 0.010129 0.012521 [1026048/1237259]
loss: 0.012012 0.012557 [1230848/1237259]
Epoch 863
-------------------------------
loss: 0.012937 0.012533 [ 2048/1237259]
loss: 0.015825 0.012587 [206848/1237259]
loss: 0.014347 0.012575 [411648/1237259]
loss: 0.010953 0.012409 [616448/1237259]
loss: 0.008409 0.012524 [821248/1237259]
loss: 0.010439 0.012527 [1026048/1237259]
loss: 0.015517 0.012594 [1230848/1237259]
Epoch 864
-------------------------------
loss: 0.016748 0.012652 [ 2048/1237259]
loss: 0.017093 0.012533 [206848/1237259]
loss: 0.013376 0.012492 [411648/1237259]
loss: 0.013220 0.012504 [616448/1237259]
loss: 0.011262 0.012558 [821248/1237259]
loss: 0.012608 0.012532 [1026048/1237259]
loss: 0.015574 0.012552 [1230848/1237259]
Epoch 865
-------------------------------
loss: 0.016262 0.012495 [ 2048/1237259]
loss: 0.012760 0.012519 [206848/1237259]
loss: 0.011227 0.012452 [411648/1237259]
loss: 0.012765 0.012574 [616448/1237259]
loss: 0.013496 0.012469 [821248/1237259]
loss: 0.018314 0.012566 [1026048/1237259]
loss: 0.015340 0.012477 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0533  
diversity: 0.1960  


Epoch 866
-------------------------------
loss: 0.012848 0.012503 [ 2048/1237259]
loss: 0.011848 0.012403 [206848/1237259]
loss: 0.015038 0.012530 [411648/1237259]
loss: 0.008646 0.012468 [616448/1237259]
loss: 0.013882 0.012517 [821248/1237259]
loss: 0.012666 0.012538 [1026048/1237259]
loss: 0.014739 0.012561 [1230848/1237259]
Epoch 867
-------------------------------
loss: 0.014761 0.012450 [ 2048/1237259]
loss: 0.012791 0.012559 [206848/1237259]
loss: 0.011022 0.012607 [411648/1237259]
loss: 0.014072 0.012469 [616448/1237259]
loss: 0.010693 0.012514 [821248/1237259]
loss: 0.011231 0.012486 [1026048/1237259]
loss: 0.012953 0.012597 [1230848/1237259]
Epoch 868
-------------------------------
loss: 0.010819 0.012482 [ 2048/1237259]
loss: 0.010027 0.012657 [206848/1237259]
loss: 0.020104 0.012497 [411648/1237259]
loss: 0.012351 0.012470 [616448/1237259]
loss: 0.009367 0.012524 [821248/1237259]
loss: 0.010286 0.012659 [1026048/1237259]
loss: 0.013184 0.012447 [1230848/1237259]
Epoch 869
-------------------------------
loss: 0.016743 0.012593 [ 2048/1237259]
loss: 0.015327 0.012550 [206848/1237259]
loss: 0.011108 0.012523 [411648/1237259]
loss: 0.012178 0.012750 [616448/1237259]
loss: 0.009574 0.012790 [821248/1237259]
loss: 0.013746 0.012503 [1026048/1237259]
loss: 0.010740 0.012481 [1230848/1237259]
Epoch 870
-------------------------------
loss: 0.016579 0.012460 [ 2048/1237259]
loss: 0.014970 0.012565 [206848/1237259]
loss: 0.011883 0.012604 [411648/1237259]
loss: 0.012890 0.012468 [616448/1237259]
loss: 0.012240 0.012500 [821248/1237259]
loss: 0.010791 0.012576 [1026048/1237259]
loss: 0.008564 0.012472 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0533  
diversity: 0.1959  


Epoch 871
-------------------------------
loss: 0.011787 0.012513 [ 2048/1237259]
loss: 0.009326 0.012452 [206848/1237259]
loss: 0.013220 0.012540 [411648/1237259]
loss: 0.010471 0.012597 [616448/1237259]
loss: 0.011268 0.012546 [821248/1237259]
loss: 0.009721 0.012511 [1026048/1237259]
loss: 0.015325 0.012539 [1230848/1237259]
Epoch 872
-------------------------------
loss: 0.009836 0.012697 [ 2048/1237259]
loss: 0.012427 0.012603 [206848/1237259]
loss: 0.013154 0.012391 [411648/1237259]
loss: 0.013401 0.012524 [616448/1237259]
loss: 0.014947 0.012401 [821248/1237259]
loss: 0.010611 0.012434 [1026048/1237259]
loss: 0.016280 0.012553 [1230848/1237259]
Epoch 873
-------------------------------
loss: 0.013834 0.012485 [ 2048/1237259]
loss: 0.016242 0.012494 [206848/1237259]
loss: 0.015582 0.012457 [411648/1237259]
loss: 0.012400 0.012441 [616448/1237259]
loss: 0.012960 0.012667 [821248/1237259]
loss: 0.014378 0.012605 [1026048/1237259]
loss: 0.015064 0.012423 [1230848/1237259]
Epoch 874
-------------------------------
loss: 0.012014 0.012514 [ 2048/1237259]
loss: 0.013229 0.012550 [206848/1237259]
loss: 0.013006 0.012558 [411648/1237259]
loss: 0.015196 0.012455 [616448/1237259]
loss: 0.010534 0.012565 [821248/1237259]
loss: 0.013506 0.012549 [1026048/1237259]
loss: 0.011496 0.012550 [1230848/1237259]
Epoch 875
-------------------------------
loss: 0.009664 0.012505 [ 2048/1237259]
loss: 0.015091 0.012415 [206848/1237259]
loss: 0.014964 0.012519 [411648/1237259]
loss: 0.015131 0.012524 [616448/1237259]
loss: 0.012499 0.012544 [821248/1237259]
loss: 0.012234 0.012511 [1026048/1237259]
loss: 0.014578 0.012550 [1230848/1237259]
Eval results: 
recall@20: 0.0648  
ndcg@20: 0.0533  
diversity: 0.1959  


Epoch 876
-------------------------------
loss: 0.011804 0.012606 [ 2048/1237259]
loss: 0.010723 0.012624 [206848/1237259]
loss: 0.014001 0.012506 [411648/1237259]
loss: 0.014765 0.012534 [616448/1237259]
loss: 0.009910 0.012573 [821248/1237259]
loss: 0.019241 0.012550 [1026048/1237259]
loss: 0.014101 0.012454 [1230848/1237259]
Epoch 877
-------------------------------
loss: 0.011502 0.012556 [ 2048/1237259]
loss: 0.014664 0.012483 [206848/1237259]
loss: 0.009635 0.012442 [411648/1237259]
loss: 0.012089 0.012625 [616448/1237259]
loss: 0.015451 0.012516 [821248/1237259]
loss: 0.011922 0.012706 [1026048/1237259]
loss: 0.009943 0.012609 [1230848/1237259]
Epoch 878
-------------------------------
loss: 0.010300 0.012577 [ 2048/1237259]
loss: 0.015552 0.012583 [206848/1237259]
loss: 0.010539 0.012514 [411648/1237259]
loss: 0.011555 0.012476 [616448/1237259]
loss: 0.011939 0.012494 [821248/1237259]
loss: 0.008998 0.012661 [1026048/1237259]
loss: 0.012068 0.012621 [1230848/1237259]
Epoch 879
-------------------------------
loss: 0.014001 0.012744 [ 2048/1237259]
loss: 0.012901 0.012467 [206848/1237259]
loss: 0.010098 0.012558 [411648/1237259]
loss: 0.012307 0.012602 [616448/1237259]
loss: 0.016230 0.012439 [821248/1237259]
loss: 0.010192 0.012619 [1026048/1237259]
loss: 0.011973 0.012700 [1230848/1237259]
Epoch 880
-------------------------------
loss: 0.010749 0.012448 [ 2048/1237259]
loss: 0.014213 0.012516 [206848/1237259]
loss: 0.015467 0.012548 [411648/1237259]
loss: 0.010993 0.012469 [616448/1237259]
loss: 0.015090 0.012489 [821248/1237259]
loss: 0.013647 0.012583 [1026048/1237259]
loss: 0.013788 0.012505 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0533  
diversity: 0.1957  


Epoch 881
-------------------------------
loss: 0.009771 0.012494 [ 2048/1237259]
loss: 0.013637 0.012464 [206848/1237259]
loss: 0.006192 0.012401 [411648/1237259]
loss: 0.013013 0.012445 [616448/1237259]
loss: 0.014055 0.012516 [821248/1237259]
loss: 0.012469 0.012462 [1026048/1237259]
loss: 0.017604 0.012551 [1230848/1237259]
Epoch 882
-------------------------------
loss: 0.015760 0.012571 [ 2048/1237259]
loss: 0.011773 0.012475 [206848/1237259]
loss: 0.013625 0.012531 [411648/1237259]
loss: 0.012434 0.012519 [616448/1237259]
loss: 0.014963 0.012594 [821248/1237259]
loss: 0.015742 0.012477 [1026048/1237259]
loss: 0.014765 0.012377 [1230848/1237259]
Epoch 883
-------------------------------
loss: 0.017237 0.012391 [ 2048/1237259]
loss: 0.011310 0.012466 [206848/1237259]
loss: 0.013257 0.012537 [411648/1237259]
loss: 0.008198 0.012639 [616448/1237259]
loss: 0.011925 0.012570 [821248/1237259]
loss: 0.011932 0.012467 [1026048/1237259]
loss: 0.011649 0.012523 [1230848/1237259]
Epoch 884
-------------------------------
loss: 0.014024 0.012491 [ 2048/1237259]
loss: 0.014826 0.012535 [206848/1237259]
loss: 0.015336 0.012516 [411648/1237259]
loss: 0.008032 0.012562 [616448/1237259]
loss: 0.011553 0.012609 [821248/1237259]
loss: 0.013318 0.012636 [1026048/1237259]
loss: 0.012574 0.012600 [1230848/1237259]
Epoch 885
-------------------------------
loss: 0.011872 0.012583 [ 2048/1237259]
loss: 0.011405 0.012496 [206848/1237259]
loss: 0.011643 0.012549 [411648/1237259]
loss: 0.012621 0.012540 [616448/1237259]
loss: 0.010859 0.012554 [821248/1237259]
loss: 0.015101 0.012592 [1026048/1237259]
loss: 0.014253 0.012504 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0533  
diversity: 0.1958  


Epoch 886
-------------------------------
loss: 0.012313 0.012521 [ 2048/1237259]
loss: 0.015045 0.012664 [206848/1237259]
loss: 0.017009 0.012430 [411648/1237259]
loss: 0.013462 0.012529 [616448/1237259]
loss: 0.010573 0.012480 [821248/1237259]
loss: 0.011564 0.012636 [1026048/1237259]
loss: 0.009695 0.012510 [1230848/1237259]
Epoch 887
-------------------------------
loss: 0.013088 0.012615 [ 2048/1237259]
loss: 0.017656 0.012479 [206848/1237259]
loss: 0.013252 0.012635 [411648/1237259]
loss: 0.017649 0.012448 [616448/1237259]
loss: 0.013497 0.012604 [821248/1237259]
loss: 0.012467 0.012595 [1026048/1237259]
loss: 0.012754 0.012549 [1230848/1237259]
Epoch 888
-------------------------------
loss: 0.009540 0.012508 [ 2048/1237259]
loss: 0.013326 0.012546 [206848/1237259]
loss: 0.011346 0.012625 [411648/1237259]
loss: 0.011836 0.012541 [616448/1237259]
loss: 0.011090 0.012558 [821248/1237259]
loss: 0.010022 0.012593 [1026048/1237259]
loss: 0.012330 0.012575 [1230848/1237259]
Epoch 889
-------------------------------
loss: 0.012096 0.012648 [ 2048/1237259]
loss: 0.010032 0.012496 [206848/1237259]
loss: 0.013427 0.012614 [411648/1237259]
loss: 0.013915 0.012537 [616448/1237259]
loss: 0.013621 0.012503 [821248/1237259]
loss: 0.010953 0.012493 [1026048/1237259]
loss: 0.010639 0.012504 [1230848/1237259]
Epoch 890
-------------------------------
loss: 0.015186 0.012524 [ 2048/1237259]
loss: 0.009332 0.012530 [206848/1237259]
loss: 0.012922 0.012513 [411648/1237259]
loss: 0.017002 0.012505 [616448/1237259]
loss: 0.007386 0.012511 [821248/1237259]
loss: 0.012383 0.012444 [1026048/1237259]
loss: 0.012873 0.012442 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0532  
diversity: 0.1957  


Epoch 891
-------------------------------
loss: 0.014575 0.012497 [ 2048/1237259]
loss: 0.010049 0.012622 [206848/1237259]
loss: 0.015456 0.012557 [411648/1237259]
loss: 0.015879 0.012583 [616448/1237259]
loss: 0.014460 0.012640 [821248/1237259]
loss: 0.014487 0.012478 [1026048/1237259]
loss: 0.013504 0.012511 [1230848/1237259]
Epoch 892
-------------------------------
loss: 0.014259 0.012432 [ 2048/1237259]
loss: 0.010709 0.012652 [206848/1237259]
loss: 0.010529 0.012525 [411648/1237259]
loss: 0.009772 0.012517 [616448/1237259]
loss: 0.010392 0.012772 [821248/1237259]
loss: 0.011691 0.012629 [1026048/1237259]
loss: 0.015510 0.012486 [1230848/1237259]
Epoch 893
-------------------------------
loss: 0.011056 0.012558 [ 2048/1237259]
loss: 0.011344 0.012516 [206848/1237259]
loss: 0.013370 0.012527 [411648/1237259]
loss: 0.009390 0.012566 [616448/1237259]
loss: 0.012407 0.012486 [821248/1237259]
loss: 0.009640 0.012668 [1026048/1237259]
loss: 0.010822 0.012491 [1230848/1237259]
Epoch 894
-------------------------------
loss: 0.014580 0.012615 [ 2048/1237259]
loss: 0.012872 0.012594 [206848/1237259]
loss: 0.013470 0.012468 [411648/1237259]
loss: 0.011294 0.012555 [616448/1237259]
loss: 0.013647 0.012547 [821248/1237259]
loss: 0.011560 0.012555 [1026048/1237259]
loss: 0.014017 0.012565 [1230848/1237259]
Epoch 895
-------------------------------
loss: 0.012515 0.012523 [ 2048/1237259]
loss: 0.011955 0.012529 [206848/1237259]
loss: 0.013401 0.012523 [411648/1237259]
loss: 0.012250 0.012518 [616448/1237259]
loss: 0.017935 0.012503 [821248/1237259]
loss: 0.011485 0.012482 [1026048/1237259]
loss: 0.009230 0.012564 [1230848/1237259]
Eval results: 
recall@20: 0.0652  
ndcg@20: 0.0533  
diversity: 0.1957  


Epoch 896
-------------------------------
loss: 0.014449 0.012455 [ 2048/1237259]
loss: 0.012378 0.012530 [206848/1237259]
loss: 0.011027 0.012476 [411648/1237259]
loss: 0.012046 0.012539 [616448/1237259]
loss: 0.011317 0.012617 [821248/1237259]
loss: 0.013569 0.012513 [1026048/1237259]
loss: 0.012780 0.012491 [1230848/1237259]
Epoch 897
-------------------------------
loss: 0.010187 0.012593 [ 2048/1237259]
loss: 0.012821 0.012565 [206848/1237259]
loss: 0.011020 0.012546 [411648/1237259]
loss: 0.011498 0.012619 [616448/1237259]
loss: 0.012207 0.012587 [821248/1237259]
loss: 0.009743 0.012423 [1026048/1237259]
loss: 0.010822 0.012526 [1230848/1237259]
Epoch 898
-------------------------------
loss: 0.012532 0.012512 [ 2048/1237259]
loss: 0.009699 0.012384 [206848/1237259]
loss: 0.012431 0.012547 [411648/1237259]
loss: 0.013321 0.012751 [616448/1237259]
loss: 0.015633 0.012581 [821248/1237259]
loss: 0.013797 0.012652 [1026048/1237259]
loss: 0.013645 0.012673 [1230848/1237259]
Epoch 899
-------------------------------
loss: 0.012397 0.012474 [ 2048/1237259]
loss: 0.011860 0.012567 [206848/1237259]
loss: 0.011428 0.012737 [411648/1237259]
loss: 0.011502 0.012546 [616448/1237259]
loss: 0.014851 0.012513 [821248/1237259]
loss: 0.015648 0.012600 [1026048/1237259]
loss: 0.012300 0.012444 [1230848/1237259]
Epoch 900
-------------------------------
loss: 0.010455 0.012547 [ 2048/1237259]
loss: 0.010665 0.012543 [206848/1237259]
loss: 0.013433 0.012497 [411648/1237259]
loss: 0.017703 0.012542 [616448/1237259]
loss: 0.018141 0.012599 [821248/1237259]
loss: 0.011256 0.012686 [1026048/1237259]
loss: 0.013876 0.012511 [1230848/1237259]
Eval results: 
recall@20: 0.0653  
ndcg@20: 0.0535  
diversity: 0.1958  


Epoch 901
-------------------------------
loss: 0.013360 0.012528 [ 2048/1237259]
loss: 0.015280 0.012489 [206848/1237259]
loss: 0.015779 0.012636 [411648/1237259]
loss: 0.013730 0.012505 [616448/1237259]
loss: 0.012774 0.012635 [821248/1237259]
loss: 0.020260 0.012517 [1026048/1237259]
loss: 0.010304 0.012621 [1230848/1237259]
Epoch 902
-------------------------------
loss: 0.015699 0.012509 [ 2048/1237259]
loss: 0.009998 0.012556 [206848/1237259]
loss: 0.013253 0.012649 [411648/1237259]
loss: 0.010283 0.012588 [616448/1237259]
loss: 0.014473 0.012480 [821248/1237259]
loss: 0.012576 0.012512 [1026048/1237259]
loss: 0.012735 0.012543 [1230848/1237259]
Epoch 903
-------------------------------
loss: 0.009152 0.012612 [ 2048/1237259]
loss: 0.017675 0.012538 [206848/1237259]
loss: 0.016372 0.012562 [411648/1237259]
loss: 0.016351 0.012659 [616448/1237259]
loss: 0.014166 0.012565 [821248/1237259]
loss: 0.010081 0.012491 [1026048/1237259]
loss: 0.010730 0.012517 [1230848/1237259]
Epoch 904
-------------------------------
loss: 0.011837 0.012619 [ 2048/1237259]
loss: 0.011368 0.012441 [206848/1237259]
loss: 0.012023 0.012672 [411648/1237259]
loss: 0.012681 0.012646 [616448/1237259]
loss: 0.011997 0.012582 [821248/1237259]
loss: 0.008716 0.012434 [1026048/1237259]
loss: 0.013075 0.012602 [1230848/1237259]
Epoch 905
-------------------------------
loss: 0.009832 0.012604 [ 2048/1237259]
loss: 0.012640 0.012592 [206848/1237259]
loss: 0.012077 0.012548 [411648/1237259]
loss: 0.011933 0.012641 [616448/1237259]
loss: 0.013037 0.012654 [821248/1237259]
loss: 0.012867 0.012490 [1026048/1237259]
loss: 0.011984 0.012570 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0534  
diversity: 0.1958  


Epoch 906
-------------------------------
loss: 0.013032 0.012690 [ 2048/1237259]
loss: 0.017266 0.012559 [206848/1237259]
loss: 0.009717 0.012515 [411648/1237259]
loss: 0.010449 0.012459 [616448/1237259]
loss: 0.011354 0.012441 [821248/1237259]
loss: 0.012479 0.012548 [1026048/1237259]
loss: 0.014179 0.012577 [1230848/1237259]
Epoch 907
-------------------------------
loss: 0.014786 0.012612 [ 2048/1237259]
loss: 0.015244 0.012434 [206848/1237259]
loss: 0.009214 0.012610 [411648/1237259]
loss: 0.012047 0.012449 [616448/1237259]
loss: 0.013758 0.012585 [821248/1237259]
loss: 0.012575 0.012516 [1026048/1237259]
loss: 0.015871 0.012677 [1230848/1237259]
Epoch 908
-------------------------------
loss: 0.013761 0.012531 [ 2048/1237259]
loss: 0.014289 0.012515 [206848/1237259]
loss: 0.013550 0.012427 [411648/1237259]
loss: 0.012288 0.012602 [616448/1237259]
loss: 0.014266 0.012576 [821248/1237259]
loss: 0.012962 0.012466 [1026048/1237259]
loss: 0.009462 0.012709 [1230848/1237259]
Epoch 909
-------------------------------
loss: 0.012239 0.012649 [ 2048/1237259]
loss: 0.010707 0.012564 [206848/1237259]
loss: 0.013634 0.012585 [411648/1237259]
loss: 0.014594 0.012373 [616448/1237259]
loss: 0.018004 0.012521 [821248/1237259]
loss: 0.014479 0.012532 [1026048/1237259]
loss: 0.010501 0.012749 [1230848/1237259]
Epoch 910
-------------------------------
loss: 0.009881 0.012478 [ 2048/1237259]
loss: 0.011079 0.012461 [206848/1237259]
loss: 0.012670 0.012764 [411648/1237259]
loss: 0.012869 0.012486 [616448/1237259]
loss: 0.012703 0.012632 [821248/1237259]
loss: 0.014583 0.012336 [1026048/1237259]
loss: 0.012163 0.012584 [1230848/1237259]
Eval results: 
recall@20: 0.0648  
ndcg@20: 0.0533  
diversity: 0.1958  


Epoch 911
-------------------------------
loss: 0.015800 0.012671 [ 2048/1237259]
loss: 0.010016 0.012490 [206848/1237259]
loss: 0.012406 0.012536 [411648/1237259]
loss: 0.012043 0.012538 [616448/1237259]
loss: 0.011340 0.012449 [821248/1237259]
loss: 0.015978 0.012620 [1026048/1237259]
loss: 0.011860 0.012424 [1230848/1237259]
Epoch 912
-------------------------------
loss: 0.013606 0.012701 [ 2048/1237259]
loss: 0.010941 0.012622 [206848/1237259]
loss: 0.010952 0.012537 [411648/1237259]
loss: 0.009682 0.012549 [616448/1237259]
loss: 0.014602 0.012527 [821248/1237259]
loss: 0.014929 0.012591 [1026048/1237259]
loss: 0.011712 0.012701 [1230848/1237259]
Epoch 913
-------------------------------
loss: 0.010250 0.012475 [ 2048/1237259]
loss: 0.011847 0.012580 [206848/1237259]
loss: 0.011673 0.012580 [411648/1237259]
loss: 0.013568 0.012627 [616448/1237259]
loss: 0.013425 0.012538 [821248/1237259]
loss: 0.011022 0.012539 [1026048/1237259]
loss: 0.014174 0.012637 [1230848/1237259]
Epoch 914
-------------------------------
loss: 0.015201 0.012582 [ 2048/1237259]
loss: 0.013722 0.012567 [206848/1237259]
loss: 0.011850 0.012688 [411648/1237259]
loss: 0.014831 0.012615 [616448/1237259]
loss: 0.017158 0.012491 [821248/1237259]
loss: 0.010630 0.012567 [1026048/1237259]
loss: 0.010388 0.012548 [1230848/1237259]
Epoch 915
-------------------------------
loss: 0.010115 0.012677 [ 2048/1237259]
loss: 0.011449 0.012568 [206848/1237259]
loss: 0.014257 0.012675 [411648/1237259]
loss: 0.015318 0.012649 [616448/1237259]
loss: 0.012066 0.012528 [821248/1237259]
loss: 0.011595 0.012618 [1026048/1237259]
loss: 0.010972 0.012421 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0532  
diversity: 0.1956  


Epoch 916
-------------------------------
loss: 0.013896 0.012532 [ 2048/1237259]
loss: 0.011095 0.012557 [206848/1237259]
loss: 0.013380 0.012563 [411648/1237259]
loss: 0.016363 0.012671 [616448/1237259]
loss: 0.013483 0.012576 [821248/1237259]
loss: 0.012252 0.012618 [1026048/1237259]
loss: 0.014990 0.012537 [1230848/1237259]
Epoch 917
-------------------------------
loss: 0.011860 0.012708 [ 2048/1237259]
loss: 0.011202 0.012588 [206848/1237259]
loss: 0.013388 0.012642 [411648/1237259]
loss: 0.012472 0.012581 [616448/1237259]
loss: 0.011894 0.012529 [821248/1237259]
loss: 0.010394 0.012496 [1026048/1237259]
loss: 0.014542 0.012598 [1230848/1237259]
Epoch 918
-------------------------------
loss: 0.009410 0.012590 [ 2048/1237259]
loss: 0.011811 0.012511 [206848/1237259]
loss: 0.010529 0.012528 [411648/1237259]
loss: 0.008849 0.012614 [616448/1237259]
loss: 0.014177 0.012650 [821248/1237259]
loss: 0.012017 0.012543 [1026048/1237259]
loss: 0.014415 0.012544 [1230848/1237259]
Epoch 919
-------------------------------
loss: 0.009545 0.012428 [ 2048/1237259]
loss: 0.007408 0.012397 [206848/1237259]
loss: 0.013239 0.012530 [411648/1237259]
loss: 0.013005 0.012670 [616448/1237259]
loss: 0.011303 0.012554 [821248/1237259]
loss: 0.013648 0.012554 [1026048/1237259]
loss: 0.013537 0.012519 [1230848/1237259]
Epoch 920
-------------------------------
loss: 0.014065 0.012595 [ 2048/1237259]
loss: 0.014362 0.012515 [206848/1237259]
loss: 0.014255 0.012566 [411648/1237259]
loss: 0.009676 0.012545 [616448/1237259]
loss: 0.013832 0.012576 [821248/1237259]
loss: 0.012331 0.012548 [1026048/1237259]
loss: 0.010661 0.012616 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0533  
diversity: 0.1960  


Epoch 921
-------------------------------
loss: 0.017015 0.012560 [ 2048/1237259]
loss: 0.013184 0.012465 [206848/1237259]
loss: 0.012460 0.012602 [411648/1237259]
loss: 0.014351 0.012579 [616448/1237259]
loss: 0.009758 0.012716 [821248/1237259]
loss: 0.017464 0.012419 [1026048/1237259]
loss: 0.011214 0.012540 [1230848/1237259]
Epoch 922
-------------------------------
loss: 0.013057 0.012585 [ 2048/1237259]
loss: 0.011192 0.012599 [206848/1237259]
loss: 0.011961 0.012629 [411648/1237259]
loss: 0.010715 0.012603 [616448/1237259]
loss: 0.011144 0.012652 [821248/1237259]
loss: 0.010629 0.012535 [1026048/1237259]
loss: 0.012156 0.012612 [1230848/1237259]
Epoch 923
-------------------------------
loss: 0.011684 0.012591 [ 2048/1237259]
loss: 0.015969 0.012552 [206848/1237259]
loss: 0.012380 0.012455 [411648/1237259]
loss: 0.010829 0.012556 [616448/1237259]
loss: 0.010553 0.012652 [821248/1237259]
loss: 0.016285 0.012595 [1026048/1237259]
loss: 0.012357 0.012507 [1230848/1237259]
Epoch 924
-------------------------------
loss: 0.014475 0.012595 [ 2048/1237259]
loss: 0.011460 0.012495 [206848/1237259]
loss: 0.013502 0.012672 [411648/1237259]
loss: 0.009723 0.012565 [616448/1237259]
loss: 0.011959 0.012406 [821248/1237259]
loss: 0.010232 0.012688 [1026048/1237259]
loss: 0.010092 0.012632 [1230848/1237259]
Epoch 925
-------------------------------
loss: 0.012360 0.012418 [ 2048/1237259]
loss: 0.012402 0.012619 [206848/1237259]
loss: 0.010090 0.012362 [411648/1237259]
loss: 0.009819 0.012587 [616448/1237259]
loss: 0.013333 0.012624 [821248/1237259]
loss: 0.016020 0.012523 [1026048/1237259]
loss: 0.017064 0.012628 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0533  
diversity: 0.1956  


Epoch 926
-------------------------------
loss: 0.013603 0.012626 [ 2048/1237259]
loss: 0.009836 0.012536 [206848/1237259]
loss: 0.013847 0.012797 [411648/1237259]
loss: 0.017749 0.012613 [616448/1237259]
loss: 0.013590 0.012573 [821248/1237259]
loss: 0.010551 0.012622 [1026048/1237259]
loss: 0.013180 0.012586 [1230848/1237259]
Epoch 927
-------------------------------
loss: 0.010023 0.012558 [ 2048/1237259]
loss: 0.011816 0.012470 [206848/1237259]
loss: 0.012483 0.012517 [411648/1237259]
loss: 0.009895 0.012555 [616448/1237259]
loss: 0.011155 0.012541 [821248/1237259]
loss: 0.011304 0.012574 [1026048/1237259]
loss: 0.015061 0.012672 [1230848/1237259]
Epoch 928
-------------------------------
loss: 0.014449 0.012516 [ 2048/1237259]
loss: 0.014995 0.012552 [206848/1237259]
loss: 0.010003 0.012460 [411648/1237259]
loss: 0.012870 0.012638 [616448/1237259]
loss: 0.009909 0.012473 [821248/1237259]
loss: 0.014142 0.012566 [1026048/1237259]
loss: 0.010769 0.012464 [1230848/1237259]
Epoch 929
-------------------------------
loss: 0.014748 0.012573 [ 2048/1237259]
loss: 0.010826 0.012483 [206848/1237259]
loss: 0.012029 0.012562 [411648/1237259]
loss: 0.014678 0.012555 [616448/1237259]
loss: 0.016490 0.012616 [821248/1237259]
loss: 0.008921 0.012635 [1026048/1237259]
loss: 0.014038 0.012615 [1230848/1237259]
Epoch 930
-------------------------------
loss: 0.011874 0.012664 [ 2048/1237259]
loss: 0.010560 0.012651 [206848/1237259]
loss: 0.016954 0.012657 [411648/1237259]
loss: 0.011205 0.012534 [616448/1237259]
loss: 0.013974 0.012523 [821248/1237259]
loss: 0.010704 0.012631 [1026048/1237259]
loss: 0.011052 0.012627 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0533  
diversity: 0.1954  


Epoch 931
-------------------------------
loss: 0.013322 0.012653 [ 2048/1237259]
loss: 0.009732 0.012683 [206848/1237259]
loss: 0.016428 0.012563 [411648/1237259]
loss: 0.012459 0.012513 [616448/1237259]
loss: 0.014155 0.012639 [821248/1237259]
loss: 0.011782 0.012531 [1026048/1237259]
loss: 0.017577 0.012657 [1230848/1237259]
Epoch 932
-------------------------------
loss: 0.015283 0.012542 [ 2048/1237259]
loss: 0.013368 0.012592 [206848/1237259]
loss: 0.011483 0.012460 [411648/1237259]
loss: 0.010703 0.012650 [616448/1237259]
loss: 0.014098 0.012534 [821248/1237259]
loss: 0.012719 0.012613 [1026048/1237259]
loss: 0.011183 0.012616 [1230848/1237259]
Epoch 933
-------------------------------
loss: 0.013014 0.012673 [ 2048/1237259]
loss: 0.011751 0.012523 [206848/1237259]
loss: 0.012536 0.012699 [411648/1237259]
loss: 0.010266 0.012623 [616448/1237259]
loss: 0.011338 0.012553 [821248/1237259]
loss: 0.010079 0.012511 [1026048/1237259]
loss: 0.013901 0.012544 [1230848/1237259]
Epoch 934
-------------------------------
loss: 0.014766 0.012599 [ 2048/1237259]
loss: 0.013204 0.012600 [206848/1237259]
loss: 0.012799 0.012673 [411648/1237259]
loss: 0.014044 0.012643 [616448/1237259]
loss: 0.013332 0.012586 [821248/1237259]
loss: 0.012334 0.012509 [1026048/1237259]
loss: 0.009633 0.012515 [1230848/1237259]
Epoch 935
-------------------------------
loss: 0.018685 0.012661 [ 2048/1237259]
loss: 0.009397 0.012671 [206848/1237259]
loss: 0.013880 0.012667 [411648/1237259]
loss: 0.011729 0.012500 [616448/1237259]
loss: 0.011622 0.012608 [821248/1237259]
loss: 0.013577 0.012542 [1026048/1237259]
loss: 0.009217 0.012536 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0534  
diversity: 0.1957  


Epoch 936
-------------------------------
loss: 0.010568 0.012588 [ 2048/1237259]
loss: 0.014239 0.012717 [206848/1237259]
loss: 0.011362 0.012616 [411648/1237259]
loss: 0.013634 0.012492 [616448/1237259]
loss: 0.009460 0.012706 [821248/1237259]
loss: 0.016739 0.012480 [1026048/1237259]
loss: 0.017706 0.012498 [1230848/1237259]
Epoch 937
-------------------------------
loss: 0.011571 0.012506 [ 2048/1237259]
loss: 0.013290 0.012718 [206848/1237259]
loss: 0.011039 0.012551 [411648/1237259]
loss: 0.013365 0.012650 [616448/1237259]
loss: 0.010053 0.012600 [821248/1237259]
loss: 0.019259 0.012672 [1026048/1237259]
loss: 0.012734 0.012513 [1230848/1237259]
Epoch 938
-------------------------------
loss: 0.010307 0.012509 [ 2048/1237259]
loss: 0.013525 0.012502 [206848/1237259]
loss: 0.010238 0.012582 [411648/1237259]
loss: 0.009954 0.012557 [616448/1237259]
loss: 0.013691 0.012471 [821248/1237259]
loss: 0.012006 0.012694 [1026048/1237259]
loss: 0.012163 0.012562 [1230848/1237259]
Epoch 939
-------------------------------
loss: 0.009485 0.012420 [ 2048/1237259]
loss: 0.015706 0.012553 [206848/1237259]
loss: 0.011260 0.012516 [411648/1237259]
loss: 0.012485 0.012693 [616448/1237259]
loss: 0.009324 0.012550 [821248/1237259]
loss: 0.010364 0.012495 [1026048/1237259]
loss: 0.011090 0.012710 [1230848/1237259]
Epoch 940
-------------------------------
loss: 0.011375 0.012702 [ 2048/1237259]
loss: 0.013782 0.012533 [206848/1237259]
loss: 0.011473 0.012618 [411648/1237259]
loss: 0.011133 0.012535 [616448/1237259]
loss: 0.015354 0.012695 [821248/1237259]
loss: 0.013404 0.012610 [1026048/1237259]
loss: 0.008996 0.012548 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0534  
diversity: 0.1957  


Epoch 941
-------------------------------
loss: 0.011397 0.012652 [ 2048/1237259]
loss: 0.009201 0.012502 [206848/1237259]
loss: 0.009957 0.012599 [411648/1237259]
loss: 0.013679 0.012607 [616448/1237259]
loss: 0.011996 0.012705 [821248/1237259]
loss: 0.011396 0.012556 [1026048/1237259]
loss: 0.010530 0.012566 [1230848/1237259]
Epoch 942
-------------------------------
loss: 0.012167 0.012533 [ 2048/1237259]
loss: 0.009084 0.012602 [206848/1237259]
loss: 0.014634 0.012614 [411648/1237259]
loss: 0.010811 0.012623 [616448/1237259]
loss: 0.012119 0.012589 [821248/1237259]
loss: 0.011506 0.012533 [1026048/1237259]
loss: 0.009103 0.012649 [1230848/1237259]
Epoch 943
-------------------------------
loss: 0.012550 0.012717 [ 2048/1237259]
loss: 0.010033 0.012565 [206848/1237259]
loss: 0.010755 0.012745 [411648/1237259]
loss: 0.012893 0.012555 [616448/1237259]
loss: 0.012209 0.012725 [821248/1237259]
loss: 0.012599 0.012546 [1026048/1237259]
loss: 0.011621 0.012455 [1230848/1237259]
Epoch 944
-------------------------------
loss: 0.013786 0.012501 [ 2048/1237259]
loss: 0.010178 0.012632 [206848/1237259]
loss: 0.014406 0.012635 [411648/1237259]
loss: 0.013525 0.012582 [616448/1237259]
loss: 0.018173 0.012691 [821248/1237259]
loss: 0.012086 0.012598 [1026048/1237259]
loss: 0.008525 0.012681 [1230848/1237259]
Epoch 945
-------------------------------
loss: 0.011941 0.012419 [ 2048/1237259]
loss: 0.011370 0.012587 [206848/1237259]
loss: 0.012981 0.012554 [411648/1237259]
loss: 0.012002 0.012669 [616448/1237259]
loss: 0.013073 0.012585 [821248/1237259]
loss: 0.013338 0.012649 [1026048/1237259]
loss: 0.011537 0.012685 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0653  
ndcg@20: 0.0535  
diversity: 0.1958  


Epoch 946
-------------------------------
loss: 0.008376 0.012611 [ 2048/1237259]
loss: 0.011956 0.012514 [206848/1237259]
loss: 0.012987 0.012593 [411648/1237259]
loss: 0.012533 0.012664 [616448/1237259]
loss: 0.007697 0.012636 [821248/1237259]
loss: 0.012537 0.012671 [1026048/1237259]
loss: 0.020620 0.012492 [1230848/1237259]
Epoch 947
-------------------------------
loss: 0.014785 0.012659 [ 2048/1237259]
loss: 0.010820 0.012668 [206848/1237259]
loss: 0.016439 0.012754 [411648/1237259]
loss: 0.010105 0.012583 [616448/1237259]
loss: 0.015640 0.012647 [821248/1237259]
loss: 0.011384 0.012645 [1026048/1237259]
loss: 0.013616 0.012694 [1230848/1237259]
Epoch 948
-------------------------------
loss: 0.009892 0.012571 [ 2048/1237259]
loss: 0.017648 0.012705 [206848/1237259]
loss: 0.011724 0.012570 [411648/1237259]
loss: 0.011955 0.012535 [616448/1237259]
loss: 0.014275 0.012585 [821248/1237259]
loss: 0.013223 0.012560 [1026048/1237259]
loss: 0.012049 0.012750 [1230848/1237259]
Epoch 949
-------------------------------
loss: 0.010756 0.012658 [ 2048/1237259]
loss: 0.013492 0.012656 [206848/1237259]
loss: 0.015584 0.012543 [411648/1237259]
loss: 0.012028 0.012615 [616448/1237259]
loss: 0.009743 0.012585 [821248/1237259]
loss: 0.011764 0.012612 [1026048/1237259]
loss: 0.010496 0.012705 [1230848/1237259]
Epoch 950
-------------------------------
loss: 0.014673 0.012531 [ 2048/1237259]
loss: 0.013997 0.012598 [206848/1237259]
loss: 0.012473 0.012582 [411648/1237259]
loss: 0.009985 0.012480 [616448/1237259]
loss: 0.016701 0.012484 [821248/1237259]
loss: 0.015736 0.012709 [1026048/1237259]
loss: 0.013490 0.012555 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0652  
ndcg@20: 0.0535  
diversity: 0.1957  


Epoch 951
-------------------------------
loss: 0.012056 0.012617 [ 2048/1237259]
loss: 0.013037 0.012592 [206848/1237259]
loss: 0.010675 0.012585 [411648/1237259]
loss: 0.010190 0.012619 [616448/1237259]
loss: 0.008676 0.012467 [821248/1237259]
loss: 0.013011 0.012588 [1026048/1237259]
loss: 0.009481 0.012610 [1230848/1237259]
Epoch 952
-------------------------------
loss: 0.009957 0.012531 [ 2048/1237259]
loss: 0.013974 0.012566 [206848/1237259]
loss: 0.014701 0.012612 [411648/1237259]
loss: 0.009420 0.012597 [616448/1237259]
loss: 0.012615 0.012554 [821248/1237259]
loss: 0.013204 0.012572 [1026048/1237259]
loss: 0.012522 0.012556 [1230848/1237259]
Epoch 953
-------------------------------
loss: 0.012680 0.012526 [ 2048/1237259]
loss: 0.009618 0.012579 [206848/1237259]
loss: 0.011151 0.012570 [411648/1237259]
loss: 0.009639 0.012560 [616448/1237259]
loss: 0.016104 0.012647 [821248/1237259]
loss: 0.013011 0.012682 [1026048/1237259]
loss: 0.015463 0.012649 [1230848/1237259]
Epoch 954
-------------------------------
loss: 0.014843 0.012542 [ 2048/1237259]
loss: 0.010671 0.012565 [206848/1237259]
loss: 0.012775 0.012601 [411648/1237259]
loss: 0.019271 0.012669 [616448/1237259]
loss: 0.013469 0.012524 [821248/1237259]
loss: 0.010905 0.012687 [1026048/1237259]
loss: 0.013927 0.012680 [1230848/1237259]
Epoch 955
-------------------------------
loss: 0.016906 0.012562 [ 2048/1237259]
loss: 0.013946 0.012500 [206848/1237259]
loss: 0.013358 0.012504 [411648/1237259]
loss: 0.012429 0.012608 [616448/1237259]
loss: 0.013575 0.012596 [821248/1237259]
loss: 0.010587 0.012535 [1026048/1237259]
loss: 0.011159 0.012617 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0654  
ndcg@20: 0.0535  
diversity: 0.1959  


Epoch 956
-------------------------------
loss: 0.011955 0.012575 [ 2048/1237259]
loss: 0.013014 0.012621 [206848/1237259]
loss: 0.014942 0.012663 [411648/1237259]
loss: 0.011055 0.012635 [616448/1237259]
loss: 0.012335 0.012644 [821248/1237259]
loss: 0.011723 0.012533 [1026048/1237259]
loss: 0.015543 0.012502 [1230848/1237259]
Epoch 957
-------------------------------
loss: 0.010684 0.012613 [ 2048/1237259]
loss: 0.012438 0.012485 [206848/1237259]
loss: 0.011092 0.012718 [411648/1237259]
loss: 0.012625 0.012665 [616448/1237259]
loss: 0.016945 0.012459 [821248/1237259]
loss: 0.009431 0.012632 [1026048/1237259]
loss: 0.010744 0.012509 [1230848/1237259]
Epoch 958
-------------------------------
loss: 0.012803 0.012671 [ 2048/1237259]
loss: 0.011541 0.012459 [206848/1237259]
loss: 0.008859 0.012542 [411648/1237259]
loss: 0.010027 0.012628 [616448/1237259]
loss: 0.010774 0.012619 [821248/1237259]
loss: 0.010001 0.012623 [1026048/1237259]
loss: 0.015739 0.012636 [1230848/1237259]
Epoch 959
-------------------------------
loss: 0.011982 0.012546 [ 2048/1237259]
loss: 0.009801 0.012740 [206848/1237259]
loss: 0.011541 0.012791 [411648/1237259]
loss: 0.012588 0.012532 [616448/1237259]
loss: 0.013185 0.012570 [821248/1237259]
loss: 0.012914 0.012707 [1026048/1237259]
loss: 0.015312 0.012528 [1230848/1237259]
Epoch 960
-------------------------------
loss: 0.011087 0.012708 [ 2048/1237259]
loss: 0.010666 0.012738 [206848/1237259]
loss: 0.017206 0.012537 [411648/1237259]
loss: 0.011013 0.012565 [616448/1237259]
loss: 0.013778 0.012651 [821248/1237259]
loss: 0.015551 0.012660 [1026048/1237259]
loss: 0.012835 0.012734 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0652  
ndcg@20: 0.0536  
diversity: 0.1961  


Epoch 961
-------------------------------
loss: 0.014006 0.012537 [ 2048/1237259]
loss: 0.010848 0.012670 [206848/1237259]
loss: 0.012330 0.012643 [411648/1237259]
loss: 0.017315 0.012609 [616448/1237259]
loss: 0.014105 0.012613 [821248/1237259]
loss: 0.012694 0.012690 [1026048/1237259]
loss: 0.010162 0.012587 [1230848/1237259]
Epoch 962
-------------------------------
loss: 0.010453 0.012567 [ 2048/1237259]
loss: 0.011321 0.012647 [206848/1237259]
loss: 0.013754 0.012731 [411648/1237259]
loss: 0.013699 0.012582 [616448/1237259]
loss: 0.011852 0.012612 [821248/1237259]
loss: 0.011449 0.012719 [1026048/1237259]
loss: 0.009696 0.012709 [1230848/1237259]
Epoch 963
-------------------------------
loss: 0.010535 0.012713 [ 2048/1237259]
loss: 0.011342 0.012594 [206848/1237259]
loss: 0.011190 0.012530 [411648/1237259]
loss: 0.011881 0.012533 [616448/1237259]
loss: 0.011301 0.012603 [821248/1237259]
loss: 0.013333 0.012735 [1026048/1237259]
loss: 0.017038 0.012582 [1230848/1237259]
Epoch 964
-------------------------------
loss: 0.012513 0.012668 [ 2048/1237259]
loss: 0.011542 0.012535 [206848/1237259]
loss: 0.011477 0.012577 [411648/1237259]
loss: 0.014268 0.012705 [616448/1237259]
loss: 0.009628 0.012697 [821248/1237259]
loss: 0.014591 0.012644 [1026048/1237259]
loss: 0.012456 0.012505 [1230848/1237259]
Epoch 965
-------------------------------
loss: 0.009541 0.012677 [ 2048/1237259]
loss: 0.014747 0.012692 [206848/1237259]
loss: 0.009804 0.012693 [411648/1237259]
loss: 0.014872 0.012631 [616448/1237259]
loss: 0.014870 0.012621 [821248/1237259]
loss: 0.009783 0.012637 [1026048/1237259]
loss: 0.014281 0.012641 [1230848/1237259]
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0653  
ndcg@20: 0.0536  
diversity: 0.1955  


Epoch 966
-------------------------------
loss: 0.012341 0.012694 [ 2048/1237259]
loss: 0.013813 0.012600 [206848/1237259]
loss: 0.015428 0.012553 [411648/1237259]
loss: 0.014180 0.012514 [616448/1237259]
loss: 0.014939 0.012557 [821248/1237259]
loss: 0.012121 0.012473 [1026048/1237259]
loss: 0.015945 0.012615 [1230848/1237259]
Epoch 967
-------------------------------
loss: 0.008980 0.012628 [ 2048/1237259]
loss: 0.011922 0.012516 [206848/1237259]
loss: 0.017039 0.012625 [411648/1237259]
loss: 0.011597 0.012562 [616448/1237259]
loss: 0.012158 0.012582 [821248/1237259]
loss: 0.013380 0.012517 [1026048/1237259]
loss: 0.016079 0.012669 [1230848/1237259]
Epoch 968
-------------------------------
loss: 0.011367 0.012648 [ 2048/1237259]
loss: 0.012155 0.012618 [206848/1237259]
loss: 0.013785 0.012611 [411648/1237259]
loss: 0.011825 0.012636 [616448/1237259]
loss: 0.010324 0.012501 [821248/1237259]
loss: 0.013359 0.012608 [1026048/1237259]
loss: 0.013691 0.012737 [1230848/1237259]
Epoch 969
-------------------------------
loss: 0.012087 0.012653 [ 2048/1237259]
loss: 0.011631 0.012740 [206848/1237259]
loss: 0.013399 0.012475 [411648/1237259]
loss: 0.015943 0.012569 [616448/1237259]
loss: 0.013741 0.012640 [821248/1237259]
loss: 0.015026 0.012664 [1026048/1237259]
loss: 0.013169 0.012612 [1230848/1237259]
Epoch 970
-------------------------------
loss: 0.012465 0.012605 [ 2048/1237259]
loss: 0.009801 0.012655 [206848/1237259]
loss: 0.012534 0.012613 [411648/1237259]
loss: 0.011282 0.012575 [616448/1237259]
loss: 0.009911 0.012562 [821248/1237259]
loss: 0.015774 0.012720 [1026048/1237259]
loss: 0.011985 0.012574 [1230848/1237259]
Best recall@20 model updated. Saving the model.
Best ndcg@20 model updated. Saving the model.
Eval results: 
recall@20: 0.0655  
ndcg@20: 0.0538  
diversity: 0.1957  


Epoch 971
-------------------------------
loss: 0.013964 0.012615 [ 2048/1237259]
loss: 0.013320 0.012667 [206848/1237259]
loss: 0.009697 0.012647 [411648/1237259]
loss: 0.013713 0.012639 [616448/1237259]
loss: 0.013020 0.012608 [821248/1237259]
loss: 0.014857 0.012573 [1026048/1237259]
loss: 0.015201 0.012662 [1230848/1237259]
Epoch 972
-------------------------------
loss: 0.009886 0.012670 [ 2048/1237259]
loss: 0.009486 0.012575 [206848/1237259]
loss: 0.013062 0.012751 [411648/1237259]
loss: 0.014142 0.012596 [616448/1237259]
loss: 0.009340 0.012785 [821248/1237259]
loss: 0.012961 0.012747 [1026048/1237259]
loss: 0.010635 0.012580 [1230848/1237259]
Epoch 973
-------------------------------
loss: 0.012433 0.012616 [ 2048/1237259]
loss: 0.012209 0.012674 [206848/1237259]
loss: 0.016487 0.012590 [411648/1237259]
loss: 0.010925 0.012694 [616448/1237259]
loss: 0.011012 0.012492 [821248/1237259]
loss: 0.014674 0.012603 [1026048/1237259]
loss: 0.012469 0.012687 [1230848/1237259]
Epoch 974
-------------------------------
loss: 0.012618 0.012594 [ 2048/1237259]
loss: 0.013522 0.012479 [206848/1237259]
loss: 0.016976 0.012581 [411648/1237259]
loss: 0.008702 0.012719 [616448/1237259]
loss: 0.010445 0.012726 [821248/1237259]
loss: 0.010210 0.012650 [1026048/1237259]
loss: 0.014458 0.012646 [1230848/1237259]
Epoch 975
-------------------------------
loss: 0.015025 0.012669 [ 2048/1237259]
loss: 0.014949 0.012509 [206848/1237259]
loss: 0.007501 0.012780 [411648/1237259]
loss: 0.012455 0.012473 [616448/1237259]
loss: 0.012258 0.012554 [821248/1237259]
loss: 0.006446 0.012560 [1026048/1237259]
loss: 0.012363 0.012615 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0535  
diversity: 0.1959  


Epoch 976
-------------------------------
loss: 0.016733 0.012620 [ 2048/1237259]
loss: 0.014146 0.012653 [206848/1237259]
loss: 0.009817 0.012556 [411648/1237259]
loss: 0.012754 0.012737 [616448/1237259]
loss: 0.014618 0.012732 [821248/1237259]
loss: 0.014783 0.012478 [1026048/1237259]
loss: 0.015392 0.012558 [1230848/1237259]
Epoch 977
-------------------------------
loss: 0.011751 0.012757 [ 2048/1237259]
loss: 0.013008 0.012515 [206848/1237259]
loss: 0.014150 0.012684 [411648/1237259]
loss: 0.011291 0.012636 [616448/1237259]
loss: 0.012105 0.012780 [821248/1237259]
loss: 0.014084 0.012794 [1026048/1237259]
loss: 0.009984 0.012577 [1230848/1237259]
Epoch 978
-------------------------------
loss: 0.010627 0.012502 [ 2048/1237259]
loss: 0.012168 0.012690 [206848/1237259]
loss: 0.011469 0.012706 [411648/1237259]
loss: 0.014521 0.012717 [616448/1237259]
loss: 0.010759 0.012608 [821248/1237259]
loss: 0.011593 0.012716 [1026048/1237259]
loss: 0.011188 0.012728 [1230848/1237259]
Epoch 979
-------------------------------
loss: 0.012246 0.012718 [ 2048/1237259]
loss: 0.013111 0.012617 [206848/1237259]
loss: 0.010601 0.012630 [411648/1237259]
loss: 0.010995 0.012567 [616448/1237259]
loss: 0.012927 0.012673 [821248/1237259]
loss: 0.012982 0.012642 [1026048/1237259]
loss: 0.010925 0.012539 [1230848/1237259]
Epoch 980
-------------------------------
loss: 0.011609 0.012667 [ 2048/1237259]
loss: 0.012386 0.012688 [206848/1237259]
loss: 0.007219 0.012617 [411648/1237259]
loss: 0.013879 0.012653 [616448/1237259]
loss: 0.011977 0.012704 [821248/1237259]
loss: 0.011268 0.012644 [1026048/1237259]
loss: 0.012282 0.012606 [1230848/1237259]
Eval results: 
recall@20: 0.0650  
ndcg@20: 0.0534  
diversity: 0.1958  


Epoch 981
-------------------------------
loss: 0.012826 0.012741 [ 2048/1237259]
loss: 0.009892 0.012618 [206848/1237259]
loss: 0.015046 0.012477 [411648/1237259]
loss: 0.010284 0.012669 [616448/1237259]
loss: 0.011705 0.012506 [821248/1237259]
loss: 0.012160 0.012647 [1026048/1237259]
loss: 0.015458 0.012575 [1230848/1237259]
Epoch 982
-------------------------------
loss: 0.009616 0.012707 [ 2048/1237259]
loss: 0.013896 0.012666 [206848/1237259]
loss: 0.010078 0.012697 [411648/1237259]
loss: 0.012343 0.012692 [616448/1237259]
loss: 0.012104 0.012573 [821248/1237259]
loss: 0.010716 0.012875 [1026048/1237259]
loss: 0.011161 0.012539 [1230848/1237259]
Epoch 983
-------------------------------
loss: 0.012982 0.012639 [ 2048/1237259]
loss: 0.012923 0.012526 [206848/1237259]
loss: 0.010479 0.012665 [411648/1237259]
loss: 0.011622 0.012679 [616448/1237259]
loss: 0.013901 0.012631 [821248/1237259]
loss: 0.011099 0.012745 [1026048/1237259]
loss: 0.010858 0.012565 [1230848/1237259]
Epoch 984
-------------------------------
loss: 0.010211 0.012543 [ 2048/1237259]
loss: 0.013922 0.012665 [206848/1237259]
loss: 0.012695 0.012646 [411648/1237259]
loss: 0.011763 0.012625 [616448/1237259]
loss: 0.014777 0.012719 [821248/1237259]
loss: 0.008685 0.012559 [1026048/1237259]
loss: 0.012448 0.012612 [1230848/1237259]
Epoch 985
-------------------------------
loss: 0.013818 0.012538 [ 2048/1237259]
loss: 0.011152 0.012654 [206848/1237259]
loss: 0.009342 0.012771 [411648/1237259]
loss: 0.012470 0.012599 [616448/1237259]
loss: 0.010461 0.012693 [821248/1237259]
loss: 0.011970 0.012717 [1026048/1237259]
loss: 0.012358 0.012645 [1230848/1237259]
Eval results: 
recall@20: 0.0651  
ndcg@20: 0.0534  
diversity: 0.1956  


Epoch 986
-------------------------------
loss: 0.013174 0.012639 [ 2048/1237259]
loss: 0.014173 0.012562 [206848/1237259]
loss: 0.010890 0.012586 [411648/1237259]
loss: 0.013171 0.012742 [616448/1237259]
loss: 0.012835 0.012616 [821248/1237259]
loss: 0.009240 0.012609 [1026048/1237259]
loss: 0.011013 0.012664 [1230848/1237259]
Epoch 987
-------------------------------
loss: 0.009152 0.012697 [ 2048/1237259]
loss: 0.011978 0.012661 [206848/1237259]
loss: 0.009842 0.012609 [411648/1237259]
loss: 0.013563 0.012735 [616448/1237259]
loss: 0.011820 0.012776 [821248/1237259]
loss: 0.012418 0.012623 [1026048/1237259]
loss: 0.014531 0.012452 [1230848/1237259]
Epoch 988
-------------------------------
loss: 0.014645 0.012601 [ 2048/1237259]
loss: 0.009668 0.012398 [206848/1237259]
loss: 0.012525 0.012659 [411648/1237259]
loss: 0.014117 0.012782 [616448/1237259]
loss: 0.011553 0.012743 [821248/1237259]
loss: 0.010142 0.012584 [1026048/1237259]
loss: 0.015133 0.012675 [1230848/1237259]
Epoch 989
-------------------------------
loss: 0.012922 0.012688 [ 2048/1237259]
loss: 0.012525 0.012632 [206848/1237259]
loss: 0.008243 0.012647 [411648/1237259]
loss: 0.011302 0.012684 [616448/1237259]
loss: 0.008265 0.012591 [821248/1237259]
loss: 0.012131 0.012479 [1026048/1237259]
loss: 0.011319 0.012708 [1230848/1237259]
Epoch 990
-------------------------------
loss: 0.012480 0.012678 [ 2048/1237259]
loss: 0.008352 0.012780 [206848/1237259]
loss: 0.012490 0.012807 [411648/1237259]
loss: 0.010365 0.012643 [616448/1237259]
loss: 0.013589 0.012494 [821248/1237259]
loss: 0.012849 0.012605 [1026048/1237259]
loss: 0.014391 0.012693 [1230848/1237259]
Best diversity model updated. Saving the model.
Eval results: 
recall@20: 0.0648  
ndcg@20: 0.0533  
diversity: 0.1962  


Epoch 991
-------------------------------
loss: 0.011694 0.012620 [ 2048/1237259]
loss: 0.009274 0.012510 [206848/1237259]
loss: 0.010132 0.012563 [411648/1237259]
loss: 0.012349 0.012630 [616448/1237259]
loss: 0.012278 0.012735 [821248/1237259]
loss: 0.009349 0.012584 [1026048/1237259]
loss: 0.013857 0.012663 [1230848/1237259]
Epoch 992
-------------------------------
loss: 0.010965 0.012596 [ 2048/1237259]
loss: 0.011515 0.012678 [206848/1237259]
loss: 0.011056 0.012596 [411648/1237259]
loss: 0.014202 0.012611 [616448/1237259]
loss: 0.013769 0.012609 [821248/1237259]
loss: 0.012067 0.012532 [1026048/1237259]
loss: 0.009489 0.012597 [1230848/1237259]
Epoch 993
-------------------------------
loss: 0.011870 0.012450 [ 2048/1237259]
loss: 0.010820 0.012526 [206848/1237259]
loss: 0.016914 0.012573 [411648/1237259]
loss: 0.009726 0.012540 [616448/1237259]
loss: 0.010528 0.012516 [821248/1237259]
loss: 0.011312 0.012627 [1026048/1237259]
loss: 0.013440 0.012622 [1230848/1237259]
Epoch 994
-------------------------------
loss: 0.012052 0.012783 [ 2048/1237259]
loss: 0.010452 0.012753 [206848/1237259]
loss: 0.014745 0.012719 [411648/1237259]
loss: 0.012978 0.012701 [616448/1237259]
loss: 0.013258 0.012634 [821248/1237259]
loss: 0.016570 0.012660 [1026048/1237259]
loss: 0.011211 0.012690 [1230848/1237259]
Epoch 995
-------------------------------
loss: 0.014093 0.012613 [ 2048/1237259]
loss: 0.012316 0.012565 [206848/1237259]
loss: 0.015031 0.012708 [411648/1237259]
loss: 0.008252 0.012577 [616448/1237259]
loss: 0.011772 0.012605 [821248/1237259]
loss: 0.016103 0.012671 [1026048/1237259]
loss: 0.011958 0.012645 [1230848/1237259]
Eval results: 
recall@20: 0.0649  
ndcg@20: 0.0534  
diversity: 0.1959  


Epoch 996
-------------------------------
loss: 0.014952 0.012646 [ 2048/1237259]
loss: 0.010892 0.012633 [206848/1237259]
loss: 0.011771 0.012658 [411648/1237259]
loss: 0.014328 0.012689 [616448/1237259]
loss: 0.012320 0.012647 [821248/1237259]
loss: 0.013151 0.012589 [1026048/1237259]
loss: 0.010145 0.012597 [1230848/1237259]
Epoch 997
-------------------------------
loss: 0.010767 0.012592 [ 2048/1237259]
loss: 0.015737 0.012688 [206848/1237259]
loss: 0.009188 0.012732 [411648/1237259]
loss: 0.012926 0.012548 [616448/1237259]
loss: 0.013373 0.012504 [821248/1237259]
loss: 0.010115 0.012433 [1026048/1237259]
loss: 0.010939 0.012748 [1230848/1237259]
Epoch 998
-------------------------------
loss: 0.009743 0.012521 [ 2048/1237259]
loss: 0.012746 0.012649 [206848/1237259]
loss: 0.015109 0.012821 [411648/1237259]
loss: 0.008992 0.012684 [616448/1237259]
loss: 0.010781 0.012731 [821248/1237259]
loss: 0.014019 0.012669 [1026048/1237259]
loss: 0.008227 0.012684 [1230848/1237259]
Epoch 999
-------------------------------
loss: 0.009980 0.012638 [ 2048/1237259]
loss: 0.011617 0.012640 [206848/1237259]
loss: 0.013951 0.012688 [411648/1237259]
loss: 0.015510 0.012604 [616448/1237259]
loss: 0.013089 0.012575 [821248/1237259]
loss: 0.012067 0.012682 [1026048/1237259]
loss: 0.011489 0.012621 [1230848/1237259]
Epoch 1000
-------------------------------
loss: 0.012608 0.012666 [ 2048/1237259]
loss: 0.011067 0.012702 [206848/1237259]
loss: 0.011498 0.012635 [411648/1237259]
loss: 0.010230 0.012750 [616448/1237259]
loss: 0.013002 0.012672 [821248/1237259]
loss: 0.008492 0.012647 [1026048/1237259]
loss: 0.009981 0.012538 [1230848/1237259]
Eval results: 
recall@20: 0.0652  
ndcg@20: 0.0535  
diversity: 0.1959  


